I"4<h1 id="linear-regression">Linear regression</h1>

<p>This page is being formulated currently by plagerising “An Introduction to Statistical Learning”. If you find this page before a lot of changes are complete, then keep this in mind. Once major changes/completion occurs, this message will be updated to references instead.</p>

<h1 id="simple-linear-regression">Simple linear regression</h1>

<p>Predict a quantitative response \(Y\) using a predictor variable \(X\); regressing \(Y\) onto \(X\).
The <em>intercept</em> and <em>splope</em> are written as \(\beta_0\) and \(\beta_1\), respectively.
These are unknwon constants and together are knwon as the model <em>coefficients</em> or <em>parameters</em>.
The simple linear regression is written as</p>

<p>[Y \approx \beta_0 + \beta_1 X]</p>

<p>Values that are estimated are labelled with a “hat”, e.g. 
\(\hat y\) - a prediction of \(Y\) based on \(X = x\).
With some sample data, one can begin to predict \(Y\) based on the predictor \(X\) using the estimated model coefficients \(\hat \beta_0\) and \(\hat \beta_1\);</p>

<p>[\hat y \approx \hat\beta_0 + \hat\beta_1 x]</p>

<p>In this case, the estimated response \(\hat y\) equals the estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) according to a sample of the predictor values (\(x\)).</p>

<h1 id="drawing-the-line">Drawing the line</h1>

<p>The estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) are unknown, therefore we need to get these values to predict \(Y\) based on \(X\).
A number (\(n\)) of obersevations are made where we measure \(X\) and \(Y\). Measurements could be recorded as: (measure 1, x = 5, y = 10), (measure 2, x = 10, y = 20), and so on up to \(n\) obersavations;</p>

<p>[(5,10), (10,20),…, (x_n,y_n)]</p>

<p>[(x_1,y_1), (x_2,y_2),…, (x_n,y_n)]</p>

<p>We want to combine each measurment on a plot so that the line drawn through the data fits well and produces coefficient estimates \(\hat \beta_0\) and \(\hat \beta_1\).
Each measurement (\(i\)) is represented with \(y_i \approx \hat \beta_0 + \hat \beta_1 x_i\) for \(i = 1,2,...,n\).
The ideal result will be a line that fits all points closely. 
The measure of <em>closeness</em> has many topics of interest, but the most common method is to minimise the <em>least squares</em>.</p>

<p>\(e_i = y_i - \hat y_i\) represents the \(i\)th <em>residual</em> - the difference between our \(i\)th response according to our model versus the true \(i\)th observed response.
The <em>residual sum of squares</em> (RSS) is written as
\(RSS = e^2 _1 + e^2 _2 +...+ e^2 _n\)</p>

<p>[RSS = (y_1-\hat\beta0-\hat\beta_1x_1)^2+(y_2-\hat\beta_0-\hat\beta_1x_2)^2+…+(y_n-\hat\beta_0-\hat\beta_1x_n)^2.]</p>

<p>The least squares method uses \(\hat\beta_0 and \hat\beta_1\) such that RSS is minimised. The minimisers are as follows</p>

<p>[\label{eq1}\tag{1} 
 \hat \beta_1  = 
\frac{ 
\sum_{i=1}^{n}	(	xi -\bar{x} )	(yi - \bar{y}	)	} 
{\sum_{i=1}^{n}	(	xi -\bar{x} )^2
}]</p>

<p>[\hat\beta_0 - \bar{y} - \hat\beta_1 \bar{x}]</p>

<p>where the sample means are
\(\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i\)
and
\(\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i\)
so the equation above defines the least squares coefficient estimates for simple linear regression.</p>

<h1 id="accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</h1>
:ET