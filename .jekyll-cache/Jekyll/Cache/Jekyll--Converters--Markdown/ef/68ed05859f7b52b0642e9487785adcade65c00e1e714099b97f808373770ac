I"Â
<h1 id="linear-regression">Linear regression</h1>

<p>This page is being formulated currently by plagerising â€œAn Introduction to Statistical Learningâ€. If you find this page before a lot of changes are complete, then keep this in mind. Once major changes/completion occurs, this message will be updated to references instead.</p>

<h1 id="simple-linear-regression">Simple linear regression</h1>

<p>Predict a quantitative response \(Y\) using a predictor variable \(X\); regressing \(Y\) onto \(X\).
The <em>intercept</em> and <em>splope</em> are written as \(\beta_0\) and \(\beta_1\), respectively.
These are unknwon constants and together are knwon as the model <em>coefficients</em> or <em>parameters</em>.
The simple linear regression is written as</p>

<p>[Y \approx \beta_0 + \beta_1 X]</p>

<p>Values that are estimated are labelled with a â€œhatâ€, e.g. 
\(\hat y\) - a prediction of \(Y\) based on \(X = x\).
With some sample data, one can begin to predict \(Y\) based on the predictor \(X\) using the estimated model coefficients \(\hat \beta_0\) and \(\hat \beta_1\);</p>

<p>[\hat y \approx \hat\beta_0 + \hat\beta_1 x]</p>

<p>In this case, the estimated response \(\hat y\) equals the estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) according to a sample of the predictor values (\(x\)).</p>

<h1 id="drawing-the-line">Drawing the line</h1>

<p>The estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) are unknown, therefore we need to get these values to predict \(Y\) based on \(X\).
A number (\(n\)) of obersevations are made where we measure \(X\) and \(Y\). Measurements could be recorded as: (measure 1, x = 5, y = 10), (measure 2, x = 10, y = 20), and so on up to \(n\) obersavations;</p>

<p>[(5,10), (10,20),â€¦, (x_n,y_n)]</p>

<p>[(x_1,y_1), (x_2,y_2),â€¦, (x_n,y_n)]</p>

<p>We want to combine each measurment on a plot so that the line drawn through the data fits well and produces coefficient estimates \(\hat \beta_0\) and \(\hat \beta_1\).
Each measurement (\(i\)) is represented with \(y_i \approx \hat \beta_0 + \hat \beta_1 x_i\) for \(i = 1,2,...,n\).
The ideal result will be a line that fits all points closely. 
The measure of <em>closeness</em> has a many topics of interest, but the most common method is to minimise the <em>least squares</em>.</p>

<p>[RSS = e^2 _1 + e^2 _2 +â€¦+ e^2 _n]</p>

<p>[RSS = (y_1-\hat\beta0-\hat\beta_1x_1)^2+(y_2-\hat\beta_0-\hat\beta_1x_2)^2+â€¦+(y_n-\hat\beta_0-\hat\beta_1x_n)^2.]</p>

<p>[\hat \beta_1  = 
\frac{ 
\sum_{i=1}^{n}	(	xi -\bar{x} )	(yi - \bar{y}	)	} 
{\sum_{i=1}^{n}	(	xi -\bar{x} )^2
}]</p>

<p>[\hat\beta_0 - \bar{y} - \hat\beta_1 \bar{x}]</p>

<p>Testing mathjax for LaTeX. Includes/head should load script to run.</p>
:ET