<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Tom Preston-Werner</title>
 <link href="http://tom.preston-werner.com/atom.xml" rel="self"/>
 <link href="http://tom.preston-werner.com/"/>
 <updated>2020-05-08T22:09:03+02:00</updated>
 <id>http://tom.preston-werner.com/</id>
 <author>
   <name>Tom Preston-Werner</name>
   <email>tom@mojombo.com</email>
 </author>

 
 <entry>
   <title>Unnecessary complexity in precision medicine</title>
   <link href="http://tom.preston-werner.com/2020/05/05/singhgupta_genes.html"/>
   <updated>2020-05-05T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2020/05/05/singhgupta_genes</id>
   <content type="html">&lt;h1 id=&quot;unnecessary-complexity-in-precision-medicine&quot;&gt;Unnecessary complexity in precision medicine&lt;/h1&gt;
&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-in-precision-medicine&quot; id=&quot;markdown-toc-unnecessary-complexity-in-precision-medicine&quot;&gt;Unnecessary complexity in precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#complexity-in-living-systems&quot; id=&quot;markdown-toc-complexity-in-living-systems&quot;&gt;Complexity in living systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#is-complexity-necessary&quot; id=&quot;markdown-toc-is-complexity-necessary&quot;&gt;Is complexity necessary?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#evolution-of-complexity-genic-genomic-and-developmental&quot; id=&quot;markdown-toc-evolution-of-complexity-genic-genomic-and-developmental&quot;&gt;Evolution of complexity: genic, genomic, and developmental&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-in-precision-medicine-1&quot; id=&quot;markdown-toc-unnecessary-complexity-in-precision-medicine-1&quot;&gt;Unnecessary complexity in precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-and-the-etiology-of-cancer&quot; id=&quot;markdown-toc-unnecessary-complexity-and-the-etiology-of-cancer&quot;&gt;Unnecessary complexity and the etiology of cancer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-beyond-precision-medicine&quot; id=&quot;markdown-toc-unnecessary-complexity-beyond-precision-medicine&quot;&gt;Unnecessary complexity beyond precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusions&quot; id=&quot;markdown-toc-conclusions&quot;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;
Singh_and_Gupta.2020.GenoMed;&lt;br /&gt;
Singh, R.S., Gupta, B.P. Genes and genomes and unnecessary complexity in precision medicine. npj Genom. Med. 5, 21 (2020). https://doi.org/10.1038/s41525-020-0128-1 Open Access
&lt;a href=&quot;https://doi.org/10.1038/s41525-020-0128-1&quot;&gt;https://doi.org/10.1038/s41525-020-0128-1&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Risk factors, gene to phenotype, are more complex that anticipated in early genomics. 
Genotype - phenotype
Molecular and evolutionary complexity.
Molecular contigency - chance driven mutation, redundancy, molectular pathway cross-over (shared phenotypes).
Necessary complexity versus unnecessary - evolutionary baggage due to molecular constraint and blind evolution.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;“Two individuals bearing the same set of risk factors may not present (or exhibit the symptoms of) the same disease. This is the kind of problem that precision medicine is expected to overcome.”
From familial studies, we often see that even clear monogenic, dominant disease does not always manifest equally among family members. “The lack of genetic determinism in disease is the result of molecular contingency.”&lt;/p&gt;

&lt;p&gt;An interesting point is made that I haven’t though much about; “Whether evolution is deterministic, i.e., predictable and repeatable, or contingent and unpredictable is an interesting topic in evolutionary biology (ref 1) (for a recent review see ref. 2)”
Although, this is just a smaller version of a question in physics that I do think about frequently: if every irreducibly small particle/force in the universe could be detected with a mapped trajectory, then we could “see” both forward and reverse in time; causal determinism. Since the particle physics answer will not come anytime soon, a discussion on the evolutionary version is reasonable.&lt;/p&gt;

&lt;p&gt;”..phenotypic evolutionary ‘repeatability’ is common when the founding populations are closely related, perhaps resulting from shared genes and developmental pathways, whereas different outcomes become more likely as historical divergences increase.”&lt;/p&gt;

&lt;p&gt;I believe that in some cases we can produces actionable levels of confidence in mutation prediction to make this a worthwhile endeavour. Knowing that a mutation is likely to occur (and be evolutionarily selected against) and result in disease means that we can prepare a treatment / cure for when it does occur naturally. 
For example, each nucleotide within each individual cell will have a particular probability for the energy required to mutate. 
Many factors affect this value; nucleotide type, methylation, DNA/RNA folding, bound proteins, chemical/UV insult, etc. 
Some of these factors can be estimated based on large studies, and some may never be quantifiable in the real world.
However, we can at least quantify some values to produce mutation predictor value X and and response Y and therefore quantify estimate coefficients, and therefore predict that mutation 1 is more likely to occur than mutation 2.&lt;/p&gt;

&lt;p&gt;Reference is made to “‘Evolution and Tinkering,’ Jacob (ref 4)” who compares natural selection to a tinkerer as opposed to an engineer.
Singh and Gupta propose “a theory of molecular complexity, consisting of necessary and unnecessary complexity in living systems”, that “exceedingly high level of unnecessary complexity” is more due to blind evolution rather than randomness of mutation. 
“Unecessary and unnecessary complexity is relevant to current discussions on genomics and precision medicine.”&lt;/p&gt;

&lt;h1 id=&quot;complexity-in-living-systems&quot;&gt;Complexity in living systems&lt;/h1&gt;
&lt;h1 id=&quot;is-complexity-necessary&quot;&gt;Is complexity necessary?&lt;/h1&gt;
&lt;h1 id=&quot;evolution-of-complexity-genic-genomic-and-developmental&quot;&gt;Evolution of complexity: genic, genomic, and developmental&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-in-precision-medicine-1&quot;&gt;Unnecessary complexity in precision medicine&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-and-the-etiology-of-cancer&quot;&gt;Unnecessary complexity and the etiology of cancer&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-beyond-precision-medicine&quot;&gt;Unnecessary complexity beyond precision medicine&lt;/h1&gt;
&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

</content>
 </entry>
 
 <entry>
   <title>Websites for basic genetic variant information</title>
   <link href="http://tom.preston-werner.com/2020/04/27/genetic_mutation_websites.html"/>
   <updated>2020-04-27T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2020/04/27/genetic_mutation_websites</id>
   <content type="html">&lt;h1 id=&quot;websites-for-basic-genetic-variant-information&quot;&gt;Websites for basic genetic variant information&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Identifying pathogenic variants with whole genome and whole exome sequencing is not simple.
Determining the correct filtering method can take some time but it is not the most difficult task.
Validating genetic factors is generally the most time consuming part of this type of research. 
Here is a compilation of some of the websites and resources that I use constantly.
I will begin this simply as a list but continue to contribute information on how to use all of these over time.
I use most of the listed resources daily.
There are several steps in assessing if a gene variant is a good candidate to explain a clinical phenotype.
Often a clear story can be made between the genetic mutation and the resulting phenotype.
Other times (usually) a genetic finding (particularly biallelic mutations) seem to have a direct link to the clinical phenotype but it can take  weeks-months to functionally validate such a finding.
 With that in mind, it is good to have some sort of routine way to quickly assess the possible pathogenicity of a mutation by hand.
I will mostly discuss these in the context of rare mutations which are likely to be under selective pressure and occur at very low frequencies in a healthy population.
 Sites and tools for getting basic genetic information For assessing rare variants Ensembl and Exac (now gnomAD) are my bread and butter.
I haven’t done it yet but I need to set up a hotkey to open a browser with both of these sites simultaneously.
To demonstrate how I like to use these we could use an example. Lets say we have NGS results for a patient with immunodeficiency with coding variants in the gene RAG2.
OK, well known gene, important for antibody production as wells as TCR and BCR development.
Looks good so far.
Let’s see if the variants are common SNPs or could they be likely to cause damage if they are not reported (of course this is in your pipeline automatically but it’s good practice and takes less than 60 seconds; valuable if there is a real person affected by your results).
 After a long time getting confused about transcripts and coordinates, I now know how important it is to accurately report coordinates so there is no confusion if collaborating or reporting the mutations etc. Jump over to Exac.
&lt;a href=&quot;http://exac.broadinstitute.org&quot;&gt;Exac.org&lt;/a&gt;
 This is in a nutshell, the exomes of ~ 60,000 individuals which can be used to view how frequently mutations occur in the general population (unfortunately it is mostly just European but there is some global representation). Exac is vital for checking coding variants.
It covers some of the intronic regions (exon intron splice sites) and some of the upstream and downstream regions.
This is typical for anyone who does whole exome sequencing.
 We mentioned confusion about transcripts and coordinates, Exac automatically loads the coverage as shown for the canonical transcript. Stick with this transcript for reporting or at least for your own notes.
When you head over to Ensembl grab the same one in the transcript table. Update! &lt;a href=&quot;http://gnomad.broadinstitute.org/about&quot;&gt;The Genome Aggregation Database (gnomAD) is online&lt;/a&gt;. This data set is the combination of “123,136 exomes and 15,496 genomes from unrelated individuals” which has “removed individuals known to be affected by severe pediatric disease, as well as their first-degree relatives.”
This is n extremely exciting resource. If you are familiar with Exac then you will know the value of this expansion into gnomAD. &lt;a href=&quot;https://youtu.be/_uRuFZv4JaU&quot;&gt;youtube.com&lt;/a&gt; &lt;a href=&quot;http://www.ensembl.org/index.html&quot;&gt;Ensembl&lt;/a&gt; Any good pipeline will have annotation of the details for any coding variants but it can be pretty valuable to go and look at these again by hand.
It doesn’t take long but can end up saving time in the long run.
If you do it often, the first check on Exac take less than 60 seconds.
The next check on Ensemble will only take 2-3 minutes.
In the quick search I plug in the gene name, luckily for my stuff the top hit is always the human gene (sorry Alpaca researchers).
When you get to the gene page first click is always “Show transcript table.”
If you are lucky there is only one coding transcript like for RAG2.
Most of the time there are about 6 transcripts of wildly varying lengths just to confuse matters.
Go for the transcript ID of the canonical transcript which you noted on Exac.
If you do so then life will be easier when you go to check the coordinates. 
On the left hand side in the table “Transcript-based displays” click “cDNA” shown under “Sequence”.
You can then search through to find the variant and amino acid to see if everything lines up.
You see the cDNA position and amino acid positions overlaid. If you were to pick a different transcript then of course the coordinates are likely to be different.
From here I usually go back to the table on the left of the screen to search Exons.
This obviously just lays out the exon sequences  in blocks along with useful information.
Only a small segment of the introns are displayed.
If you want reference sequences of multiple types just find the down load sequence button and chose FASTA and decide which type you want to display. You would likely have the information based on the annotated NGS data but you may want to look at the different transcripts and Ensembl is the best option. So far (in just a couple of minutes) you could have looked up the allele frequencies, affect of mutation on different transcripts and check that everything that should be reported from the NGS output matches up.
My next step is to check if these variants a already reported.
Everyone has their favourite method, searching PubMed etc. For my topics OMIM often produces good results and a quick search. &lt;a href=&quot;https://www.omim.org/&quot;&gt;Online Mendelian Inheritance in Man&lt;/a&gt;
This is a curated database and is generally very good.
Hopefully it continues to grow for a long time into the future.
Depending on how much you already know about your gene it is sometimes helpful to jump straight down to the “Allelic variants” section (if one is present).
You may find a few variants already reported with a similar phenotype being described as your case.
You may find the exact mutations already reported.
If this is the case then it is likely that it would have taken a few minutes longer to find the same cases on one of the other databases.
Whether you have found that there are many mutations reported similar to those that interest you or if you have found nothing reported so far, my next step is always to run through UniProt.
&lt;a href=&quot;http://www.uniprot.org&quot;&gt;UniProt&lt;/a&gt;
UniProt is so rich in information that there is no need to expnad on it here.
If you have never used it then just pick your favourite protein and go look it up now.
There is (usually) a combination of nearly everything you need to get a quick overview of a protein.
Gene function, functional domains, known variants, reported knockouts/mutagenesis studies, protein structures, expression, localisation, the list goes on.
Actually, as much as I love PDB, I find that using UniProt is usually quicker to check for available PDB protein structures before actually going to PDB to download from the source.
With these four websites one would likely be able to decide how confident you are about a candidate mutation/s.
At least if you are just looking coding variants.
Assessing non-coding regions is much messier business.
From here on in validation of a mutation can require a widely variable amount of functional work.
One thing is certain however, Sanger sequencing will be needed to confirm your NGS finding. &lt;a href=&quot;https://www.youtube.com/watch?v=3amsDkyiMu8&quot;&gt;youtube.com&lt;/a&gt; &lt;a href=&quot;https://github.com/gantzgraf/autoprimer3/releases/tag/v3.0.2&quot;&gt;Autoprimer3&lt;/a&gt;
Autoprimer3 is an excellent application that you can use to design primers for a gene of interest.
It is super quick for producing primers to be used on genomic DNA for “any UCSC genome and design PCR/sequencing primers to genes or genome coordinates”.
As an example I timed myself to see how long it takes to get a primer list for all exons of the gene RAG2 and a reference sequence from default genomic coordinates on hg38 while avoiding SNPs based on dbsnp142.
It took me 46 seconds to open the application and produce a primer list and reference sequence.
Less than 1% of the time I may have to go and redesign a primer manually because of an awkward sequence or a patient’s DNA may have some uncommon variant at the primer site. 
Depending on which supplier you order oligos from, Sanger sequencing to confirm a variant by found during NGS can be done within 3 days; about 90 seconds to design and order the oligos, a day or 2 until they are delivered,  and a day to PCR and sequence.
The explanation may be a bit long winded here but this app is excellent.
Just give it a try if you do any routine PCR or sequencing for coding variant.
As the name suggests, it is a simple version of Primer3 but super quick.
&lt;a href=&quot;https://software.broadinstitute.org/gatk/&quot;&gt;Genome Analysis Toolkit: Variant Discovery in High-Throughput Sequencing Data.&lt;/a&gt;
GATK most useful to jump straight to: &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/tooldocs/&quot;&gt;Tool Documentation Index&lt;/a&gt; Genome hg38 &lt;a href=&quot;http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=chr7:142299011,142813287&quot;&gt;(TCR region as example)&lt;/a&gt;
&lt;a href=&quot;https://gpgtools.org&quot;&gt;GPGtools&lt;/a&gt;
for sending sensitive patient info.
&lt;a href=&quot;https://www.gnupg.org&quot;&gt;GnuPG&lt;/a&gt; is GPL licensed alternative to the PGP suite for sending sensitive patient info.
See also Pretty good privacy for academic data. &lt;a href=&quot;http://www.umd.be/HSF3/HSF.html&quot;&gt;Human splice finder&lt;/a&gt; Illumina-Pipeline-V2 (“Version 2 of Illumina pipeline that incorporates &lt;a href=&quot;https://github.com/nirav99/Illumina-Pipeline-V2/blob/master/IlluminaPipelineCASAVA1_8.pdf&quot;&gt;CASAVA 1.8”)&lt;/a&gt; Sequence Manipulation Suite http://www.coccidia.icb.usp.br/sms2/index.html
Sequence Ontology http://www.sequenceontology.org
UCSC Genome Bioinformatics FAQ https://genome.ucsc.edu/FAQ/FAQformat
UCSC Table Browser https://genome.ucsc.edu/cgi-bin/hgTables
MutScan - https://github.com/OpenGene/MutScan
Detect and visualise target mutations by scanning FastQ files directly. Very useful if you are interested in some certain mutations but saves the time it would take to normally through your pipeline. &lt;/p&gt;
&lt;h2 id=&quot;communities-and-learning&quot;&gt;Communities and learning&lt;/h2&gt;
&lt;p&gt;No need to reinvent the wheel here. Stephen Turner has a better list of resources than I will produce with his post ”Staying Current in Bioinformatics &amp;amp; Genomics: 2017 Edition.” 
http://www.gettinggeneticsdone.com/2017/02/staying-current-in-bioinformatics-genomics-2017.html
Essentially it boils down to the journals, Twitter, some expert blogs, and several genomics communities.
The journals and other sites I like to follow are detailed here. When all directed into a single feed I think it produces an essential resource for most genetics/bioinformatics scientists.
Literature of Interest - In this post I show the use of Feedly to condense all the litereature that I follow into a single source and allow the option to view by category.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-and-cloud-computing&quot;&gt;Machine learning and cloud computing&lt;/h2&gt;
&lt;p&gt;In this post I have started to gather some of the resources I like to use and topics that I find interesting.
Some other links tagged on at the end:
BioStarts - Bioinformatics academic community https://www.biostars.org
Useful bash Bioinformatics one-liners&lt;br /&gt;
https://github.com/stephenturner/oneliners&lt;br /&gt;
Efficient R programming https://csgillespie.github.io/efficientR/
Cheat sheets for data.   science http://www.datasciencecentral.com/…
RStudio Cheat Sheets  
https://www.rstudio.com/resources/cheatsheets/#515&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Test - make a new md file</title>
   <link href="http://tom.preston-werner.com/2020/04/26/tester.html"/>
   <updated>2020-04-26T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2020/04/26/tester</id>
   <content type="html">&lt;h1 id=&quot;test---make-a-new-md-file&quot;&gt;Test - make a new md file&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;10 Nov 2016 - San Francisco&lt;/p&gt;

&lt;p&gt;This is a story about a company called
&lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk&lt;/a&gt; (pronounced “sneak”), their
founder Guy Podjarny, my decision to become one of their advisors, and how they
are going to help save you from malevolent agents trying to steal your digital
stuff.&lt;/p&gt;

&lt;p&gt;If you’re anything like me, you’re simultaneously terrified and in awe of the
increasing commonality of large corporate security breaches. Even big names like
Ebay, Home Depot, Anthem, JP Morgan Chase, Target, LinkedIn, Dropbox, and Yahoo
are falling victim to sophisticated attacks. If you spend even a few minutes
looking into it, you’ll be shocked at how frequently these breaches are
happening now. The fine folks at Information is Beautiful have an excellent
interactive visualization of the &lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;World’s Biggest Data
Breaches&lt;/a&gt;
over the last twelve years, in case you want to read all the gory details and
never get a restful night of sleep ever again:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;
  &lt;img src=&quot;/images/posts/2016-11-10/breaches.png&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve used a fair number of emotionally charged words above that might be
triggering your FUD detectors right about now. But be advised: it’s not paranoia
when they really are out to get you. If recent, extremely high profile (and
subsequently weaponized) breaches like those of the Clinton Campaign and the DNC
aren’t enough to make you want to air gap your entire life, then I envy your
steely-eyed mettle and implore you to teach me your meditation techniques.&lt;/p&gt;

&lt;p&gt;The fact is, security is hard. And it’s getting harder every day. To win, you
have to get it right every single time. To lose (and lose big), you only have to
screw it up once.&lt;/p&gt;

&lt;p&gt;During my years at GitHub, I spent a lot of time assembling a dedicated security
team, managing security audits and penetration tests, and working to establish a
culture of security awareness amongst our development team. All of this is
challenging and expensive, especially for a young company. Even worse, it’s the
kind of investment that’s totally invisible when it’s working, making it hard to
sustain until that crucial and terrible moment you end up on the front page of
Hacker News as the latest victim.&lt;/p&gt;

&lt;p&gt;A year ago I was contemplating this, especially the difficult proposition of
having developers, furious at work on new features, constantly maintain
awareness of security vulnerabilities they might be inadvertently weaving into
the product. Web application developers are generally not security experts, and
though I would love to live in a world where that wasn’t true, it’s just not a
realistic expectation. Meanwhile, modern development means an increasing
reliance on 3rd party code. Even a small Rails app will probably have 300 or
more gem dependencies after a few months of development. It’s even more in the
nodejs world. This level of modularization and code reuse, driven by the
explosion of high quality open source over the last decade, is amazing and I
absolutely love it, but it comes at a security expense.&lt;/p&gt;

&lt;p&gt;Open source projects are not known for their excellent security records.
Vulnerabilities like &lt;a href=&quot;http://heartbleed.com/&quot;&gt;Heartbleed&lt;/a&gt; and
&lt;a href=&quot;https://blog.cloudflare.com/inside-shellshock/&quot;&gt;Shellshock&lt;/a&gt; painfully
demonstrate the idea that “given enough eyeballs, all bugs are shallow” is
completely false. In fact, due to a flaw in YAML, Rails had a &lt;a href=&quot;http://blog.codeclimate.com/blog/2013/01/10/rails-remote-code-execution-vulnerability-explained/&quot;&gt;pretty extreme
remote code execution
vulnerability&lt;/a&gt;
for years. If you were running any version of Rails prior to the fix, you were
vulnerable. This stuff is real, and as responsible developers, we need to be
more proactive about it.&lt;/p&gt;

&lt;p&gt;Luckily, at the time I was pondering these matters, I ran into Guy Podjarny. As
a former cofounder of Blaze.io and then CTO of Web Experience at Akamai (which
acquired Blaze.io), Guy intimately understands the impact of security on today’s
web developers. He was working on an automated tool to scan and fix security
vulnerabilities in 3rd party dependencies. I was intrigued. They already had a
way to scan nodejs projects and look for known security vulnerabilities in the
dependency tree and automatically upgrade or patch affected libraries. I thought
this was pretty cool, but it was his vision for what automated security tooling
could be that sold me on him and his company. I can’t talk much about that
now, but just know that what Snyk is today is just the tip of what will
become an intelligent and proactive bodyguard for your entire codebase.&lt;/p&gt;

&lt;p&gt;A few months ago, Snyk released GitHub integration to make it fantastically
simple to hook up your repos to Snyk and, my favorite feature: the ability to
monitor your repo for future vulnerabilities and then &lt;strong&gt;automatically submit a
pull request&lt;/strong&gt; with the suggested package upgrade or hotfix patch (nodejs only for
now).&lt;/p&gt;

&lt;p&gt;Today, &lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk announced support for
Ruby&lt;/a&gt;. Take a look at that blog post,
it does an awesome job of explaining how simple it is to set up and what the
generated pull requests look like. It’s totally free for open source projects,
and extremely cheap insurance for your important projects.&lt;/p&gt;

&lt;p&gt;Make no mistake, 3rd party code is a clear and present danger to your business.
If you don’t know if you’re vulnerable, then you must assume that you are and
take steps to protect yourself. Snyk makes it easy.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Cryptography</title>
   <link href="http://tom.preston-werner.com/2020/04/26/cryptography.html"/>
   <updated>2020-04-26T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2020/04/26/cryptography</id>
   <content type="html">&lt;h1 id=&quot;cryptography&quot;&gt;Cryptography&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#breaking&quot;&gt;Breaking crypto&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aes&quot;&gt;AES is the most important current encryption method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#quantum&quot;&gt;Quantum computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thoughts&quot;&gt;Some thoughts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reading&quot;&gt;Reading list on more advanced topics&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Like most of the posts on this blog, this will be a work in progress. Cryptography is a topic which I stumbled upon and really enjoy.
For the reading list skip to the end of this page.
There is a long interesting history which would appeal to a casual reader.
Most people would be familiar with stories about crypto during WWII, particularly because of movies like
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.imdb.com/title/tt2084970/&quot;&gt;The Imitation Game&lt;span id=&quot;titleYear&quot;&gt;(2014).
&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;
Cracking of Enigma falls into the espionage theme along with stories like that of books from  &lt;span class=&quot;author notFaded&quot; style=&quot;color: #0000ff;&quot;&gt;&lt;span class=&quot;a-declarative&quot;&gt;&lt;a href=&quot;https://www.amazon.co.uk/Ben-Macintyre/e/B001H6WAL8/ref=dp_byline_cont_book_1&quot;&gt;Ben Macintyre.&lt;/a&gt; &lt;/span&gt;&lt;/span&gt;One of my favorites is: &lt;span id=&quot;productTitle&quot; class=&quot;a-size-large&quot;&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.amazon.co.uk/d/Books/Operation-Mincemeat-True-Story-Changed-Course-World/1408809214&quot;&gt;Operation Mincemeat&lt;/a&gt;:&lt;/span&gt; The True Spy Story That Changed the Course of World War II. &lt;/span&gt;
Reading so much about the non-fiction side of this topic ultimately led me to the &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_James_Bond_novels_and_short_stories&quot;&gt;Ian Flemming novels&lt;/a&gt;.&lt;/span&gt; Of course I had seen all the movies as a kid, and like most, loved them. Reading the novels in their order of release ended up being much more fun than I have with most fictional book series perhaps because of Flemming’s true involvements during WWII.&lt;/p&gt;

&lt;p&gt;Gentle brushing against the topic of cryptography with these classical stories  eventually lead me to an interest in modern crypto. Real, crypto! Like most sciences portrayed in popular culture, it really only gets interesting when you get into the technical reading.
Computerphile has several good videos on cryptographic topics. This video describes SHA1 in a way that I find quite interesting. This is just about hashing methods but it is a lovely introduction to crypto.
&lt;a href=&quot;https://www.youtube.com/watch?v=DMtFhACPnTY&quot;&gt;www.youtube.com&lt;/a&gt;
Another video from the series now gets to actual crypto  in the same entertaining way; &lt;a href=&quot;https://www.youtube.com/watch?v=jkV1KEJGKRA&quot;&gt;End to End Encryption (E2EE)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;breaking&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;breaking-crypto&quot;&gt;Breaking crypto&lt;/h1&gt;
&lt;p&gt;Learning the basics of crypto and how it’s broken is best done at the same time. Of course actually breaking the crypto is difficult. But understanding it doesn’t have to be. To learn this you can quickly get the main points about modular arithmetic, exponentiation, and periods in this video.
&lt;a href=&quot;https://www.youtube.com/watch?v=12Q3Mrh03Gk&quot;&gt;Shor’s algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think getting a clear grasp on the topic relies on getting used to modular arithmetic. For example on a clock we use Mod 12. If you get up at 12am and the time is now 1pm well then obviously you have been up for 13 hours. &lt;strong&gt;13 mod 12 = 1&lt;/strong&gt;.
You know just as well that if you get up at 7am and it is now 8pm you have also been up for 13 hours. We can do this in our head very easily, and can do other examples easily too if you get over the initial confusion. &lt;strong&gt;A/B = Q remainder R&lt;/strong&gt;. In some cases we only care about the remainder R. In that case we say: &lt;strong&gt;A modulo B is equal to R&lt;/strong&gt;. Where B is referred to as the modulus (or mod for short).
The only difficulty is when the numbers become quite large.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/what-is-modular-arithmetic&quot;&gt;Here is a page that describes this very well. &lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This video is summed up with the 4 steps. The reason that RSA works is because Step 2, finding the period, takes a very long time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png&quot; alt=&quot;rsa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Quantum computing is expected to dramatically speed up this step.
Another good intro video that has some interesting discussion on Diffie-Hellman key exchange was given at the Chaos Communication Congress:
J. Alex Halderman, Nadia Heninger: Logjam: Diffie-Hellman, discrete logs, the NSA, and you.&lt;/p&gt;

&lt;p&gt;“Earlier this year, we discovered that Diffie-Hellman key exchange – cornerstone of modern cryptography – is less secure in practice than the security community believed. In this talk, we’ll explain how the NSA is likely exploiting this weakness to allow it to decrypt connections to at least 20% of HTTPS websites, 25% of SSH servers, and 66% of IPsec VPNs.”
&lt;a href=&quot;https://www.youtube.com/watch?v=mS8gm-_rJgM&quot;&gt;www.youtube.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Applied Cypto Handbook is a very good technical introduction and probably as far as a general reader will ever want to go.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cacr.uwaterloo.ca/hac/&quot;&gt;http://cacr.uwaterloo.ca/hac/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;aes&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;aes-is-the-most-important-current-encryption-method&quot;&gt;AES is the most important current encryption method&lt;/h1&gt;
&lt;p&gt;This lecture is the perfect intro if you already know what methods are out there.
&lt;a href=&quot;https://www.youtube.com/watch?v=x1v2tX4_dkQ&quot;&gt;www.youtube.com&lt;/a&gt;
The accompanying book is worth the money if you’re looking for a textbook. The table of contents is available on amazon.
&lt;span style=&quot;color: #0000ff;&quot;&gt; &lt;a href=&quot;http://www.crypto-textbook.com&quot;&gt;http://www.crypto-textbook.com.&lt;/a&gt;&lt;/span&gt;
Here is a link to &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Évariste_Galois#Galois_theory&quot;&gt;Galois’ wiki.&lt;/a&gt;&lt;/span&gt;
This might lead you down a wiki rabbit hole learning about interesting maths.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;quantum&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;quantum-computing&quot;&gt;Quantum computing&lt;/h1&gt;
&lt;p&gt;Here are simply two videos from PBS that will be more entertaining and succinct at discussing this really interesting topic than I.
&lt;a href=&quot;https://www.youtube.com/watch?v=IrbJYsep45E&quot;&gt;How quantum computing works&lt;/a&gt;
How might quantum computing destroy computer security?
&lt;a href=&quot;https://www.youtube.com/watch?v=wUwZZaI5u0c&quot;&gt;By utilising Shor’s algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A fun little topic brought up in this videos is: that quantum Fourier transform uses resonance to amplify the basic state associated with the correct period.
If you’re reading this site then it’s likely that you are a biologist.
If that is the case you may be more familiar with protein structures than quantum mechanics.
I first became introduced to the practical application of Fourier transformation while learning nuclear magnetic resonance (NMR) spectroscopy for protein structuring.
Of course, you don’t actually have to learn it to do NMR.
It happens automatically during data analysis but most people in the field surely would still like to know the details.
Wiki has a great page: &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;&gt;https://en.wikipedia.org/wiki/Fourier_transform&lt;/a&gt;
&lt;img src=&quot;https://dylanlawlessblog.files.wordpress.com/2017/02/ft.png&quot; alt=&quot;ft&quot; /&gt;
“In NMR an exponentially shaped free induction decay (FID) signal is acquired in the time domain and Fourier-transformed to a Lorentzian line-shape in the frequency domain.”&lt;/p&gt;

&lt;p&gt;The next main point addressed in this video is: Complex roots of unity.
This is introduced quite well in the video.
If you have never seen anything like this before then I highly recommend the short book by Feynman;
&lt;a href=&quot;https://www.amazon.co.uk/dp/B00BR40XJ6?ref_=k4w_oembed_ICZkE7ckZ2ZUfR&amp;amp;tag=kpembed-20&amp;amp;linkCode=kpd&quot;&gt;QED: The Strange Theory of Light and Matter&lt;/a&gt;
&lt;!--
(https://www.amazon.com/QED-Strange-Princeton-Science-Library/dp/0691164096/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1494067439&amp;sr=1-1&amp;keywords=qed+the+strange+theory+of+light+and+matter)
--&gt;
In no way does this little book talk about quantum computing.
If fact it is pretty old now and is not the kind of thing that professionals will be using for reference.
Why would I suggest this for someone who is new to the topic? Well it is an extremely fun introduction to the topic of QED and lays the foundation of ideas that have become mainstream over the next 30 years.
Understanding some basic ideas will leave you open to recognise more complex applications, especially important if you want to only look at the basics of quantum computing.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;thoughts&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;some-thoughts&quot;&gt;Some thoughts&lt;/h1&gt;
&lt;p&gt;This talk at Google by Peter Warren Singer based on his book,
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Cybersecurity-Cyberwar-Everyone-Needs-Know%C2%AE/dp/0199918112&quot;&gt;Cybersecurity and Cyberwar&lt;/a&gt;&lt;/span&gt;,
may be a pretty interesting watch for anyone into technology security in some way. This is not a technical talk, more of something to get you into the mindset up why this topic may be interesting.
&lt;a href=&quot;https://www.youtube.com/watch?v=h0SXO5KUZIo&quot;&gt;www.youtube.com&lt;/a&gt;
&lt;a href=&quot;https://www.cl.cam.ac.uk/~rja14/book.html&quot;&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;https://www.cl.cam.ac.uk/~rja14/book.html&lt;/span&gt;&lt;/a&gt; Security Engineering — The Book&lt;/p&gt;

&lt;p&gt;The cryptopals crypto challenges are a fun way to learn some hands on application of cryptographic techniques. &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.cryptopals.com/&quot;&gt;http://www.cryptopals.com/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Announcing the first SHA1 collision on February 23, 2017.
This was a really big event in the crypto community.
I think many people in the cyber security field assume that experiments and findings in public and academic research are a few years behind government capabilities.
Take from that what you will.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html&quot;&gt;https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;There are countless reasons why crypto is interesting.
The applications range from the most mundane day to day requirements in the modern world such as banking, personal communication, the use of medical data (which I post about here &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://dylanlawlessblog.wordpress.com/2017/02/21/pretty-good-privacy-for-academic-data/&quot;&gt;Pretty good privacy for academic data&lt;/a&gt;&lt;/span&gt;) all the way out to the most hypothetical academic applications.
An interesting point to think about is the journey that each data packet makes across the mystical &lt;em&gt;internet&lt;/em&gt;.
Most electronic communications travel across a number of boarders and further distances than most people will travel in their entire life.
Our world would not run very smoothly if all communication was sent in a readable format with no protection.
Here is some basic info on the infrastructure require for modern electronic communication:
&lt;a href=&quot;https://www.youtube.com/watch?v=DKHZKTRyzeg&quot;&gt;www.youtube.com&lt;/a&gt;,
&lt;a href=&quot;https://www.youtube.com/watch?v=0TZwiUwZwIE&quot;&gt;www.youtube.com&lt;/a&gt;
And the &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.submarinecablemap.com&quot;&gt;Submarine Cable Map&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;While we’re on the topic, I found this video on the Cornwall cable landing station.
The physical infrastructure and engineering requirements of global communication are sometimes easy to forget if one spends more time on computer programming or mathematics
&lt;a href=&quot;https://www.youtube.com/watch?v=K_nnUbX7uuQ&quot;&gt;www.youtube.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;reading&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;reading-list-on-more-advanced-topics&quot;&gt;Reading list on more advanced topics&lt;/h1&gt;
&lt;p&gt;/r/crypto wiki&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/crypto/wiki/index&quot;&gt;https://www.reddit.com/r/crypto/wiki/index&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Textbook: An Introduction to Mathematical Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Cryptology ePrint Archive&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://eprint.iacr.org&quot;&gt;http://eprint.iacr.org&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Handbook of Applied Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cacr.uwaterloo.ca/hac/&quot;&gt;http://cacr.uwaterloo.ca/hac/&lt;/a&gt;&lt;/span&gt;
Goldreich: The Foundations of Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html&quot;&gt;http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Handbook of Elliptic and Hyperelliptic Curve Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.hyperelliptic.org/HEHCC/&quot;&gt;http://www.hyperelliptic.org/HEHCC/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
eBACS: ECRYPT Benchmarking of Cryptographic Systems&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://bench.cr.yp.to&quot;&gt;http://bench.cr.yp.to&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Mihir Bellare and Shafi Goldwasser’s Lecture Notes&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf&quot;&gt;http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Charm: A tool for rapid cryptographic prototyping&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.charm-crypto.com/index.html&quot;&gt;http://www.charm-crypto.com/index.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
eHash Wiki&lt;br /&gt;
&lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_Hash_Function_Zoo&quot;&gt;Hash Function Zoo&lt;/a&gt;&lt;br /&gt;
and the &lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo&quot;&gt;SHA-3 Zoo&lt;/a&gt;&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page&quot;&gt;http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Cryptology ePrint Archive&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://eprint.iacr.org&quot;&gt;http://eprint.iacr.org&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
IACR Conferences (Crypto, Eurocrypt, Asiacrypt)&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.iacr.org/conferences/&quot;&gt;http://www.iacr.org/conferences/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
IEEE Symposium on Security and Privacy (There are loads of papers and talks on YouTube under Program of past events)&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.ieee-security.org/TC/SP2017/past.html&quot;&gt;https://www.ieee-security.org/TC/SP2017/past.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Crypto Stack Exchange&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://crypto.stackexchange.com&quot;&gt;https://crypto.stackexchange.com&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Blogpost so-you-want-to-crypto&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.seancassidy.me/so-you-want-to-crypto.html&quot;&gt;https://www.seancassidy.me/so-you-want-to-crypto.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Authenticated Encryption Zoo&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo&quot;&gt;https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Helger Lipmaa Cryptology Pointers&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://kodu.ut.ee/~lipmaa/crypto/&quot;&gt;http://kodu.ut.ee/~lipmaa/crypto/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Free Course: Applied Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.udacity.com/course/applied-cryptography--cs387&quot;&gt;https://www.udacity.com/course/applied-cryptography–cs387&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Kerckhoffs’s principle&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle&quot;&gt;https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Schneier’s Law&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.schneier.com/blog/archives/2011/04/schneiers_law.html&quot;&gt;https://www.schneier.com/blog/archives/2011/04/schneiers_law.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
crypto blogs from David Wong’s github&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://github.com/mimoo/crypto_blogs&quot;&gt;https://github.com/mimoo/crypto_blogs&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Shor in Haskell The Quantum IO Monad&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf&quot;&gt;http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
The Quipper Language: programming language for quantum computing&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.mathstat.dal.ca/%7Eselinger/quipper/&quot;&gt;http://www.mathstat.dal.ca/%7Eselinger/quipper/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;!-- 
![encrpytdata](https://i.imgur.com/UubXs0H.gif)
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Snyk - Automatically Scan and Fix Ruby and Nodejs Vulnerabilities</title>
   <link href="http://tom.preston-werner.com/2016/11/10/snyk.html"/>
   <updated>2016-11-10T00:00:00+01:00</updated>
   <id>http://tom.preston-werner.com/2016/11/10/snyk</id>
   <content type="html">&lt;h1 id=&quot;snyk---automatically-scan-and-fix-ruby-and-nodejs-vulnerabilities&quot;&gt;Snyk - Automatically Scan and Fix Ruby and Nodejs Vulnerabilities&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;10 Nov 2016 - San Francisco&lt;/p&gt;

&lt;p&gt;This is a story about a company called
&lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk&lt;/a&gt; (pronounced “sneak”), their
founder Guy Podjarny, my decision to become one of their advisors, and how they
are going to help save you from malevolent agents trying to steal your digital
stuff.&lt;/p&gt;

&lt;p&gt;If you’re anything like me, you’re simultaneously terrified and in awe of the
increasing commonality of large corporate security breaches. Even big names like
Ebay, Home Depot, Anthem, JP Morgan Chase, Target, LinkedIn, Dropbox, and Yahoo
are falling victim to sophisticated attacks. If you spend even a few minutes
looking into it, you’ll be shocked at how frequently these breaches are
happening now. The fine folks at Information is Beautiful have an excellent
interactive visualization of the &lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;World’s Biggest Data
Breaches&lt;/a&gt;
over the last twelve years, in case you want to read all the gory details and
never get a restful night of sleep ever again:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;
  &lt;img src=&quot;/images/posts/2016-11-10/breaches.png&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve used a fair number of emotionally charged words above that might be
triggering your FUD detectors right about now. But be advised: it’s not paranoia
when they really are out to get you. If recent, extremely high profile (and
subsequently weaponized) breaches like those of the Clinton Campaign and the DNC
aren’t enough to make you want to air gap your entire life, then I envy your
steely-eyed mettle and implore you to teach me your meditation techniques.&lt;/p&gt;

&lt;p&gt;The fact is, security is hard. And it’s getting harder every day. To win, you
have to get it right every single time. To lose (and lose big), you only have to
screw it up once.&lt;/p&gt;

&lt;p&gt;During my years at GitHub, I spent a lot of time assembling a dedicated security
team, managing security audits and penetration tests, and working to establish a
culture of security awareness amongst our development team. All of this is
challenging and expensive, especially for a young company. Even worse, it’s the
kind of investment that’s totally invisible when it’s working, making it hard to
sustain until that crucial and terrible moment you end up on the front page of
Hacker News as the latest victim.&lt;/p&gt;

&lt;p&gt;A year ago I was contemplating this, especially the difficult proposition of
having developers, furious at work on new features, constantly maintain
awareness of security vulnerabilities they might be inadvertently weaving into
the product. Web application developers are generally not security experts, and
though I would love to live in a world where that wasn’t true, it’s just not a
realistic expectation. Meanwhile, modern development means an increasing
reliance on 3rd party code. Even a small Rails app will probably have 300 or
more gem dependencies after a few months of development. It’s even more in the
nodejs world. This level of modularization and code reuse, driven by the
explosion of high quality open source over the last decade, is amazing and I
absolutely love it, but it comes at a security expense.&lt;/p&gt;

&lt;p&gt;Open source projects are not known for their excellent security records.
Vulnerabilities like &lt;a href=&quot;http://heartbleed.com/&quot;&gt;Heartbleed&lt;/a&gt; and
&lt;a href=&quot;https://blog.cloudflare.com/inside-shellshock/&quot;&gt;Shellshock&lt;/a&gt; painfully
demonstrate the idea that “given enough eyeballs, all bugs are shallow” is
completely false. In fact, due to a flaw in YAML, Rails had a &lt;a href=&quot;http://blog.codeclimate.com/blog/2013/01/10/rails-remote-code-execution-vulnerability-explained/&quot;&gt;pretty extreme
remote code execution
vulnerability&lt;/a&gt;
for years. If you were running any version of Rails prior to the fix, you were
vulnerable. This stuff is real, and as responsible developers, we need to be
more proactive about it.&lt;/p&gt;

&lt;p&gt;Luckily, at the time I was pondering these matters, I ran into Guy Podjarny. As
a former cofounder of Blaze.io and then CTO of Web Experience at Akamai (which
acquired Blaze.io), Guy intimately understands the impact of security on today’s
web developers. He was working on an automated tool to scan and fix security
vulnerabilities in 3rd party dependencies. I was intrigued. They already had a
way to scan nodejs projects and look for known security vulnerabilities in the
dependency tree and automatically upgrade or patch affected libraries. I thought
this was pretty cool, but it was his vision for what automated security tooling
could be that sold me on him and his company. I can’t talk much about that
now, but just know that what Snyk is today is just the tip of what will
become an intelligent and proactive bodyguard for your entire codebase.&lt;/p&gt;

&lt;p&gt;A few months ago, Snyk released GitHub integration to make it fantastically
simple to hook up your repos to Snyk and, my favorite feature: the ability to
monitor your repo for future vulnerabilities and then &lt;strong&gt;automatically submit a
pull request&lt;/strong&gt; with the suggested package upgrade or hotfix patch (nodejs only for
now).&lt;/p&gt;

&lt;p&gt;Today, &lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk announced support for
Ruby&lt;/a&gt;. Take a look at that blog post,
it does an awesome job of explaining how simple it is to set up and what the
generated pull requests look like. It’s totally free for open source projects,
and extremely cheap insurance for your important projects.&lt;/p&gt;

&lt;p&gt;Make no mistake, 3rd party code is a clear and present danger to your business.
If you don’t know if you’re vulnerable, then you must assume that you are and
take steps to protect yourself. Snyk makes it easy.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Replicated - An Easier Path from SaaS to Enterprise</title>
   <link href="http://tom.preston-werner.com/2015/06/19/replicated.html"/>
   <updated>2015-06-19T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2015/06/19/replicated</id>
   <content type="html">&lt;h1 id=&quot;replicated---an-easier-path-from-saas-to-enterprise&quot;&gt;Replicated - An Easier Path from SaaS to Enterprise&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;19 Jun 2015 - San Francisco&lt;/p&gt;

&lt;p&gt;Over the last year I’ve had a chance to learn a lot more about early stage
funding and made angel investments in a handful of startups. So far I’ve
restricted my involvement to companies with ideas in which I have significant
domain knowledge. I also insist on founders with relentless product focus, a
fierce desire to help their customers be more awesome, and excellent
communication skills. I recently met just such a company, with just such
founders, tackling a problem that has personally caused me much pain.&lt;/p&gt;

&lt;p&gt;The company is &lt;a href=&quot;http://replicated.com&quot;&gt;Replicated&lt;/a&gt;, and founders Grant Miller
and Marc Campbell are making it easier to roll out an on-prem Enterprise
offering based on an existing cloud-based SaaS product.&lt;/p&gt;

&lt;p&gt;At GitHub, we burned through &lt;strong&gt;a lot&lt;/strong&gt; of developer cycles building our own
installer (several times), securing the installation environment, coding an
automated licensing management system, integrating single sign-on services
(LDAP, Active Directory, CAS, etc, etc), building out a searchable audit system,
supporting customer-reviewable support bundles (logs and other diagnostic
output), allowing numerous backup strategies, and countless other
Enterprise-specific features that were killing our Enterprise deals. All of this
on top of hiring and building out the necessary sales, support, and accounting
teams to create a smooth Enterprise experience for our customers.&lt;/p&gt;

&lt;p&gt;Replicated provides common Enterprise functionality (much of what I mentioned
above, and all of it eventually) that you can wrap around your SaaS product,
resulting in a first-class on-prem product in a fraction of the time. Beyond
just technology, Replicated will help you understand your Enterprise customers
through documentation on best practices and insight into the requirements and
reasons that large companies desire the features they do. Until you can
empathise with your customer (which is very hard to do as a fast-moving SaaS
startup), you’ll never build the best product possible.&lt;/p&gt;

&lt;p&gt;Getting into the Enterprise market will always be hard. But by reducing the
technology burden, Replicated plans to erase much of the pain so you can focus
on the other human-centric tasks. Not only am I an investor in Replicated, I
believe in their mission and their founders so much that I’ve joined as an
advisor. I understand what the uphill slog of the SaaS to Enterprise climb feels
like, and I’m going to do my best to ensure you don’t have to suffer it as much
as I did.&lt;/p&gt;

&lt;p&gt;I’m also pleased to announce that Travis CI is now shipping their Enterprise
product using Replicated. To see what the installation process is like, &lt;a href=&quot;https://www.youtube.com/watch?v=ViN-qkcovL0&amp;amp;feature=youtu.be&quot;&gt;watch
Grant install Travis CI
Enterprise&lt;/a&gt; on a
fresh server in about seven minutes. For a deeper dive, Travis CI has also
published a blog post covering some of their process in &lt;a href=&quot;http://blog.travis-ci.com/2015-06-19-how-we-improved-travis-ci-installation/&quot;&gt;getting their
Enterprise installer ready using
Replicated&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the coming weeks, you’ll start to see other well-known startups launching (or
re-launching) Enterprise versions of their SaaS software on top of Replicated.
If you’re looking to do the same, and want to save yourself a lot of heartache,
email &lt;a href=&quot;mailto:contact@replicated.com&quot;&gt;contact@replicated.com&lt;/a&gt;, and start
focusing on what matters the most: your unique and kickass product.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Farewell GitHub, Hello Immersive Computing</title>
   <link href="http://tom.preston-werner.com/2014/04/21/farewell-github-hello-immersive-computing.html"/>
   <updated>2014-04-21T00:00:00+02:00</updated>
   <id>http://tom.preston-werner.com/2014/04/21/farewell-github-hello-immersive-computing</id>
   <content type="html">&lt;h1 id=&quot;farewell-github-hello-immersive-computing&quot;&gt;Farewell GitHub, Hello Immersive Computing&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;21 Apr 2014 - New York City&lt;/p&gt;

&lt;p&gt;Today is my last day at GitHub. Recent events have given me a lot of time to
reflect on what’s important to me, and I’ve decided to switch gears and focus on
building something from scratch again. Since visiting the Oculus VR team at
their office three months ago, I’ve come to believe that immersive computing
(aka virtual reality) is poised to rival the personal computer, the web, social
networking, and mobile devices in its impact. While the timing is more abrupt
than I had intended, with everything that’s happened, I think now is the right
time to do this, and I’d like to explain why.&lt;/p&gt;

&lt;p&gt;First, I want to address the serious accusations that were made against me and
my family over the past month. With every decision I made at GitHub and in every
interaction I had with employees, I tried to treat people better than they
expected and to resolve conflict with empathy. Despite that, I’ve made mistakes,
and I am deeply sorry to anyone who was hurt by those mistakes. It devastates me
to know that I missed the mark, and I will strive to do better, every day.&lt;/p&gt;

&lt;p&gt;That said, I want to be very clear about one thing: neither my wife, Theresa,
nor I have ever engaged in gender-based harassment or discrimination. The
results of GitHub’s independent investigation unequivocally confirm this and we
are prepared to fight any further false claims on this matter to the full extent
of the law. I believe in diversity and equality for all people in all
professions, especially the tech sector. It’s immensely important to me and I
will continue to do my very best to further that belief.&lt;/p&gt;

&lt;p&gt;Unfortunately, the investigation and all the attention surrounding it have me
concerned that remaining at GitHub would be a distraction for both me and the
company. I’m incredibly proud of what I’ve helped build at GitHub and I don’t
want the events of the past month to jeopardize that. I care too much about the
company and the people here to let that happen. The GitHub team is incredibly
strong, with fierce vision, and I have no doubt they will continue to
revolutionize software development for decades to come. Founding and building
GitHub has been the greatest adventure of my life. I’ve been so lucky to be on
this journey with such amazing, helpful, talented, and real people. I’m going to
miss working with such a great team, but I’m also insanely excited about the
future.&lt;/p&gt;

&lt;p&gt;Since the early days of GitHub, I’ve wanted to create a different kind of
business. One that was &lt;a href=&quot;http://tom.preston-werner.com/2010/10/18/optimize-for-happiness.html&quot;&gt;Optimized for
Happiness&lt;/a&gt;
and built atop a &lt;a href=&quot;https://www.youtube.com/watch?v=i0FzgvYxdvk&quot;&gt;Framework of
Happiness&lt;/a&gt;. One where great people
could work on hard problems together to create unbelievably good products. I
believe I was able to achieve a great deal of success with that model at GitHub,
even if things didn’t always go perfectly according to plan. All of this has
been a tremendous learning experience for me.&lt;/p&gt;

&lt;p&gt;Last January I stepped down as CEO and handed that role over to cofounder Chris
Wanstrath so I could focus on future-facing R&amp;amp;D projects with small teams. This
kind of rapid, team-based innovation is what I live for. During my time away
from GitHub I started experimenting with Go, OpenGL, and Unity with an eye
towards the software side of immersive computing. It felt really good to get
back into a code editor and challenge the deeply logical and analytical part of
my brain. I’ve enjoyed the challenges of learning how to lead a company with
hundreds of people, but it’s very hard for me to deny the allure of coding a
system that could once again change the course of history.&lt;/p&gt;

&lt;p&gt;I’m telling you this because I think stealth mode is bullshit and if you feel
the same way I do about immersive computing then I want to talk with you about
it. For the next few months I’m going to be living in Manhattan. My wife,
Theresa, is currently participating in Techstars NYC as their very first
nonprofit. Her startup, &lt;a href=&quot;https://omakasecharity.org/&quot;&gt;The Omakase Charity&lt;/a&gt;,
helps donors learn about and support nonprofits that are changing the world with
technology. She’s one of the strongest and most thoughtful women I know, and I’m
hoping to help her succeed with her mission while I’m here.&lt;/p&gt;

&lt;p&gt;Thank you to everyone that reached out to me over the last month, including the
generous team at Andreessen Horowitz. Your support has made a huge difference
and I’m truly excited for what’s next.&lt;/p&gt;
</content>
 </entry>
 

 <!--
 
  <h2> - </h2>
  <p><h1 id="cryptography">Cryptography</h1>
<p class="meta">28 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#cryptography" id="markdown-toc-cryptography">Cryptography</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#breaking-crypto" id="markdown-toc-breaking-crypto">Breaking crypto</a></li>
  <li><a href="#aes-is-the-most-important-current-encryption-method" id="markdown-toc-aes-is-the-most-important-current-encryption-method">AES is the most important current encryption method</a></li>
  <li><a href="#quantum-computing" id="markdown-toc-quantum-computing">Quantum computing</a></li>
  <li><a href="#some-thoughts" id="markdown-toc-some-thoughts">Some thoughts</a></li>
  <li><a href="#reading-list-on-more-advanced-topics" id="markdown-toc-reading-list-on-more-advanced-topics">Reading list on more advanced topics</a></li>
</ul>

<!--
cross link and label
1. [Introduction](#introduction)
<a name="introduction"></a>
-->
<h1 id="introduction">Introduction</h1>
<p>Like most of the posts on this blog, this will be a work in progress. Cryptography is a topic which I stumbled upon and really enjoy.
For the reading list skip to the end of this page.
There is a long interesting history which would appeal to a casual reader.
Most people would be familiar with stories about crypto during WWII, particularly because of movies like
<span style="color: #0000ff;"><a href="http://www.imdb.com/title/tt2084970/">The Imitation Game<span id="titleYear">(2014).
</span></a></span>
Cracking of Enigma falls into the espionage theme along with stories like that of books from  <span class="author notFaded" style="color: #0000ff;"><span class="a-declarative"><a href="https://www.amazon.co.uk/Ben-Macintyre/e/B001H6WAL8/ref=dp_byline_cont_book_1">Ben Macintyre.</a> </span></span>One of my favorites is: <span id="productTitle" class="a-size-large"><span style="color: #0000ff;"><a href="https://www.amazon.co.uk/d/Books/Operation-Mincemeat-True-Story-Changed-Course-World/1408809214">Operation Mincemeat</a>:</span> The True Spy Story That Changed the Course of World War II. </span>
Reading so much about the non-fiction side of this topic ultimately led me to the <span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/List_of_James_Bond_novels_and_short_stories">Ian Flemming novels</a>.</span> Of course I had seen all the movies as a kid, and like most, loved them. Reading the novels in their order of release ended up being much more fun than I have with most fictional book series perhaps because of Flemming’s true involvements during WWII.</p>

<p>Gentle brushing against the topic of cryptography with these classical stories  eventually lead me to an interest in modern crypto. Real, crypto! Like most sciences portrayed in popular culture, it really only gets interesting when you get into the technical reading.
Computerphile has several good videos on cryptographic topics. This video describes SHA1 in a way that I find quite interesting. This is just about hashing methods but it is a lovely introduction to crypto.
<a href="https://www.youtube.com/watch?v=DMtFhACPnTY">www.youtube.com</a>
Another video from the series now gets to actual crypto  in the same entertaining way; <a href="https://www.youtube.com/watch?v=jkV1KEJGKRA">End to End Encryption (E2EE)</a>.</p>

<p><a name="breaking"></a></p>
<h1 id="breaking-crypto">Breaking crypto</h1>
<p>Learning the basics of crypto and how it’s broken is best done at the same time. Of course actually breaking the crypto is difficult. But understanding it doesn’t have to be. To learn this you can quickly get the main points about modular arithmetic, exponentiation, and periods in this video.
<a href="https://www.youtube.com/watch?v=12Q3Mrh03Gk">Shor’s algorithm</a></p>

<p>I think getting a clear grasp on the topic relies on getting used to modular arithmetic. For example on a clock we use Mod 12. If you get up at 12am and the time is now 1pm well then obviously you have been up for 13 hours. <strong>13 mod 12 = 1</strong>.
You know just as well that if you get up at 7am and it is now 8pm you have also been up for 13 hours. We can do this in our head very easily, and can do other examples easily too if you get over the initial confusion. <strong>A/B = Q remainder R</strong>. In some cases we only care about the remainder R. In that case we say: <strong>A modulo B is equal to R</strong>. Where B is referred to as the modulus (or mod for short).
The only difficulty is when the numbers become quite large.
<span style="color: #0000ff;"><a href="https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/what-is-modular-arithmetic">Here is a page that describes this very well. </a></span></p>

<p>This video is summed up with the 4 steps. The reason that RSA works is because Step 2, finding the period, takes a very long time:</p>

<!-- ![rsa](https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png) -->
<p><img src="https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png" width="40%" /></p>

<p>Quantum computing is expected to dramatically speed up this step.
Another good intro video that has some interesting discussion on Diffie-Hellman key exchange was given at the Chaos Communication Congress:
J. Alex Halderman, Nadia Heninger: Logjam: Diffie-Hellman, discrete logs, the NSA, and you.</p>

<p>“Earlier this year, we discovered that Diffie-Hellman key exchange – cornerstone of modern cryptography – is less secure in practice than the security community believed. In this talk, we’ll explain how the NSA is likely exploiting this weakness to allow it to decrypt connections to at least 20% of HTTPS websites, 25% of SSH servers, and 66% of IPsec VPNs.”
<a href="https://www.youtube.com/watch?v=mS8gm-_rJgM">www.youtube.com</a></p>

<p>Applied Cypto Handbook is a very good technical introduction and probably as far as a general reader will ever want to go.
<span style="color: #0000ff;"><a href="http://cacr.uwaterloo.ca/hac/">http://cacr.uwaterloo.ca/hac/</a></span></p>

<p><a name="aes"></a></p>
<h1 id="aes-is-the-most-important-current-encryption-method">AES is the most important current encryption method</h1>
<p>This lecture is the perfect intro if you already know what methods are out there.
<a href="https://www.youtube.com/watch?v=x1v2tX4_dkQ">www.youtube.com</a>
The accompanying book is worth the money if you’re looking for a textbook. The table of contents is available on amazon.
<span style="color: #0000ff;"> <a href="http://www.crypto-textbook.com">http://www.crypto-textbook.com.</a></span>
Here is a link to <span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/Évariste_Galois#Galois_theory">Galois’ wiki.</a></span>
This might lead you down a wiki rabbit hole learning about interesting maths.</p>

<p><a name="quantum"></a></p>
<h1 id="quantum-computing">Quantum computing</h1>
<p>Here are simply two videos from PBS that will be more entertaining and succinct at discussing this really interesting topic than I.
<a href="https://www.youtube.com/watch?v=IrbJYsep45E">How quantum computing works</a>
How might quantum computing destroy computer security?
<a href="https://www.youtube.com/watch?v=wUwZZaI5u0c">By utilising Shor’s algorithm</a></p>

<p>A fun little topic brought up in this videos is: that quantum Fourier transform uses resonance to amplify the basic state associated with the correct period.
If you’re reading this site then it’s likely that you are a biologist.
If that is the case you may be more familiar with protein structures than quantum mechanics.
I first became introduced to the practical application of Fourier transformation while learning nuclear magnetic resonance (NMR) spectroscopy for protein structuring.
Of course, you don’t actually have to learn it to do NMR.
It happens automatically during data analysis but most people in the field surely would still like to know the details.
Wiki has a great page: <a href="https://en.wikipedia.org/wiki/Fourier_transform">https://en.wikipedia.org/wiki/Fourier_transform</a></p>

<p><img src="https://dylanlawlessblog.files.wordpress.com/2017/02/ft.png" width="50%" /><br />
“In NMR an exponentially shaped free induction decay (FID) signal is acquired in the time domain and Fourier-transformed to a Lorentzian line-shape in the frequency domain.”</p>

<p>The next main point addressed in this video is: Complex roots of unity.
This is introduced quite well in the video.
If you have never seen anything like this before then I highly recommend the short book by Feynman;
<a href="https://www.amazon.co.uk/dp/B00BR40XJ6?ref_=k4w_oembed_ICZkE7ckZ2ZUfR&amp;tag=kpembed-20&amp;linkCode=kpd">QED: The Strange Theory of Light and Matter</a>
<!--
(https://www.amazon.com/QED-Strange-Princeton-Science-Library/dp/0691164096/ref=sr_1_1?s=books&ie=UTF8&qid=1494067439&sr=1-1&keywords=qed+the+strange+theory+of+light+and+matter)
-->
In no way does this little book talk about quantum computing.
If fact it is pretty old now and is not the kind of thing that professionals will be using for reference.
Why would I suggest this for someone who is new to the topic? Well it is an extremely fun introduction to the topic of QED and lays the foundation of ideas that have become mainstream over the next 30 years.
Understanding some basic ideas will leave you open to recognise more complex applications, especially important if you want to only look at the basics of quantum computing.</p>

<p><a name="thoughts"></a></p>
<h1 id="some-thoughts">Some thoughts</h1>
<p>This talk at Google by Peter Warren Singer based on his book,
<span style="color: #0000ff;"><a href="https://www.amazon.com/Cybersecurity-Cyberwar-Everyone-Needs-Know%C2%AE/dp/0199918112">Cybersecurity and Cyberwar</a></span>,
may be a pretty interesting watch for anyone into technology security in some way. This is not a technical talk, more of something to get you into the mindset up why this topic may be interesting.
<a href="https://www.youtube.com/watch?v=h0SXO5KUZIo">www.youtube.com</a>
<a href="https://www.cl.cam.ac.uk/~rja14/book.html"><span style="color: #0000ff;">https://www.cl.cam.ac.uk/~rja14/book.html</span></a> Security Engineering — The Book</p>

<p>The cryptopals crypto challenges are a fun way to learn some hands on application of cryptographic techniques. <span style="color: #0000ff;"><a href="http://www.cryptopals.com/">http://www.cryptopals.com/</a></span></p>

<p>Announcing the first SHA1 collision on February 23, 2017.
This was a really big event in the crypto community.
I think many people in the cyber security field assume that experiments and findings in public and academic research are a few years behind government capabilities.
Take from that what you will.
<span style="color: #0000ff;"><a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html">https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html</a></span></p>

<p>There are countless reasons why crypto is interesting.
The applications range from the most mundane day to day requirements in the modern world such as banking, personal communication, the use of medical data (which I post about here <span style="color: #0000ff;"><a href="https://dylanlawlessblog.wordpress.com/2017/02/21/pretty-good-privacy-for-academic-data/">Pretty good privacy for academic data</a></span>) all the way out to the most hypothetical academic applications.
An interesting point to think about is the journey that each data packet makes across the mystical <em>internet</em>.
Most electronic communications travel across a number of boarders and further distances than most people will travel in their entire life.
Our world would not run very smoothly if all communication was sent in a readable format with no protection.
Here is some basic info on the infrastructure require for modern electronic communication:
<a href="https://www.youtube.com/watch?v=DKHZKTRyzeg">www.youtube.com</a>,
<a href="https://www.youtube.com/watch?v=0TZwiUwZwIE">www.youtube.com</a>
And the <span style="color: #0000ff;"><a href="http://www.submarinecablemap.com">Submarine Cable Map</a></span>.</p>

<p>While we’re on the topic, I found this video on the Cornwall cable landing station.
The physical infrastructure and engineering requirements of global communication are sometimes easy to forget if one spends more time on computer programming or mathematics
<a href="https://www.youtube.com/watch?v=K_nnUbX7uuQ">www.youtube.com</a>.</p>

<p><a name="reading"></a></p>
<h1 id="reading-list-on-more-advanced-topics">Reading list on more advanced topics</h1>
<p>/r/crypto wiki<br />
<span style="color: #0000ff;"><a href="https://www.reddit.com/r/crypto/wiki/index">https://www.reddit.com/r/crypto/wiki/index</a></span><br />
Textbook: An Introduction to Mathematical Cryptography<br />
<span style="color: #0000ff;"><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;rep=rep1&amp;type=pdf</a></span><br />
Cryptology ePrint Archive<br />
<span style="color: #0000ff;"><a href="http://eprint.iacr.org">http://eprint.iacr.org</a></span><br />
Handbook of Applied Cryptography<br />
<span style="color: #0000ff;"><a href="http://cacr.uwaterloo.ca/hac/">http://cacr.uwaterloo.ca/hac/</a></span>
Goldreich: The Foundations of Cryptography<br />
<span style="color: #0000ff;"><a href="http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html">http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html</a></span><br />
Handbook of Elliptic and Hyperelliptic Curve Cryptography<br />
<span style="color: #0000ff;"><a href="http://www.hyperelliptic.org/HEHCC/">http://www.hyperelliptic.org/HEHCC/</a></span><br />
eBACS: ECRYPT Benchmarking of Cryptographic Systems<br />
<span style="color: #0000ff;"><a href="http://bench.cr.yp.to">http://bench.cr.yp.to</a></span><br />
Mihir Bellare and Shafi Goldwasser’s Lecture Notes<br />
<span style="color: #0000ff;"><a href="http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf">http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf</a></span><br />
Charm: A tool for rapid cryptographic prototyping<br />
<span style="color: #0000ff;"><a href="http://www.charm-crypto.com/index.html">http://www.charm-crypto.com/index.html</a></span><br />
eHash Wiki<br />
<a href="http://ehash.iaik.tugraz.at/wiki/The_Hash_Function_Zoo">Hash Function Zoo</a><br />
and the <a href="http://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo">SHA-3 Zoo</a><br />
<span style="color: #0000ff;"><a href="http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page">http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page</a></span><br />
Cryptology ePrint Archive<br />
<span style="color: #0000ff;"><a href="http://eprint.iacr.org">http://eprint.iacr.org</a></span><br />
IACR Conferences (Crypto, Eurocrypt, Asiacrypt)<br />
<span style="color: #0000ff;"><a href="http://www.iacr.org/conferences/">http://www.iacr.org/conferences/</a></span><br />
IEEE Symposium on Security and Privacy (There are loads of papers and talks on YouTube under Program of past events)<br />
<span style="color: #0000ff;"><a href="https://www.ieee-security.org/TC/SP2017/past.html">https://www.ieee-security.org/TC/SP2017/past.html</a></span><br />
Crypto Stack Exchange<br />
<span style="color: #0000ff;"><a href="https://crypto.stackexchange.com">https://crypto.stackexchange.com</a></span><br />
Blogpost so-you-want-to-crypto<br />
<span style="color: #0000ff;"><a href="https://www.seancassidy.me/so-you-want-to-crypto.html">https://www.seancassidy.me/so-you-want-to-crypto.html</a></span><br />
Authenticated Encryption Zoo<br />
<span style="color: #0000ff;"><a href="https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo">https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo</a></span><br />
Helger Lipmaa Cryptology Pointers<br />
<span style="color: #0000ff;"><a href="http://kodu.ut.ee/~lipmaa/crypto/">http://kodu.ut.ee/~lipmaa/crypto/</a></span><br />
Free Course: Applied Cryptography<br />
<span style="color: #0000ff;"><a href="https://www.udacity.com/course/applied-cryptography--cs387">https://www.udacity.com/course/applied-cryptography–cs387</a></span><br />
Kerckhoffs’s principle<br />
<span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle">https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle</a></span><br />
Schneier’s Law<br />
<span style="color: #0000ff;"><a href="https://www.schneier.com/blog/archives/2011/04/schneiers_law.html">https://www.schneier.com/blog/archives/2011/04/schneiers_law.html</a></span><br />
crypto blogs from David Wong’s github<br />
<span style="color: #0000ff;"><a href="https://github.com/mimoo/crypto_blogs">https://github.com/mimoo/crypto_blogs</a></span><br />
Shor in Haskell The Quantum IO Monad<br />
<span style="color: #0000ff;"><a href="http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf">http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf</a></span><br />
The Quipper Language: programming language for quantum computing<br />
<span style="color: #0000ff;"><a href="http://www.mathstat.dal.ca/%7Eselinger/quipper/">http://www.mathstat.dal.ca/%7Eselinger/quipper/</a></span></p>

<!-- 
![encrpytdata](https://i.imgur.com/UubXs0H.gif)
-->
<p><small></small></p>

<p>&lt;/small&gt;</p>
</p>

  <h2> - </h2>
  <p><h1 id="rag">RAG</h1>
<ul id="markdown-toc">
  <li><a href="#rag" id="markdown-toc-rag">RAG</a></li>
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#methods" id="markdown-toc-methods">Methods</a>    <ul>
      <li><a href="#population-genetics-and-data-sources" id="markdown-toc-population-genetics-and-data-sources">Population genetics and data sources</a></li>
      <li><a href="#data-processing" id="markdown-toc-data-processing">Data processing</a></li>
      <li><a href="#raw-data-availability-and-analysis-script" id="markdown-toc-raw-data-availability-and-analysis-script">Raw data availability and analysis script</a></li>
      <li><a href="#data-visualisation" id="markdown-toc-data-visualisation">Data visualisation</a></li>
      <li><a href="#validation-of-mrf-against-functional-data" id="markdown-toc-validation-of-mrf-against-functional-data">Validation of MRF against functional data</a></li>
    </ul>
  </li>
  <li><a href="#results" id="markdown-toc-results">Results</a>    <ul>
      <li><a href="#rag1-and-rag2-conservation-and-mutation-rate-residue-frequency" id="markdown-toc-rag1-and-rag2-conservation-and-mutation-rate-residue-frequency">RAG1 and RAG2 conservation and mutation rate residue frequency</a></li>
      <li><a href="#mrf-scores-select-for-confirmed-variants-in-human-disease" id="markdown-toc-mrf-scores-select-for-confirmed-variants-in-human-disease">MRF scores select for confirmed variants in human disease</a></li>
      <li><a href="#top-candidate-variants-require-validation" id="markdown-toc-top-candidate-variants-require-validation">Top candidate variants require validation</a></li>
      <li><a href="#false-positives-in-transib-domains-do-not-negatively-impact-prediction" id="markdown-toc-false-positives-in-transib-domains-do-not-negatively-impact-prediction">False positives in <em>Transib</em> domains do not negatively impact prediction</a></li>
      <li><a href="#mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants" id="markdown-toc-mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants">MRF predicts RAG deficiency amongst PID patients harbouring rare variants</a></li>
      <li><a href="#mrf-supplements-pathogenicity-prediction-tools-for-translational-research" id="markdown-toc-mrf-supplements-pathogenicity-prediction-tools-for-translational-research">MRF supplements pathogenicity prediction tools for translational research</a></li>
      <li><a href="#figure" id="markdown-toc-figure">Figure</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ul>
      <li><a href="#main-supplemental-data-table" id="markdown-toc-main-supplemental-data-table">Main supplemental data table</a></li>
      <li><a href="#clinical-relevance-of-top-candidates" id="markdown-toc-clinical-relevance-of-top-candidates">Clinical relevance of top candidates</a></li>
      <li><a href="#supplemental-analysis-tables" id="markdown-toc-supplemental-analysis-tables">Supplemental analysis tables</a></li>
      <li><a href="#protein-structure-application" id="markdown-toc-protein-structure-application">Protein structure application</a></li>
      <li><a href="#median-cadd-score-per-residue" id="markdown-toc-median-cadd-score-per-residue">Median CADD score per residue</a></li>
      <li><a href="#supplemental-file" id="markdown-toc-supplemental-file">Supplemental file</a></li>
      <li><a href="#genome-wide-and-disease-specific-application" id="markdown-toc-genome-wide-and-disease-specific-application">Genome-wide and disease-specific application</a></li>
      <li><a href="#bayesian-probability" id="markdown-toc-bayesian-probability">Bayesian probability</a></li>
    </ul>
  </li>
</ul>

<h1 id="abstract">Abstract</h1>
<p>While widespread genome sequencing ushers in a new era of preventive medicine, the tools for predictive genomics are still lacking.
Time and resource limitations mean that human diseases remain uncharacterised because of an inability to predict clinically relevant genetic variants.
A strategy of targeting highly conserved protein regions is used commonly in functional studies. 
However, this benefit is lost for rare diseases where the attributable genes are mostly conserved. 
An immunological disorder exemplifying this challenge occurs through damaging mutations in <em>RAG1</em> and <em>RAG2</em> which presents at an early age with a distinct phenotype of life-threatening immunodeficiency or autoimmunity. 
Many tools exist for variant pathogenicity prediction but these cannot account for the probability of variant occurrence. 
Here, we present a method that predicts the likelihood of mutation for every amino acid residue in the RAG1 and RAG2 proteins. 
Population genetics data from approximately 146,000 individuals was used for rare variant
analysis. 
Forty-four known pathogenic variants reported in patients and recombination activity measurements from 110 RAG1/2 mutants were used to
validate calculated scores. 
Probabilities were compared with 98
currently known human cases of disease. 
A genome sequence dataset of 558
patients who have primary immunodeficiency but that are negative for RAG
deficiency were also used as validation controls. 
We compared the
difference between mutation likelihood and pathogenicity prediction. 
Our
method builds a map of most probable mutations allowing pre-emptive
functional analysis. 
This method may be applied to other diseases with
hopes of improving preparedness for clinical diagnosis.</p>

<p>Authors:
Dylan Lawless,
Hana Lango Allen,
James Thaventhiran,
‘NIHR BioResource–Rare Diseases Consortium’,
Flavia Hodel,
Rashida Anwar,
Jacques Fellay,
Jolan E. Walter’,
Sinisa Savic.</p>

<h4 id="ethics-statement">Ethics statement</h4>

<p>The study was performed in accordance with the Declaration of Helsinki.
The NIHR BioResource projects were approved by Research Ethics
Committees in the UK and appropriate national ethics authorities in
non-UK enrolment centres.</p>

<h4 id="abbreviations">Abbreviations</h4>

<p>BCR (B-cell receptor), CADD (combined annotation dependent depletion),
CID-G/A (combined immunodeficiency with granuloma and/or autoimmunity),
GWAS (genome-wide association studies), HGMD (human gene mutation
database), \({M}_{r}\) (mutation rate), MRF (mutation rate residue
frequency), PID (primary immunodeficiency), pLI (probability of being
loss-of-function intolerant), <em>RAG1</em> (recombination activating gene 1),
\({R}_{f}\) (residue frequency), RNH (RNase H), RSS (recombination signal
sequence), SCID (severe combined immunodeficiency), TCR (T-cell
receptor).</p>

<h1 id="introduction">Introduction</h1>

<p>Costs associated with genomic investigations continue to reduce [payne2018cost] while the richness of data generated increases.
Globally, the adoption of wide scale genome sequencing implies that all new-born infants may receive screening for pathogenic genetic variants
in an asymptomatic stage, pre-emptively  [kwan2014newborn]. 
The one dimensionality of individual genomes is now being expanded by the possibility of massive parallel sequencing for somatic variant analysis and by single-cell or lineage-specific genotyping; culminating in a genotype spectrum. 
In whole blood, virtually every nucleotide position may be mutated across \(10^5\) cells 
[Liggett208066]. 
Mapping one’s genotype across multiple cell types and at several periods during a person’s life may soon be feasible 
[clark2018scnmt]. 
Such genotype snapshots might allow for prediction and tracking of somatic, epigenetic, and transcriptomic profiling.</p>

<p>The predictive value of genomic screening highly depends on the computation tools used for data analysis and its correlation with functional assays or prior clinical experience. 
Interpretation of that data is especially challenging for rare human genetic disorders;
candidate disease-causing variants that are predicted as pathogenic
often require complex functional investigations to confirm their significance. 
There is a need for predictive genomic modelling with aims to provide a reliable guidance for therapeutic intervention for patients harbouring genetic defects for life-threatening disease before the illness becomes clinically significant.</p>

<p>The study of predictive genomics is exemplified by consideration of gene essentiality, accomplished by observing intolerance to loss-of-function variants. 
Several gene essentiality scoring methods are available for both the coding and non-coding genome  [bartha2017human]. 
Approximately 3,000 human genes cannot tolerate the loss of one allele
[bartha2017human]. 
The greatest hurdle in monogenic disease is the interpretation of variants of unknown significance while functional validation is a major time and cost investment for laboratories investigating rare disease.</p>

<p>Severe, life-threatening immune diseases are caused by genetic
variations in almost 300 genes
[picard2018international conley2014discovery] however, only a small percentage of disease causing variants have been characterised using functional studies. 
Several robust tools are in common usage for predicting variant pathogenicity.  Compared to methods for pathogenicity prediction, a void remains for predicting mutation probability, essential for efficient pre-emptive validation. 
Our investigation aims to apply predictive genomics as a tool to identify genetic variants that are most likely to be seen in patient cohorts.</p>

<p>We present the first application of our novel approach of predictive genomics using Recombination activating gene 1 (RAG1) and RAG2 deficiency as a model for a rare primary immunodeficiency (PID) caused by autosomal recessive variants. 
<em>RAG1</em> and <em>RAG2</em> encode lymphoid-specific proteins that are essential for V(D)J recombination.
This genetic recombination mechanism is essential for a robust immune response by diversification the T and B cell repertoire in the thymus and bone marrow, respectively  [schatz1989v oettinger1990rag].
Deficiency of RAG1  [mombaerts1992rag] and RAG2  [shinkai1992rag] in mice causes inhibition of B and T cell development. 
[schwarz1996rag ]
formed the first publication reporting that RAG mutations in humans causes severe combined immunodeficiency (SCID), and deficiency in peripheral B and T cells. 
Patient studies identified a form of immune dysregulation known as Omenn syndrome
[de1991restricted rieux1998highly]. 
The patient phenotype includes multi-organ infiltration with oligoclonal, activated T cells. 
The first reported cases of Omenn syndrome identified infants with hypomophic RAG variants which retained partial recombination activity
[villa1998partial]. 
RAG deficiency can be measured by in vitro quantification of recombination activity
[lawless2018prevalence lee2014systematic tirosh2018recombination].
Hypomorphic <em>RAG1</em> and <em>RAG2</em> mutations, responsible for residual V(D)J recombination activity (on average 5-30%), result in a distinct phenotype of combined immunodeficiency with granuloma and/or autoimmunity (CID-G/A)
[kwan2014newborn walter2015broad schuetz2008immunodeficiency].</p>

<p>Human RAG deficiency has traditionally been identified at very early ages due to the rapid drop of maternally-acquired antibody in the first six months of life. 
A loss of adequate lymphocyte development quickly results in compromised immune responses. 
More recently, we have found that RAG deficiency is also found for some adults living with PID
[lawless2018prevalence].</p>

<p><em>RAG1</em> and <em>RAG2</em> are highly conserved genes but disease is only reported with autosomal recessive inheritance. 
Only 44% of amino acids in RAG1 and RAG2 are reported as mutated on GnomAD and functional validation of candidate variants is difficult 
[lek2016analysis].
Pre-emptive selection of residues for functional validation is a major challenge; 
a selection based on low allele frequency alone is infeasible since the majority of each gene is highly conserved. 
A shortened time between genetic analysis and diagnosis means that treatments may be delivered earlier. 
RAG deficiency may present with diverse phenotypes and treatment strategies vary. 
With such tools, early intervention may be prompted. 
Some patients could benefit from hematopoietic stem cell transplant  [john2016unrelated] when necessary while others may be provided mechanism-based treatment  [casanova2014guidelines]. 
Here, we provide a new method for predictive scoring that was validated against
groups of functional assay values, human disease cases, and population genetics data. 
We present the list of variants most likely seen as future determinants of RAG deficiency, meriting functional investigation.</p>

<h1 id="methods">Methods</h1>
<h2 id="population-genetics-and-data-sources">Population genetics and data sources</h2>

<p>GnomAD (version r2.0.2)  [lek2016analysis] was queried for the canonical transcripts of <em>RAG1</em> and <em>RAG2</em> from population genetics data of approximately 146,000 individuals; ENST00000299440 (<em>RAG1</em>) 1586 variants, GRCh37 11:36532259-36614706 and ENST00000311485 (<em>RAG2</em>) 831 variants, GRCh37 11:36597124 - 36619829. 
Data was filtered to contain the variant effect identifiers: frameshift, inframe deletion, inframe
insertion, missense, stop lost, or stop gained. 
Reference transcripts were sourced from Ensembl in the FASTA format amino acid sequence for transcript RAG1-201 ENST00000299440.5 HGNC:9831 and transcript RAG2-201 ENST00000311485.7 HGNC:9832.
These sequences were converted to their three-letter code format using <em>One to Three</em> from the Sequence Manipulation Suite (SMS2) 
[stothard2000sequence]. 
Combined Annotation Dependent Depletion (CADD) scores were sourced from (Nov 2018) and are reported by 
[kircher2014general ]. 
The dataset used was “All possibleSNVs” from whole genome data, from which we extracted the data for coding regions of <em>RAG1</em> and <em>RAG2</em>. 
We used the Human Gene Mutation Database (HGMD) from the Institute of Medical Genetics in Cardiff as a pre-defined source of known RAG deficiency cases (Feb 2019, free access
version to NM_000448.2.) 
[stenson2014human]. 
Data was formatted into CSV and imported into R for combined analysis with PHRED-scaled CADD scores and the main dataframe. 
The crystal structure render of DNA bound RAG complex was produced with data from RCSB Protein Data Bank
(3jbw.pdb)  [ru2015molecular]. 
Structures were visualised using the software VMD from the Theoretical and Computational Biophysics Group
[Humphrey1996vmd], imaged with Tachyon rendering 
[Stone1998Ane], and colour mapped using our scoring method.</p>

<h2 id="data-processing">Data processing</h2>

<p>The population genetics input dataset used GnomAD variant allele frequencies and reference sequences processed as CSV files, cleaned and sorted to contain only amino acid codes, residue numbers, alternate residues, alternate allele frequencies, and a score of 0 or 1 to indicate presence or absence of variants where 1 represented none reported. 
An annotation column was also provided to label where multiple alternate variants existed. 
Statistics and calculation steps are listed in order in Supplemental Tables E3-E8.</p>

<p>The percentage of conserved residues was calculated (55.99% of amino acids contained no reported variants in RAG1, 55.98% in RAG2 (
Table E4). 
Basic protein statistics were generated using canonical reference transcript sequences of <em>RAG1</em> and <em>RAG2</em> with the SMS2 tool <em>Protein Stats</em> 
[stothard2000sequence]. 
The resulting pattern percentage value was converted to a frequency (decimal 0-1) based on the number of residues per protein to generate the residue frequency
(\({R}_{f}\)). 
The \({R}_{f}\) values were found for both proteins as shown in <strong>Table E5</strong> and summarised in <strong>Table E6</strong>.</p>

<p>The count of variants per residue were found for both proteins and the mutation rates (\({M}_{r}\)) per residue were calculated as shown in
** Table E7**. 
\({M}_{r}\) was found by counting the number of mutations per residue in a window, sized to contain each protein
individually. 
For genome-wide application the window size may be increased or decreased. 
In this case the window consisted of only the coding regions. 
The \({M}_{r}\) values were then converted to the frequencies based on the number of residues per protein. 
Separate, and overlapping windows could also be used based on genome phase data and regions of linkage disequilibrium to account for non-random association of alleles at different loci; this might be particularly important for disorders with multiple genetic determinants.</p>

<p>The \({M}_{r}\) and \({R}_{f}\) multiply to give the raw mutation rate residue frequency (MRF) value (**Table E8
). 
This value is also shown in **Tables mrf.assay] and 
Table:1
. 
Our investigation used a Boolean score \(C\) to account for the presence or absence of a mutation in the general population; 0 for any variant existing in the population and 1 for conserved residues.
\(C \times {M}_{r} \times {R}_{f}\), in our case, produced the MRF score for conserved residues. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a></p>

<p>(ii)** illustrates the raw MRF as a histogram and the MRF, after applying \(C\), as a heatmap.</p>

<p>An important consideration for future application is whether to use this Boolean score or instead use a discrete variable which accounts for the
true allele frequency in the general population.<br />
In the clinical setting, the likelihood of <em>de novo</em> mutations and inherited mutations have different impacts when considering recessive and dominant diseases.
A patient is more likely to inherit a variant that exists even at a very low frequency than to acquire a random <em>de novo</em> mutation. 
Therefore, a value representing an allele frequency may be used to replace \(C\) in many investigations, particularity when considering variants that exist
at low rates. 
PRHED-scaled CADD score data consisted of nucleotide level values. 
For comparison with MRF, the median CADD scores were averaged per codon as demonstrated in text. 
A summary of data processing and analysis is illustrated in 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">method_map</a>.</p>

<h2 id="raw-data-availability-and-analysis-script">Raw data availability and analysis script</h2>

<p>The supplemental files 
<em>“Raw_data_R_analysis_for_figures”</em> 
contains all raw data and analysis methods used to produce figures (except illustrations in Figures 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>\ and 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>).
<em>“data_analysis.R”</em> 
is an R script that contains the methods used to produce figures. 
Each of the input data CSV files are explained on first usage within the analysis script. 
Running 
<em>“data_analysis.R”</em> 
from within the same directory as the associated input data CSV files will replicate analysis.</p>

<h2 id="data-visualisation">Data visualisation</h2>

<p>For our visualisation of MRF scores, small clusters of high MRF values were of more appealing than individual highly conserved residues.
Therefore, we applied a 1% average filter where values were averaged over a sliding window of N number of residues (10 in the case of RAG1, 6
in the case of RAG2). 
For a clear distinction of MRF clusters, a cut-off threshold was applied at the 75\(^{th}\) percentile (e.g. 
0.0168 in RAG1) as shown in heatmaps in  FIGURE
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii)** and</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>. 
The gene heatmaps for coding regions in <em>RAG1</em>
and <em>RAG2</em> (Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a> 
) were populated with (i)
Boolean \(C\) score from population genetics data, (ii) raw MRF scores, and (iii) MRF clusters with 1% average and cut-off threshold. 
GraphPad Prism was used for heatmaps. 
The data used for heatmaps is available in
TABLE Table:1
and in the supplemental R source to allow for alternative visualisations. 
An example of alternative output for non-R users is shown in
&lt;FIGURE RAG_MRF_map. 
Adobe Illustrator and Photoshop were used for protein domain illustrations in Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iv).</p>

<h2 id="validation-of-mrf-against-functional-data">Validation of MRF against functional data</h2>

<p>The recombination activity of RAG1 and RAG2 was previously measured on known or candidate pathogenic variants
[lee2014systematic lawless2018prevalence tirosh2018recombination].
Briefly, the pathogenicity of variants in <em>RAG1</em> and <em>RAG2</em> was measured functionally <em>in vitro</em> by either expression of RAG1 and RAG2 in
combination with a recombination substrate plasmid containing recombination signal sequence (RSS) sites which are targeted by RAG complex during normal V(D)J recombination, or Abelson virus-transformed Rag2-/- pro-B cells with an RSS-flanked inverted GFP cassette.
Recombination events were assessed by quantitative real-time PCR using comparative CT or expression of GFP evaluated by flow cytometry,
respectively. 
The inverse score of recombination activity (0-100%) was used to quantify pathogenicity of variants in our study. 
Comparison between known pathogenicity scores and MRF was done by scaling MRF scores from 0-100% (100% being highest probability of occurring as
damaging). 
A data and analysis is summarised in Figure
FIGURE 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">method_map</a></p>

<h1 id="results">Results</h1>
<h2 id="rag1-and-rag2-conservation-and-mutation-rate-residue-frequency">RAG1 and RAG2 conservation and mutation rate residue frequency</h2>

<p>Variant probability prediction is dependent on population genetics data.
Our study queried GnomAD 
[lek2016analysis] 
to identify conserved residues using a Boolean score \(C\) of 0 (present in population) or 1 (conserved). 
The gene-specific mutation rate \({M}_{r}\) of each residue was calculated from allele frequencies. 
The gene-specific residue frequency \({R}_{f}\) represented the frequency of a residue occurring per gene, acquired by converting gene residue percentage (from the SMS2 tool
<em>Protein stats</em>) to a frequency (decimal 0-1) 
[stothard2000sequence].
Together the values were used to calculate the most probable disease-causing variants which have not yet been identified in patients.
We termed the resulting score a mutation rate residue frequency, where \(MRF = {C} \times {M}_{r} \times {R}_f\). 
This score represents the likelihood that a clinically relevant mutation will occur.</p>

<p>Figure 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a> 
presents the most probable unidentified
disease-causing variants in <em>RAG1/2</em>. 
Variants with a low MRF may still be damaging but resources for functional validation are best spent on
gene regions with high MRF. 
Clusters of conserved residues are shown in
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
 (i)** and are generally considered important for protein structure or function. 
However, these clusters do not predict the likelihood of mutation. 
Raw MRF scores are presented in
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(ii)
Histograms illustrates the MRF without Boolean scoring applied and 
FIGURE
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii) provides a clearer illustration of top MRF score clusters. 
For visualisation, a noise reduction method was applied; a sliding window was used to find the average MRF per 1% interval of each gene. 
The resulting scores displayed in 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii)**
contain a cut-off threshold to highlight the top scoring residues (using
the 75\(^{th}\) percentile). 
Variant sites most likely to present in disease cases are identified by high MRF scoring. 
This model may be expanded by the addition of phenotypic or epigenetic data
(<strong>Supplemental;</strong> ).</p>

<p>![RAG1 (red, left) and RAG2 (blue, right) conservation and mutation rate
residue frequency. 
(i) Gene conservation score; non-conserved 0 and
conserved 1. 
Colour indicates no known mutations in humans. 
(ii)
Histogram; raw MRF score. 
Heatmap; MRF prediction for conserved residues, graded 0 to 0.05 (scale of increasing mutation likelihood with human disease). 
(iii) Coloured bars indicate most likely clinically relevant variant clusters. 
MRF score averaged with 1% intervals for each gene and cut-off below 75th percentile, graded 0 to 0.03 (noise
reduction method). 
(iv) Gene structure with functional domains. 
Full list of residues and scores available in 
TABLE Table:1
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure_main</a></p>

<p>Table [Table:1
 provides all MRF scores for both proteins. 
Raw data used for calculations and the list of validated residues of RAG1 and RAG2 are available in 
TABLE **Tables E3 - E8
. 
shows the MRF mutation likelihood score for mutations that have also been reported as tested for recombination activity in functional assays.
The likelihood of mutation does not correlate with pathogenicity;</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
and
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
show that most mutations tested had severe loss of protein function, while the likelihood each mutation occurring in humans varied significantly. 
Analysis-ready files are also available in Supplemental data along with the associated R source file to allow for alternative visualisations as shown in **Figure
FIGURE RAG_MRF_map
.</p>

<p>TABLE ref</p>

<h2 id="mrf-scores-select-for-confirmed-variants-in-human-disease">MRF scores select for confirmed variants in human disease</h2>

<p>We have applied MRF scores to known damaging mutations from other extensive reports in cases of human disease
[schuetz2008immunodeficiency lee2014systematic villa2001v abolhassani2014hypomorphic kutukculer2012novel sobacchi2006rag villa1998partial noordzij2002immunophenotypic crestani2014rag1 dalal2005evolution kuijpers2011idiopathic gruber2009clinical de2010hypomorphic buchbinder2015identification felgentreff2011clinical schwarz1996rag reiff2013exome corneo2001identical asai2011analysis kato2015rag1 yu2014human de2005novel zhang2005novel henderson2013expanding avila2010highly riccetto2014compound walter2015broad gomez2000mutations chou2012novel]
[originally compiled by [notarangelo2016human ]]. 
This dataset compares a total of 44 variants. 
We expected that functionally damaging variants (resulting in low recombination activity in vitro) that have the highest probability of occurrence would be identified with high MRF scores. 
MRF prediction correctly identified clinically relevant mutations in <em>RAG1</em> and <em>RAG2</em>
(Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i)).
Variants reported on GnomAD which are clinically found to cause disease had significantly higher MRF scores than variants which have not been
reported to cause disease. 
We observed that rare and likely mutations provided high scores while rare but unlikely or common variants had low scores
(Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i)).</p>

<p>![RAG1 and RAG2 MRF score predict the likelihood of mutations that are clinically relevant. 
(i) Known damaging variants (clinically diagnosed with genetic confirmation) reported on GnomAD have significantly higher
MRF scores than unreported variants. 
(ii) GnomAD rare variant allele frequency &lt;0.0001. 
No significant difference in allele frequency is found between known damaging and non-clinically reported variants.
Unpaired t-test. 
RAG1 P-value 0.002** RAG2 P-value 0.0339*. 
MRF; mutation rate residue frequency, ns; non-significant.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 2</a></p>

<p>Allele frequency is generally the single most important filtering method for rare disease in whole genome (and exome) sequencing experiments.
Variants under pressure from purifying selection are more likely to cause disease than common variants. 
However, most RAG mutations are rare. 
Therefore, allele frequencies of rare variants reported on GnomAD cannot differentially predict the likelihood of causing disease 
(<strong>Fig.
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(ii)</strong>). 
As such, we found no significant difference between known damaging variants and those that have not yet
been reported as disease-causing. 
The comparison between 
<strong>Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i) and (ii)</strong> 
illustrates the reasoning for the
design of our method.</p>

<p>Many non-clinically-reported rare variants may cause disease; the MRF score identifies the top clinically relevant candidates. 
Based on the frequency of protein-truncating variants in the general population, 
<em>RAG1</em> and <em>RAG2</em> are considered to be tolerant to the loss of one allele, as indicated by their low probability of being loss-of-function
intolerant (pLI) scores of 0.00 and 0.01, respectively 
[lek2016analysis]. 
This is particularly important for recessive diseases such as RAG deficiency where most new missense variants will be of unknown significance until functionally validated.</p>

<h2 id="top-candidate-variants-require-validation">Top candidate variants require validation</h2>

<p>Functionally characterising protein activity is both costly and time consuming. 
RAG1 and RAG2 have now been investigated by multiple functional assays for at least 110 coding variants
[lee2014systematic tirosh2018recombination lawless2018prevalence].
In each case, researchers selected variants in <em>RAG1</em> and <em>RAG2</em> that were potentially damaging or were identified from PID patients as the
most probable genetic determinant of disease. 
Functional assays for RAG deficiency in those cases, and generally, measured a loss of recombination activity as a percentage of wild type function (0-100%).</p>

<p>Pre-emptively performing functional variant studies benefits those who will be identified with the same variants in the future, before the
onset of disease complications. 
While more than 100 variants have been assayed in vitro, we calculated that only one-quarter of them are most
probable candidates for clinical presentation. 
<em>*Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
 illustrates that while functional work targeted “hand picked” variants that were ultimately
confirmed as damaging, many of them may be unlikely to arise based on population genetics data. 
**Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
 presents, in increasing order, the number of potential variants based on likelihood of presentation and
stacked by the number of variants per score category. 
Variants that have been measured for their loss of protein activity are coloured by
severity. 
Potential variants that remain untested are coloured in grey.
Only 21 of the top 66 most probable clinically relevant variants have been assayed in *RAG1</em>.</p>

<p>![RAG1 and RAG2 MRF score categories and variants assayed to date.
Protein residues are ranked and stacked into categories based on their MRF score. 
High scores (0.043 and 0.038 in RAG1 and RAG2, respectively) represent a greater mutation likelihood. 
Functional assays have measured recombination activity (as its inverse; % loss of activity) in a total
of 110 mutants. 
The severity of protein loss of function is represented by a red gradient. 
Residues that have not been functionally tested are shown in grey. 
While many protein residues are critical to protein function, their mutation is less probable than many of the top MRF
candidates. 
Data further expanded in Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>. 
MRF; mutation rate residue
frequency.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a></p>

<p>Supplemental Figure **
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
 further
illustrates the individual variants which have been tested functionally (the coloured <em>recombination activity</em> subset of Fig</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>]). 
We compared predicted MRF scores to assay measurements for 71 <em>RAG1</em> and 39 <em>RAG2</em> mutants. 
Most mutations tested showed severe loss of protein function (bottom panel of
Supplemental Figure **
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
), while the likelihood each mutation occurring in humans varied significantly (top panels).</p>

<p>If MRF scoring was used in the same cases pre-emptively, the loss of investment would be minimal; only 8 variants out of 71 mutants tested
had an above-average MRF score while being measured as functionally benign (a rate of 11.27%). 
RAG2 had only 3 out of 39 variants (7.69%) with an above-average MRF score while functionally benign. 
For the expended resources, approximately 30% more top candidates would have been tested in place of unlikely and functionally non-damaging
mutations. 
However, the true measurement of accuracy is limited in that very few of the most likely clinically relevant variants predicted by
MRF scoring have been tested to date.</p>

<h2 id="false-positives-in-transib-domains-do-not-negatively-impact-prediction">False positives in <em>Transib</em> domains do not negatively impact prediction</h2>

<p>Adaptive immunity is considered to have evolved through jawed vertebrates after integration of the RAG transposon into an ancestral
antigen receptor gene  [agrawal1998transposition hiom1998dna]. 
The <em>Transib</em> transposon is a 600 amino acid core region of RAG1 that targets RSS-like sequences in many invertebrates. 
A linked <em>RAG1/RAG2</em> was shown in the lower dueterosome (sea urchin), indicating an earlier common ancestor than the invertebrate  [fugmann2006ancient], and more
recently, a recombinatorially active RAG transposon (ProtoRAG) was found in the lower chordate amphioxus (or lancelet); the most basal extant
chordate and a “living fossil of RAG”  [huang2016discovery].</p>

<p>![False positives in <em>Transib</em> domains do not worsen probability prediction. 
The <em>Transib</em> domains contain critical conserved protein residues. 
(i) False positives were simulated by scoring <em>Transib</em> domain MRF without omitting Boolean conservation weight \(C=0\). 
(ii) Allele frequencies on GnomAD had conservation levels inversely proportional to
simulated false-positive MRF scoring. 
(iii) When testing for all Boolean component \(C&gt;0\) after MRF calculation the effect of false positives remained non-significant, illustrating the non-negative impact of MRF for prediciting the mutation. 
Unpaired t-test, * P = 0.0195, *** P
&lt; 0.0001. 
MRF; mutation rate residue frequency, ns; non-significant.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 4</a></p>

<p>A set of conserved motifs in core <em>RAG1</em> are shared with the <em>Transib</em> transposase, including the critical DDE residue catalytic triad
(residues 603, 711, and 965)  [kapitonov2005rag1]. 
Ten <em>RAG1</em> core motifs are conserved amongst a set of diverse species including human
[kapitonov2005rag1]. 
This evolutionarily conserved region is considered as most important to protein function. 
Therefore, we chose this region to determine if MRF scoring would have a negative impact if mutations were falsely predicted as clinically important. 
To assess the influence of a false positive effect on prediction, the MRF scores for conserved residues in this group were compared to GnomAD allele frequencies.
Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">False_positives_in_Transip_does_not_worsen_probability</a>
(i) 
plots the MRF (without omitting the Boolean component \(C=0\)) for conserved <em>Transib</em> motif residues, non-conserved <em>Transib</em> motif
residues, and non-<em>Transib</em> residues. 
<strong>Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">False_positives_in_Transip_does_not_worsen_probability</a>
(ii)</strong> 
shows the percentage of these which were reported as mutated on GnomAD. 
By accounting for unreported variants by applying \(C&gt;0\), the resulting effect on incorrectly scoring MRF in the conserved <em>Transib</em>
motifs remained neutral.</p>

<h2 id="mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants">MRF predicts RAG deficiency amongst PID patients harbouring rare variants</h2>

<p>We have previously measured the recombination activity of RAG1 and RAG2
disease-causing variants in several patients  [lawless2018prevalence].
We have compiled our own and other functional assay data from
@lee2014systematic and @tirosh2018recombination to produce a panel of
recombination activity measurements for coding variants in both <em>RAG1</em>
and <em>RAG2</em>.</p>

<p>RAG deficiency was measured as the level of recombination potential produced by the protein complex. 
Each method of investigation simulated the efficiency of wild-type or mutant proteins expressed by patients for their ability to produce a diverse repertoire of T-cell receptor (TCR) and B-cell receptor (BCR) and coding for immunoglobulins.
In functional experiments, mutant proteins were assayed for their ability to perform recombination on a substrate which mimics the RSS of TCR and BCR in comparison to wild-type protein complex (as % SEM).</p>

<p>By gathering confirmed RAG deficiency cases, we compiled the MRF scores for 43 damaging <em>RAG1</em> variants in 77 PID cases and 14 damaging <em>RAG2</em> variants in 21 PID cases (MRF scores spanning over 22 categories). 
To test our method against a strong control group, we identified coding variants in patients with PID where RAG deficiency due to coding variants has been ruled out as the cause of disease. 
We obtained <em>RAG1/2</em> variants in 558 PID patients who had their genomes sequenced as part of the NIHR BioResource - Rare Diseases study
[lawless2018prevalence]. 
Filtering initially identified 32 variants in 166 people. 
This set was trimmed to contain only rare variants; 29 variants over 26 MRF scoring categories from 72 cases of
non-RAG-deficient PID. 
The scatterplot in Figure 5 shows that most PID cases had damaging variants with a high MRF score, while PID cases carried benign variants in RAG1/2 with lower MRF scores; i.e. 
an MRF &gt;0.04 was seen for 31 cases of a damaging variant and only 2 cases of a non-damaging variant. 
Linear regression on this control group produced negative or near-zero slopes for <em>RAG1</em> and <em>RAG2</em>, respectively. 
The same analysis for known-damaging mutations in disease cases had a
significant prediction accuracy for <em>RAG1</em>. 
Analysis for <em>RAG2</em> was not significant. 
However, the sample size to date may be too small to significantly measure <em>RAG2</em> MRF scoring although a positive correlation
was inferred in  FIGURE
FIGURE <a href="RAG\_mrf\_linear\_regression">RAG_mrf_linear_regression</a></p>

<p>[altman1995statistics]. 
R source and raw data can be found in supplemental material.</p>

<p>![A linear regression model of RAG1/2 MRF scoring in cases of primary
immune deficiency. 
MRF prediction correlates with clinical presentation.
Damaging variants identified in confirmed RAG deficiency cases.
Non-damaging variants sourced from cases of PID with rare variants but not responsible for disease. 
An MRF &gt;0.04 was seen for 31 cases of a damaging RAG1 variants. 
(Slopes of RAG1: Damaging: 0.0008* (\(\pm\)
0.0004) P&lt;0.05, intercept 5.82e-05 ***, Non-damaging: -0.0007
(\(\pm\) 0.001). 
Slopes of RAG2; Damaging: 0.0023 (\(\pm\) 0.0018), intercept 0.0312 *, Non-damaging 0.0001 (\(\pm\) 0.0008). 
Source data and script in supplemental material).
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 5</a></p>

<h2 id="mrf-supplements-pathogenicity-prediction-tools-for-translational-research">MRF supplements pathogenicity prediction tools for translational research</h2>

<p>CADD scoring  [kircher2014general] is an important bioinformatics tool that exemplifies pathogenicity prediction. 
While CADD is a valuable scoring method, its purpose is not to predict likelihood of variation.
Similarly, MRF scoring is not a measure of pathogenicity. 
MRF scoring may be complemented by tools for scoring variant deleteriousness. 
We compare MRF to the PHRED-scaled CADD scores for all possible SNV positions in <em>RAG1</em> (**Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>
) illustrating that pathogenicity prediction cannot account for mutation probability.
Combining both methods allows researchers to identify highly probable mutations before querying predicted pathogenicity.</p>

<p>![<em>RAG1</em> PHRED-scaled CADD score versus GnomAD conservation rate and MRF score. 
Allele frequency conservation rate (<strong>top</strong>) is vastly important for identifying critical structural and functional protein regions. 
The impact of mutation in one of these conserved regions is often estimated using CADD scoring (<strong>middle</strong>). 
CADD score heatmap is aligned by codon and separated into three layers for individual nucleotide positions. 
The MRF score (<strong>bottom</strong>)(visualised using the 75\(^{th}\) percentile with 1% averaging) highlights protein regions which are most likely to present
clinically and may require pre-emptive functional investigation.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a></p>

<p>To further develop this concept, we first annotated variants with MRF likelihood scores and pathogenic prediction PHRED-scaled CADD scores
( FIGURE FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
), and secondly, performed a manual investigation of the clinical relevance of top candidates
(<strong>Table hgmd_data
). 
We used HGMD as an unbiased source of known RAG deficiency cases in both instances. 
CADD score was very successful at predicting the pathogenicity of a variant, (a high-density cluster of variants with CADD scores &gt;25) as shown in **red</strong> in</p>
<h2 id="figure">Figure</h2>
<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(i)<strong>. 
At about the same rate, CADD score also predicted variants as pathogenic that are, to date,
unreported (as **pink</strong> in **Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>]
(i)<strong>). 
Indeed, those unreported variants may very well be pathogenic.
However, the likelihood of each mutation varies. 
As such, we developed the MRF score to account for that likelihood. 
As expected, the likelihood of mutations occurring that were unreported was low according
to MRF (</strong>Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(ii)<strong>, **pink</strong>), while the mutations which did occur were highly enriched in at high MRF scores
(**Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(ii)<strong>, **red</strong> high-density cluster &gt;0.043). 
Combining mutation prediction (MRF) with pathogenicity prediction (tools like CADD) increases the accuracy of pre-emptively targeting clinically relevant variants. 
**Figure</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(iii)** shows that while the number of variants presented to date is relatively small, they already account for 36% of the top MRF score candidates.</p>

<p>![<em>RAG1</em> PHRED-scaled CADD score versus MRF score against HGMD data. 
(i)
A high CADD score is a predictor of deleteriousness. 
Both reported (red) and non-reported residues (pink) have a high density of high CADD score.
(ii) MRF scores only show a high-density cluster for high-likelihood variants, reflected by the high MRF score observed for known RAG
deficiency variants. 
The number of pathogenic variants is outweighed by conserved residues; (i-ii) shows density of scores to normalise between
groups. 
AUC overlap difference in CADD score of 21.43% and MRF score of 74.28% (above intersects &gt;22.84 and &gt;0.0409, in <em>(i-ii)</em>
respectively). 
(iii) The number of residues per MRF category shows that disease reported on HGMD accounts for 36% of top MRF candidates. 
AUC;
area under curve, CADD; Combined Annotation Dependent Depletion, HGMD;
Human Gene Mutation Database.<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a></p>

<h1 id="discussion">Discussion</h1>

<p>Determining disease-causing variants for functional analysis typically aims to target conserved gene regions. 
On GnomAD 56% of <em>RAG1</em> (approx. 246,000 alleles) is conserved with no reported variants. 
Functional validation of unknown variants in genes with this level purifying selection is generally infeasible. 
Furthermore, we saw that a vast number of candidates are “predicted pathogenic” by commonly used pathogenicity tools, which may indeed be damaging but unlikely to occur.
To overcome the challenge of manual selection we quantified the likelihood of mutation for each candidate variant.</p>

<p>Targeting clearly defined regions with high MRF scores allows for functional validation studies tailored to the most clinically relevant
protein regions. 
An example of high MRF score clustering occurred in the RAG1 catalytic RNase H (RNH) domain at p.Ser638-Leu658 which is also
considered a conserved <em>Transib</em> motif.</p>

<p>While many hypothetical variants with low MRF scores may be uncovered as functionally damaging, our findings suggest that human genomic studies
will benefit by first targeting variants with the highest probability of occurrence (gene regions with high MRF). 
**Table 
Table:1
 lists the values for calculated MRFs for RAG1 and RAG2.</p>

<p>We have presented a basic application of MRF scoring for RAG deficiency.
The method can be applied genome-wide. 
This can include phenotypically derived weights to target candidate genes or tissue-specific epigenetic features. 
In the state presented here, MRF scores are used for pre-clinical studies. 
A more advanced development may allow for use in single cases. 
During clinical investigations using personalised analysis of patient data, further scoring methods may be applied based on disease features. 
A patient phenotype can contribute a weight based on known genotype correlations separating primary immunodeficiencies or
autoinflammatory diseases  [picard2018international]. 
For example, a patient with autoinflammatory features may require a selection that favors genes associated with proinflammatory disease such as <em>MEFV</em> and
<em>TNFAIP3</em>, whereas a patient with mainly immunodeficiency may have preferential scoring for genes such as <em>BTK</em> and <em>DOCK8</em>. 
In this way, a check-list of most likely candidates can be confirmed or excluded by whole genome or panel sequencing. 
However, validation of these expanded implementations requires a deeper consolidation of functional studies than is currently available.</p>

<p>[Havrilla220814 ]
have recently developed a method with similar possible applications for human health mapping constrained coding regions. 
Their study employed a method that included weighting by sequencing depth.
Similarly, genome-wide scoring may benefit from mutation significance cut-off, which is applied for tools such as CADD, PolyPhen-2, and SIFT
[itan2016mutation]. 
We have not included an adjustment method as our analysis was gene-specific but implementation is advised when calculating genome-wide MRF scores.</p>

<p>The MRF score was developed to identify the top most probable variants that have the potential to cause disease. 
It is not a predictor of pathogenicity. 
However, MRF may contribute to disease prediction; a clinician may ask for the likelihood of RAG deficiency (or any other
Mendelian disease of interest) prior to examination (<em>**</em>).</p>

<p>Predicting the likelihood of discovering novel mutations has implications in genome-wide association studies (GWAS). 
Variants with low minor allele frequencies have a low discovery rate and low probability of disease association  [kido2018minor], an important
consideration for rare diseases such as RAG deficiency. 
An analysis of the NHGRI-EBI catalogue data highlighted diseases whose average risk allele frequency was low  [kido2018minor]. 
Autoimmune diseases had risk allele frequencies considered low at approximately 0.4. 
Without a method to rank most probable novel disease-causing variants, it is unlikely that GWAS will identify very rare disease alleles (with frequencies
&lt;0.001). 
It is conceivable that a number of rare immune diseases are attributable to polygenic rare variants. 
However, evidence for low-frequency polygenic compounding mutations will not be available until large, accessible genetics databases are available, exemplified by
the NIHR BioResource Rare Diseases study  [lawless2018prevalence]. 
An interesting consideration when predicting probabilities of variant frequency, is that of protective mutations. 
Disease risk variants are quelled at low frequency by negative selection, while protective variants may drift at higher allele frequencies  [chan2014excess].</p>

<p>The cost-effectiveness of genomic diagnostic tests is already outperforming traditional, targeted sequencing  [payne2018cost]. 
Even with substantial increases in data sharing capabilities and adoption of clinical genomics, rare diseases due to variants of unknown significance
and low allele frequencies will remain non-actionable until reliable predictive genomics practices are developed. 
Bioinformatics as a whole has made staggering advances in the field of genetics 
[libbrecht2015machine]. 
Challenges that remain unsolved, hindering the benefit of national or global genomics databases, include DNA data storage and random access retrieval  [Organick114553], data privacy management 
[Huang:224980], 
and predictive genomics analysis methods.
Variant filtration in rare disease is based on reference allele frequency, yet the result is not clinically actionable in many cases.
Development of predictive genomics tools may provide a critical role for single patient studies and timely diagnosis 
[casanova2014guidelines].</p>

<h1 id="conclusion">Conclusion</h1>
<p>We provide a list of amino acid residues for RAG1 and RAG2 that have not been reported to date, but are most likely to present clinically as RAG
deficiency.<br />
This method may be applied to other diseases with hopes of improving preparedness for clinical diagnosis.</p>

<p>Supplemental
sec:Supplemental_text</p>

<p>![Data analysis summary map. 
Raw data and analysis scripts are provided in the supplemental. 
Analysis steps and data sources for each procedure described in <em>methods</em>. 
MRF; mutation rate residue frequency, PID;
primary immunodeficiency.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 2</a></p>

<p>![An alternative visualisation of MRF scores for RAG1 and RAG2 proteins.
The data from Table 
Table:1
in column “Average over 1%” is displayed on both the y-axis and colour scale. 
An analysis-friendly long form CSV of the Table 
Table:1
data is also provided in the compressed supplemental R data
“mrf.csv”.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 3</a></p>

<p>![MRF likelihood score versus known functional activity. 
We compiled all variants that we know to have been assayed for protein function to date.
The inverse of functional assay measurements were used, where 0% activity represents 100% loss of activity. 
MRF scores are presented as a percentage of the maximum score per gene (i.e., for RAG1 \(MRF_{max} = 0.043\) (100%) and \(MRF_{min} = 0.0048\) (0%)). 
Top panels show how likely each mutation is predicted to occur in humans.
Bottom panels show the loss of protein activity as a percentage compared to wild-type (% SEM); most mutations tested produced severe loss of protein function, regardless of their mutation likelihood.
Subset of <em>Recombination activity</em> data from Figure</p>

<p>FIGURE [Figure 3](https://link.springer.com/article/10.1007/s10875-019-00670-z}.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a></p>

<h2 id="main-supplemental-data-table">Main supplemental data table</h2>

<h2 id="clinical-relevance-of-top-candidates">Clinical relevance of top candidates</h2>

<p>The top scoring candidates in RAG1 were assessed for potential clinical relevance (**Table hgmd_data
). 
HGMD was chosen as a reliable, curated source of identifying pathogenic variants. 
45% of RAG1 variants reported on HGMD (23 of 51) were predicted by our model as the most likely candidates seen clinically (the top scoring MRF group of had
66 residues total). 
The remaining variants in the top MRF group, which were not reported by HGMD (43 of 66), were assessed manually for their likelihood as potentially disease causing. 
21 (49%) were highly conserved, not reported on GnomAD, and would be considered probable RAG deficiency on presentation as homozygous or compound heterozygous with a
second damaging variant. 
The remainder had allele frequencies &lt;0.0006, were only found as low frequency heterozygous in the general population, and justify functional validation. 
We expect that none of the top candidate mutations are benign.</p>

<p><strong>Number variants</strong>
Top MRF score candidates total 66%
(i) Of which are reported on HGMD 23%
(ii) Not reported on HGMD to date 43%</p>

<p><strong>Number variants</strong> <strong>Unreported top candidate (%)</strong>%
<strong>GnomAD allele frequency</strong>%
Not found 21 of 43 49% 0%
Very rare 15 of 43 35% 0.00002*
Very rare 7 of 43 16% 0.00006**</p>

<h2 id="supplemental-analysis-tables">Supplemental analysis tables</h2>

<h2 id="protein-structure-application">Protein structure application</h2>

<p>With the availability of a structured protein complex, modelling can be carried out prior to functional assays. 
Residues with the highest MRF for both RAG1 and RAG2 were mapped in  FIGURE
FIGURE Structure
.</p>

<p>![The RAG1 (blue) and RAG2 (grey) protein structure with top candidate MRF scores. 
(i) Protein dimers and (ii=iv) monomers illustrating the three highest category MRF scores for predicted clinically relevant
variants. 
Increasing in score the top three MRF categories (illustrated in  FIGURE
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
) for each protein are highlighted; yellow, orange, red. 
DNA (green) is bound by the RAG protein complex at recombination signal sequences (PDB:3jbw).
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 5</a></p>

<h2 id="median-cadd-score-per-residue">Median CADD score per residue</h2>

<p>The sourced PHRED-scaled CADD score data consisted of nucleotide level values. 
We were interested in CADD scores averaged per codon.<br />
For every nucleotide position there were three alternative variants to consider,
e.g.</p>

<p>Chrom Pos Ref Alt1 Alt2
Alt3 PHRED1 PHRED2 PHRED3
11 36594855 A C G T 22.3
18.81 22.4</p>

<p>The PHRED-scaled scores are listed here; raw CADD scores are also
included in the original database. 
To produce a working input we used
the median score per codon, that is three scores per nucleotide and
three nucleotides per codon. 
This produced median PHRED-scaled score per
codon / residue, e.g.:</p>

<p>Chrom   Pos PHRED1  PHRED2 PHRED3 <br />
11 36594855 22.3 18.81 22.4
11 36594856 25.3 23.6 24.6
11 36594857 24.8 24.3 24.5</p>

<p>Median PHRED = 24.3\</p>
<h2 id="supplemental-file">Supplemental file</h2>
<p>** <em>‘RAG1.cadd.amino.csv’</em> within the analysis data <em>‘Raw_data_R_analysis_for_figures’</em> contains the median values over
a three-nucleotide window, starting at nucleotide 1 to produce input data with the correct reading frame. 
The “PHRED-scaled” values are used as a normalised and externally comparable unit of analysis, rather than
raw CADD scores. 
The area under the curve was calculated for density plots to quantify the difference between pathogenic and unreported variants with high scores, above the intersects &gt;0.0409 and &gt;22.84 for MRF and CADD, respectively, using score value (\(x\)) versus density (\(y\)) (Fig. 
 (i-ii)) with
\(\int_a^b \! f(x) \, \mathrm{d}x \approx\ \left(b-a\right) \left[\frac{f\left(a\right)\ + f\left(b\right)}{2}\right].\)</p>

<h2 id="genome-wide-and-disease-specific-application">Genome-wide and disease-specific application</h2>

<p>Weighting data can also be applied to the MRF score model to amplify the selectivity. 
The mutation rate can be applied genome wide with a process common in the study of information retrieval; term frequency, inverse document frequency (\(tf-idf\)). 
In this case the “term” and “document” are replaced by amino acid residue \(r\) and gene \(g\) , respectively such that,</p>

<p>[{rf-igf}<em>{r,g} ={rf}</em>{r,g} \times {igf}_r]</p>

<p>We may view each gene as a vector with one component corresponding to each residue mutation in the gene, together with a weight for each component that is given by (1).
Therefore, we can find the overlap score measure with the \({rf-igf}\) weight of each term in \(g\), for a query \(q\);</p>

<p>[\mbox{Score}(q,g)=\sum_{r\in q} \mbox{rf-igf}_{r,g}.]</p>

<p>In respect to MRF scoring, this information retrieval method might be applied as follows; the \({rf-igf}\) weight of a term is the product of
its \(rf\) weight and its \(igf\) weight (\({W}_{r,g}={rf}_{r,g} \times \log \frac{N}{{gf}_{r}}\)) or
(
\({W}_{r,g}=(1 + \log {rf}_{r,g}) \times \log \frac{N}{{gf}_{r}}\)
).
That is, firstly, the number of times a residue mutates in a gene
(
\(rf={rf}_{r,g}\)
) and secondly, the rarity of the mutation genome-wide in \(N\) number of genes (\(igf=N/{gf}_{r}\)). 
Finally, ranking the score of genes for a mutation query \(q\) by;
\(\mbox{Score}(q,g)=\sum_{r\in q\cap g} \mbox{rf-igf}_{r,g}\) The score of the query (Score(\(q,g\))) equals the mutations (terms) that appear in
both the query and the gene (\(r\in q\cap g\)). 
Working out the \(rf-igf\) weight for each of those variants (\({rf.igf}_{r,g}\)) and then summing them (\(\sum\)) to give the score for the specific gene with respect to the query.</p>

<h2 id="bayesian-probability">Bayesian probability</h2>

<p>MRF score may provide a limiting component required for applying Bayesian probability to disease prediction. 
A clinician may ask for the likelihood of RAG deficiency (or any Mendelian disease of interest) for a patient given a set of gene variants \(P(H|E)\) using Bayes’ theorem,
\(P(H|E) = \frac{P(E|H) P(H)}{P(E)}\) where \(P(H)\) is the probability of a patient having RAG deficiency, \(P(E | H)\) is the probability of RAG
deficiency due to a set of variants that have been pre-emptively assayed, and \(P(E)\) is the probability of having a set of gene variants.</p>

<p>\(P(H)\) is known since the rate of RAG deficiency is estimated at an incidence of 1:181,000  [kumanovics2017estimated], SCID at a rate of 1:330,000  [kwan2014newborn], and we also recently show the rate of RAG deficiency in adults with PID  [lawless2018prevalence]. 
Being a recessive disease, \(P(E)\) must account for biallelic variants and is the most difficult value to determine. 
This may be found from population genetics data for (i) the rate of two separate, compound heterozygous variants, (ii) the rate of a homozygous variant or potential consanguinity, or (iii) the rate of de novo variation
[lek2016analysis]. 
\(P(E|H)\) would be identified where all variants are functionally validated. 
This requires a major investment, however the MRF score provides a good approximation.</p>

</p>

  <h2> - </h2>
  <p><h1 id="websites-for-basic-genetic-variant-information">Websites for basic genetic variant information</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#websites-for-basic-genetic-variant-information" id="markdown-toc-websites-for-basic-genetic-variant-information">Websites for basic genetic variant information</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#sites-and-tools-for-getting-basic-genetic-information" id="markdown-toc-sites-and-tools-for-getting-basic-genetic-information">Sites and tools for getting basic genetic information</a></li>
  <li><a href="#communities-and-learning" id="markdown-toc-communities-and-learning">Communities and learning</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>Identifying pathogenic variants with whole genome and whole exome sequencing is not simple.
Determining the correct filtering method can take some time but it is not the most difficult task.
Validating genetic factors is generally the most time consuming part of this type of research.
Here is a compilation of some of the websites and resources that I use constantly.
I will begin this simply as a list but continue to contribute information on how to use all of these over time.
I use most of the listed resources daily.</p>

<p>There are several steps in assessing if a gene variant is a good candidate to explain a clinical phenotype.
Often a clear story can be made between the genetic mutation and the resulting phenotype.
Other times (usually) a genetic finding (particularly biallelic mutations) seem to have a direct link to the clinical phenotype but it can take  weeks-months to functionally validate such a finding.
 With that in mind, it is good to have some sort of routine way to quickly assess the possible pathogenicity of a mutation by hand.
I will mostly discuss these in the context of rare mutations which are likely to be under selective pressure and occur at very low frequencies in a healthy population.</p>

<h1 id="sites-and-tools-for-getting-basic-genetic-information">Sites and tools for getting basic genetic information</h1>
<p>For assessing rare variants Ensembl and Exac (now gnomAD) are my bread and butter.
I haven’t done it yet but I need to set up a hotkey to open a browser with both of these sites simultaneously.
To demonstrate how I like to use these we could use an example.
Lets say we have NGS results for a patient with immunodeficiency with coding variants in the gene RAG2.
OK, well known gene, important for antibody production as wells as TCR and BCR development.
Looks good so far.
Let’s see if the variants are common SNPs or could they be likely to cause damage if they are not reported (of course this is in your pipeline automatically but it’s good practice and takes less than 60 seconds; valuable if there is a real person affected by your results).</p>

<p>After a long time getting confused about transcripts and coordinates, I now know how important it is to accurately report coordinates so there is no confusion if collaborating or reporting the mutations etc.</p>

<p><a href="http://exac.broadinstitute.org">Exac.org</a></p>

<p>This is, in a nutshell, the exomes of ~ 60,000 individuals which can be used to view how frequently mutations occur in the general population (unfortunately it is mostly just European but there is some global representation). Exac is vital for checking coding variants.
It covers some of the intronic regions (exon intron splice sites) and some of the upstream and downstream regions.
This is typical for anyone who does whole exome sequencing.
We mentioned confusion about transcripts and coordinates, Exac automatically loads the coverage as shown for the canonical transcript.
Stick with this transcript for reporting or at least for your own notes.
When you head over to Ensembl grab the same one in the transcript table.<br />
Update! <a href="http://gnomad.broadinstitute.org/about">The Genome Aggregation Database (gnomAD) is online</a>.<br />
This data set is the combination of “123,136 exomes and 15,496 genomes from unrelated individuals” which has “removed individuals known to be affected by severe pediatric disease, as well as their first-degree relatives.”
This is n extremely exciting resource. If you are familiar with Exac then you will know the value of this expansion into gnomAD.
<a href="https://youtu.be/_uRuFZv4JaU">youtube.com</a><br />
<a href="http://www.ensembl.org/index.html">Ensembl</a><br />
Any good pipeline will have annotation of the details for any coding variants but it can be pretty valuable to go and look at these again by hand.
It doesn’t take long but can end up saving time in the long run.
If you do it often, the first check on Exac take less than 60 seconds.
The next check on Ensemble will only take 2-3 minutes.
In the quick search I plug in the gene name, luckily for my stuff the top hit is always the human gene (sorry Alpaca researchers).</p>

<p>When you get to the gene page first click is always “Show transcript table.”
If you are lucky there is only one coding transcript like for RAG2.
Most of the time there are about 6 transcripts of wildly varying lengths just to confuse matters.
Go for the transcript ID of the canonical transcript which you noted on Exac.
If you do so then life will be easier when you go to check the coordinates. 
On the left hand side in the table “Transcript-based displays” click “cDNA” shown under “Sequence”.
You can then search through to find the variant and amino acid to see if everything lines up.
You see the cDNA position and amino acid positions overlaid. If you were to pick a different transcript then of course the coordinates are likely to be different.
From here I usually go back to the table on the left of the screen to search Exons.
This obviously just lays out the exon sequences in blocks along with useful information.
Only a small segment of the introns are displayed.
If you want reference sequences of multiple types just find the down load sequence button and chose FASTA and decide which type you want to display.
You would likely have the information based on the annotated NGS data but you may want to look at the different transcripts and Ensembl is the best option.</p>

<p>So far (in just a couple of minutes) you could have looked up the allele frequencies, affect of mutation on different transcripts and check that everything that should be reported from the NGS output matches up.
My next step is to check if these variants a already reported.
Everyone has their favourite method, searching PubMed etc.
For my topics OMIM often produces good results and a quick search.<br />
<a href="https://www.omim.org/">Online Mendelian Inheritance in Man</a>
This is a curated database and is generally very good.
Hopefully it continues to grow for a long time into the future.
Depending on how much you already know about your gene it is sometimes helpful to jump straight down to the “Allelic variants” section (if one is present).
You may find a few variants already reported with a similar phenotype being described as your case.
You may find the exact mutations already reported.
If this is the case then it is likely that it would have taken a few minutes longer to find the same cases on one of the other databases.</p>

<p>Whether you have found that there are many mutations reported similar to those that interest you or if you have found nothing reported so far, my next step is always to run through UniProt.<br />
<a href="http://www.uniprot.org">UniProt</a><br />
UniProt is so rich in information that there is no need to expnad on it here.
If you have never used it then just pick your favourite protein and go look it up now.
There is (usually) a combination of nearly everything you need to get a quick overview of a protein.
Gene function, functional domains, known variants, reported knockouts/mutagenesis studies, protein structures, expression, localisation, the list goes on.
Actually, as much as I love PDB, I find that using UniProt is usually quicker to check for available PDB protein structures before actually going to PDB to download from the source.</p>

<p>With these four websites one would likely be able to decide how confident you are about a candidate mutation/s.
At least if you are just looking coding variants.
Assessing non-coding regions is much messier business.
From here on in validation of a mutation can require a widely variable amount of functional work.
One thing is certain however, Sanger sequencing will be needed to confirm your NGS finding.<br />
<a href="https://www.youtube.com/watch?v=3amsDkyiMu8">youtube.com</a><br />
<a href="https://github.com/gantzgraf/autoprimer3/releases/tag/v3.0.2">Autoprimer3</a></p>

<p>Autoprimer3 is an excellent application that you can use to design primers for a gene of interest.
It is super quick for producing primers to be used on genomic DNA for “any UCSC genome and design PCR/sequencing primers to genes or genome coordinates”.
As an example I timed myself to see how long it takes to get a primer list for all exons of the gene RAG2 and a reference sequence from default genomic coordinates on hg38 while avoiding SNPs based on dbsnp142.
It took me 46 seconds to open the application and produce a primer list and reference sequence.
Less than 1% of the time I may have to go and redesign a primer manually because of an awkward sequence or a patient’s DNA may have some uncommon variant at the primer site. 
Depending on which supplier you order oligos from, Sanger sequencing to confirm a variant by found during NGS can be done within 3 days; about 90 seconds to design and order the oligos, a day or 2 until they are delivered,  and a day to PCR and sequence.
The explanation may be a bit long winded here but this app is excellent.
Just give it a try if you do any routine PCR or sequencing for coding variant.
As the name suggests, it is a simple version of Primer3 but super quick.
<a href="https://software.broadinstitute.org/gatk/">Genome Analysis Toolkit: Variant Discovery in High-Throughput Sequencing Data.</a>
GATK most useful to jump straight to: <a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/">Tool Documentation Index</a> Genome hg38 <a href="http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=chr7:142299011,142813287">(TCR region as example)</a>
<a href="https://gpgtools.org">GPGtools</a>
for sending sensitive patient info.
<a href="https://www.gnupg.org">GnuPG</a> is GPL licensed alternative to the PGP suite for sending sensitive patient info.
See also Pretty good privacy for academic data.<br />
Human splice finder <a href="http://www.umd.be/HSF3/HSF.html">http://www.umd.be/HSF3/HSF.html</a><br />
Illumina-Pipeline-V2 (“Version 2 of Illumina pipeline that incorporates <a href="https://github.com/nirav99/Illumina-Pipeline-V2/blob/master/IlluminaPipelineCASAVA1_8.pdf">CASAVA 1.8”)</a><br />
Sequence Manipulation Suite<br />
<a href="http://www.coccidia.icb.usp.br/sms2/index.html">http://www.coccidia.icb.usp.br/sms2/index.html</a><br />
Sequence Ontology <a href="http://www.sequenceontology.org">http://www.sequenceontology.org</a><br />
UCSC Genome Bioinformatics FAQ <a href="https://genome.ucsc.edu/FAQ/FAQformat">https://genome.ucsc.edu/FAQ/FAQformat</a><br />
UCSC Table Browser <a href="https://genome.ucsc.edu/cgi-bin/hgTables">https://genome.ucsc.edu/cgi-bin/hgTables</a><br />
MutScan <a href="https://github.com/OpenGene/MutScan">https://github.com/OpenGene/MutScan</a><br />
Detect and visualise target mutations by scanning FastQ files directly.
Very useful if you are interested in some certain mutations but saves the time it would take to normally through your pipeline. </p>

<h1 id="communities-and-learning">Communities and learning</h1>
<p>No need to reinvent the wheel here. Stephen Turner has a better list of resources than I will produce with his post “Staying Current in Bioinformatics &amp; Genomics: 2017 Edition.” 
http://www.gettinggeneticsdone.com/2017/02/staying-current-in-bioinformatics-genomics-2017.html<br />
Essentially it boils down to the journals, Twitter, some expert blogs, and several genomics communities.
The journals and other sites I like to follow are detailed here.
When all directed into a single feed I think it produces an essential resource for most genetics/bioinformatics scientists.
Literature of Interest - In this post I show the use of Feedly to condense all the litereature that I follow into a single source and allow the option to view by category.</p>

<p>In this post I have started to gather some of the resources I like to use and topics that I find interesting.
Some other links tagged on at the end:<br />
BioStarts - Bioinformatics academic community https://www.biostars.org<br />
Useful bash Bioinformatics one-liners<br />
https://github.com/stephenturner/oneliners<br />
Efficient R programming https://csgillespie.github.io/efficientR/<br />
Cheat sheets for data.   science http://www.datasciencecentral.com/…
RStudio Cheat Sheets<br />
https://www.rstudio.com/resources/cheatsheets/#515</p>
</p>

  <h2> - </h2>
  <p><h1 id="genomic-analysis-tools">Genomic analysis tools</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#genomic-analysis-tools" id="markdown-toc-genomic-analysis-tools">Genomic analysis tools</a></li>
  <li><a href="#command-line-tool" id="markdown-toc-command-line-tool">command line tool</a></li>
  <li><a href="#desktop_applications" id="markdown-toc-desktop_applications">desktop_applications</a></li>
  <li><a href="#websites" id="markdown-toc-websites">Websites</a></li>
</ul>

<h1 id="command-line-tool">command line tool</h1>
<p><a href="https://github.com/virajbdeshpande/AmpliconArchitect">AmpliconArchitect</a> AmpliconArchitect is used to identify circular DNA fragments in genomic data.<br />
<a href="https://anaconda.org">Anaconda2</a> <br />
<a href="https://anaconda.org">Anaconda3</a> <br />
<a href="http://annovar.openbioinformatics.org/">annovar</a> Annotate functional consequences of genetic variation from sequencing data<br />
<a href="https://github.com/cancerit/ascatNgs">ASCAT</a> <br />
<a href="https://github.com/Crick-CancerGenomics/ascat">ASCAT</a> <br />
<a href="https://www.gnu.org/software/autoconf/autoconf.html">Autoconf</a> <br />
<a href="https://www.gnu.org/software/automake/">Automake</a> <br />
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">Autotools</a> <br />
<a href="https://github.com/cancerit/cgpBattenberg">Battenberg</a> <br />
<a href="https://github.com/Wedge-Oxford/battenberg">Battenberg</a> <br />
<a href="https://github.com/andyrimmer/Platypus/blob/master/extensions/DeNovo/bayesianDeNovoFilter.py">bayesianDeNovoFilter</a> <br />
<a href="http://www.htslib.org/doc/bcftools.html">bcftools</a> Utilities for variant calling and manipulating VCFs and BCFs<br />
<a href="http://bedops.readthedocs.io/">bedops</a> Toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data.<br />
<a href="http://bedtools.readthedocs.io/">bedtools</a> Set of tools for a wide-range of genomics analysis tasks.<br />
<a href="NA">bertha</a> <br />
<a href="https://www.gnu.org/software/binutils/">binutils</a> <br />
<a href="https://www.gnu.org/software/bison/">Bison</a> <br />
<a href="ftp://ftp.ncbi.nlm.nih.gov/blast/db/">Blast</a> It searches through non-human sequence looking for bacteria/viruses.<br />
<a href="http://www.bzip.org">bzip2</a> <br />
<a href="http://github.com/Illumina/canvas">canvas</a> A tool for calling copy number variants [CNVs) from human DNA sequencing data<br />
<a href="https://github.com/opencb/cellbase/wiki">cellbase</a> A comprehensive collection of RESTful web services for retrieving relevant biological information from heterogeneous sources<br />
<a href="https://github.com/lindaszabo/KNIFE">Circular RNA analysis</a> <br />
<a href="http://bonsai.hgc.jp/%7Emdehoon/software/cluster/software.htm">cluster</a> <br />
<a href="https://github.com/abyzovlab/CNVnator">CNVnator</a> A tool for CNV discovery and genotyping from depth-of-coverage by mapped reads<br />
<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwia5f_lu93cAhUI3KQKHTZNC1kQFjABegQIAhAB&amp;url=https%3A%2F%2Fdeveloper.nvidia.com%2Fcuda-zone&amp;usg=AOvVaw2J1e7C-ir5D_lz8OgoiUKF">CUDA</a> <br />
<a href="https://developer.nvidia.com/cudnn">cuDNN</a> <br />
<a href="https://curl.haxx.se">curl</a> <br />
<a href="https://github.com/im3sanger/dndscv">dndscv</a> Ability to calculate the non-synonymous to synonymous ratio [dN/dS) in cancer to find genes under negative or positive selection.<br />
<a href="https://github.com/dotnet">dotnet</a> <br />
<a href="NA">Eagle</a> <br />
<a href="https://github.com/easybuilders/easybuild">EasyBuild</a> <br />
<a href="NA">ERDS</a> <br />
<a href="https://sites.google.com/site/bioericscript/home">Eric-script</a> <br />
<a href="https://libexpat.github.io">expat</a> <br />
<a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/">FastQC</a> All the necessary tools for RNA-Seq analysis from raw-sequence read quality assessment, to alignment, transcript quantification, and differential expression. Also it includes analysis for gene-fusion analysis and circular RNA analysis.<br />
<a href="http://www.fftw.org">FFTW</a> <br />
<a href="https://github.com/gabraham/flashpca">flashPCA</a> flashPCA performs fast principal component analysis [PCA) of single nucleotide polymorphism [SNP) data<br />
<a href="xhttps://github.com/westes/flex">fle</a> <br />
<a href="https://www.freedesktop.org/wiki/Software/fontconfig/">fontconfig</a> <br />
<a href="https://www.freetype.org">freetype</a> <br />
<a href="https://software.broadinstitute.org/gatk/">GATK</a> Toolkit with a primary focus on variant discovery and genotyping<br />
<a href="http://gcc.gnu.org">GCC</a> <br />
<a href="https://github.com/easybuilders/easybuild-easyconfigs/tree/master/easybuild/easyconfigs/g/GCCcore">GCCcore</a> <br />
<a href="https://www.gnu.org/software/gettext/">gettext</a> <br />
<a href="https://gmplib.org">GMP</a> <br />
<a href="NA">gompi</a> <br />
<a href="http://www.brown.edu/Research/Istrail_Lab/resources/hapcompass_manual.html">hapcompass</a> A software package for haplotype assembly of diploid, polyploid, and tumor genomes<br />
<a href="https://github.com/vibansal/hapcut/blob/master/README.md">hapcut</a> A max-cut based algorithm for haplotype assembly that uses the mix of sequenced fragments from the two chromosomes of an individual<br />
<a href="https://www.gnu.org/software/help2man/">help2man</a> <br />
<a href="https://ccb.jhu.edu/software/hisat2/index.shtml">HiSAT</a> <br />
<a href="http://www.htslib.org/doc/#manual-pages">htslib</a> A C library for reading/writing high-throughput sequencing data<br />
<a href="https://www.open-mpi.org/projects/hwloc/">hwloc</a> <br />
<a href="http://mathgen.stats.ox.ac.uk/impute/impute_v2.html">impute2</a> IMPUTE version 2 [also known as IMPUTE2) is a genotype imputation and haplotype phasing program based on ideas from Howie et al. 2009<br />
<a href="https://www.ebi.ac.uk/~zerbino/velvet/">install velvet</a> It Assembles unmapped reads into longer contigs, to help identify their source.<br />
<a href="https://java.com/en/download/">java</a> <br />
<a href="https://github.com/comprna/Junckey">Junckey</a> <br />
<a href="https://pachterlab.github.io/kallisto/download.htm">Kallisto</a> <br />
<a href="http://people.virginia.edu/~wc9c/KING/manual.html">king</a> A toolset to explore genotype data from a genome-wide association study and a sequencing project<br />
<a href="https://github.com/davidaknowles/leafcutter">LeafCutter</a> <br />
<a href="http://www.libpng.org/pub/png/libpng.html">libpng</a> <br />
<a href="https://tiswww.case.edu/php/chet/readline/rltop.html">libreadline</a> <br />
<a href="https://www.gnu.org/software/libtool/">libtool</a> <br />
<a href="http://xmlsoft.org">libxml2</a> <br />
<a href="https://genome.ucsc.edu/util.html">LiftOver</a> <br />
<a href="https://github.com/arq5x/lumpy-sv/blob/master/README.md">lumpy</a> A probabilistic framework for structural variant discovery<br />
<a href="https://www.gnu.org/software/m4/m4.html">M4</a> <br />
<a href="https://majiq.biociphers.org/">MAJIQ</a> <br />
<a href="https://getmanta.com">manta</a> <br />
<a href="https://maven.apache.org">maven</a> <br />
<a href="https://bitbucket.org/uwlabmed/msings">msings</a> <br />
<a href="http://multiqc.info/docs/#manual-installation">MultiQC</a> MultiQC is a program that generates reports from the log files of common bioinformatics tools, and would be very good to have in the environment. It provides report creation, information aggregation functionalities.<br />
<a href="https://www.gnu.org/software/ncurses/">ncurses</a> <br />
<a href="https://linux.die.net/man/8/numactl">numactl</a> <br />
<a href="https://www.openblas.net">OpenBLAS</a> <br />
<a href="https://www.open-mpi.org">OpenMPI</a> <br />
<a href="https://pandoc.org/">Pandoc</a> Pandoc is a Haskell library for converting from one markup format to another, and a command-line tool that uses this library.<br />
<a href="https://www.gnu.org/software/parallel/">parallel</a> <br />
<a href="https://www.perl.org">perl</a> <br />
<a href="http://evolution.genetics.washington.edu/phylip.html">phylip</a> <br />
<a href="https://broadinstitute.github.io/picard/">picard</a> <br />
<a href="http://www.pixman.org">pixman</a> <br />
<a href="https://www.freedesktop.org/wiki/Software/pkg-config/">pkg-config</a> <br />
<a href="http://www.well.ox.ac.uk/platypus-doc">Platypus</a> A Haplotype-Based Variant Caller For Next Generation Sequence Data<br />
<a href="http://zzz.bwh.harvard.edu/plink/">PLINK</a> A whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner<br />
<a href="https://www.python.org">python</a> <br />
<a href="https://www.r-project.org">R</a> <br />
<a href="http://rnaseq-mats.sourceforge.net/">rMATS</a> <br />
<a href="https://bioconductor.org/biocLite.R">RMySQL R package</a> <br />
<a href="https://github.com/RealTimeGenomics/rtg-tools">rtg-tools</a> Utilities for accurate VCF comparison and manipulation<br />
<a href="https://www.ruby-lang.org/en/">ruby</a> <br />
<a href="https://github.com/COMBINE-lab/salmon">Salmon</a> For direct transcript quantification from FastQ<br />
<a href="http://lomereiter.github.io/sambamba/index.html">Sambamba</a> Utilities for viewing, manipulating and merging bam files<br />
<a href="https://github.com/GregoryFaust/samblaster">samblaster</a> A tool to mark duplicates and extract discordant and split reads from sam files<br />
<a href="http://www.htslib.org/doc/samtools.html">samtools</a> Utilities for accessing and manipulating sam files<br />
<a href="https://www.osc.edu/resources/available_software/software_list/scalapack">ScaLAPACK</a> <br />
<a href="http://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.html">SHAPEIT2</a> SHAPEIT is a fast and accurate method for estimation of haplotypes [aka phasing) from genotype or sequencing data.<br />
<a href="https://github.com/alexdobin/STAR">STAR</a> <br />
<a href="https://github.com/STAR-Fusion/STAR-Fusion/wiki">STAR-fusion</a> <br />
<a href="https://github.com/comprna/SUPPA">SUPPA</a> <br />
<a href="https://tug.org/texlive/">texlive</a> <br />
<a href="https://github.com/vcflib/vcflib/blob/master/README.md">vcflib</a> A C++ library for parsing and manipulating VCF files<br />
<a href="https://vcftools.github.io/index.html">vcftools</a> A program package designed for working with VCF files<br />
<a href="https://github.com/Ensembl/ensembl-vep">vep</a> Determines the effect of variants [SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions<br />
<a href="https://github.com/statgen/verifyBamID/releases">verifybamid</a> <br />
<a href="http://www.lstmed.ac.uk/vtbuilder">vt</a> A tool for the inference of non-chimeric contigs from read data that has been sequenced from complex multi-isoformic transcriptomes<br />
<a href="NA">weCall</a> <br />
<a href="https://tukaani.org/xz/">xz</a> <br />
<a href="https://www.zlib.net">zlib</a></p>
<h1 id="desktop_applications">desktop_applications</h1>
<p><a href="https://atom.io/packages/atom-ide-ui">atom-ide-ui</a> <br />
<a href="xhttps://www.mozilla.org/en-US/firefox/new/">Firefo</a> <br />
<a href="https://atom.io/packages/ide-python">ide-python</a> requires server https://github.com/palantir/python-language-server<br />
<a href="https://www.libreoffice.org">LibreOffice</a> <br />
<a href="https://www.rstudio.com">RStudio</a></p>
<h1 id="websites">Websites</h1>
<p><a href="www.ncbi.nlm.nih.gov/clinvar/*">ClinVar</a> www.ncbi.nlm.nih.gov/clinvar/<br />
<a href="compbio.charite.de/hpoweb/*">Compbio HPO Browser</a> http://compbio.charite.de/hpoweb/<br />
<a href="www.ncbi.nlm.nih.gov/CCDS/*">Consensus CDS Project</a> www.ncbi.nlm.nih.gov/CCDS/<br />
<a href="cancer.sanger.ac.uk/wgs/*">COSMIC</a> http://cancer.sanger.ac.uk/cosmic/<br />
<a href="www.ncbi.nlm.nih.gov/snp/*">DBsnp</a> www.ncbi.nlm.nih.gov/snp/<br />
<a href="decipher.sanger.ac.uk/*">DECIPHER</a> http://decipher.sanger.ac.uk/<br />
<a href="www.ebi.ac.uk/*">EMBL EBI</a> www.ebi.ac.uk/<br />
<a href="www.ebi.ac.uk/interpro/*">EMBL EBI</a> <br />
<a href="www.ebi.ac.uk/wen_guidelines/*">EMBL EBI</a> <br />
<a href="static.ensembl.org/*">Ensembl</a> <br />
<a href="static.ensembl.org/minified/*">Ensembl</a> <br />
<a href="www.ensembl.org/id/*">Ensembl</a> <br />
<a href="ensembl.org/*">Ensembl Human</a> http://ensembl.org/Homo_sapiens/<br />
<a href="ensembl.org/Homo_sapiens/Gene/*">Ensembl Human</a> <br />
<a href="ensembl.org/Homo_sapiens/*">Ensembl Human</a> <br />
<a href="http://exac.broadinstitute.org">exAC Browser beta</a> http://exac.broadinstitute.org<br />
<a href="ghr.nlm.nih.gov/*">Genetics Home Reference Gene Browser</a> http://ghr.nlm.nih.gov/gene<br />
<a href="ghr.nlm.nih.gov/gene/*">Genetics Home Reference Gene Browser</a> <br />
<a href="ghr.nlm.nih.gov/images/*">Genetics Home Reference Gene Browser</a> <br />
<a href=".incoming01.genomicsplc.com">Genomics Plc Misc.</a> <br />
<a href="http://gnomad.broadinstitute.org">gnomAD Browser beta</a> http://gnomad.broadinstitute.org<br />
<a href="www.gstatic.com/charts/*">Google Misc.</a> <br />
<a href="www.genenames.org/*">HUGO Gene Nomenclature Committee</a> http://www.genenames.org/<br />
<a href="www.genenames.org/data/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/sites/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/css/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/js/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="rest.genenames.org/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.human-phenotype-ontology.org/hpoweb/*">Human Phenotype Ontology</a> http://compbio.charite.de/hpoweb/showterm<br />
<a href="www.human-phenotype-ontology.org/*">Human Phenotype Ontology</a> <br />
<a href="human-phenotype-ontology.github.io/*">Human Phenotype Ontology</a> <br />
<a href="igv.broadinstitute.org/*">IGV Browser</a> <br />
<a href="igvdata.broadinstitute.org/*">IGV Browser</a> <br />
<a href="www.ncbi.nlm.nih.gov/portal*">NCBI</a> www.ncbi.nlm.nih.gov/portal<br />
<a href="www.nlm.nih.gov/core/*">NCBI</a> <br />
<a href=".nlm.nih.gov/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/gtr/tests/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/core/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/project/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/gene*">NCBI Gene</a> www.ncbi.nlm.nih.gov/gene<br />
<a href="www.ncbi.nlm.nih.gov/nuccore/*">NCBI Nucleotide</a> www.ncbi.nlm.nih.gov/nuccore/<br />
<a href="omim.org/*">OMIM</a> http://omim.org<br />
<a href="omim.org/entry/*">OMIM</a> <br />
<a href="omim.org/static/*">OMIM</a> <br />
<a href="omim.org/search/*">OMIM</a> <br />
<a href="api.europe.omim.org/api/*">OMIM</a> <br />
<a href="https://panelapp.genomicsengland.co.uk/">Panelapp</a> https://panelapp.genomicsengland.co.uk/<br />
<a href="static.pubmed.gov/*">Pubmed</a> www.ncbi.nlm.nih.gov/pubmed/<br />
<a href="www.ncbi.nlm.nih.gov/pubmed/*">Pubmed</a> <br />
<a href="dev-static.pubmed.gov/*">Pubmed</a> <br />
<a href="cancer.sanger.ac.uk/_asset/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/cancergenome/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/javascripts/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/cosmic/*">Sanger</a> <br />
[SMART <a href="smart.embl-heidelberg.de/*">a Simple Modular Architecture Research Tool)</a> http://smart.embl-heidelberg.de<br />
<a href="https://www.sib.swiss/*">Swiss Institute of Bioinformatics</a> https://www.sib.swiss<br />
<a href="www.hgmd.cf.ac.uk/ac/*">The Human Gene Mutation Database</a> www.hgmd.cf.ac.uk<br />
<a href="www.hgmd.cf.ac.uk/*">The Human Gene Mutation Database</a> <br />
<a href="www.sequenceontology.org/miso/current_release/term/*">The Sequence Ontology  database</a> www.sequenceontology.org/<br />
<a href="www.sequenceontology.org/css/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/js/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/img/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/browser/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/*">The Sequence Ontology  database</a> <br />
<a href="www.genome.ucsc.edu/*">UCSC Genome Browser</a> <br />
[Xfam <a href="pfam.sanger.ac.uk/family/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a> http://xfam.org<br />
[Xfam <a href="pfam.sanger.ac.uk/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a> <br />
[Xfam <a href="xfam.org/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a></p>
</p>

  <h2> - </h2>
  <p><h1 id="pharmacogenomics-for-personal-medicine">Pharmacogenomics for personal medicine</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#pharmacogenomics-for-personal-medicine" id="markdown-toc-pharmacogenomics-for-personal-medicine">Pharmacogenomics for personal medicine</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#running-a-real-example" id="markdown-toc-running-a-real-example">Running a real example</a></li>
  <li><a href="#comparing-annotated-genetic-data-to-drug-lists" id="markdown-toc-comparing-annotated-genetic-data-to-drug-lists">Comparing annotated genetic data to drug lists</a>    <ul>
      <li><a href="#small-example-check-of-gene-list-versus-drug-gene-list" id="markdown-toc-small-example-check-of-gene-list-versus-drug-gene-list">Small example check of gene list versus drug-gene list</a></li>
    </ul>
  </li>
  <li><a href="#a-real-example-of-merging-genetic-and-pharmacogenomic-data" id="markdown-toc-a-real-example-of-merging-genetic-and-pharmacogenomic-data">A real example of merging genetic and pharmacogenomic data</a></li>
  <li><a href="#annotation" id="markdown-toc-annotation">Annotation</a></li>
  <li><a href="#drug-indication" id="markdown-toc-drug-indication">Drug indication</a></li>
  <li><a href="#optimising-vcf-annotation" id="markdown-toc-optimising-vcf-annotation">Optimising VCF annotation</a></li>
  <li><a href="#how-to-get-coordinates-for-a-gene-list" id="markdown-toc-how-to-get-coordinates-for-a-gene-list">How to get coordinates for a gene list</a></li>
  <li><a href="#extracting-regions-from-a-vcf-using-a-bed-file" id="markdown-toc-extracting-regions-from-a-vcf-using-a-bed-file">Extracting regions from a VCF using a bed file</a></li>
  <li><a href="#unknown-variants" id="markdown-toc-unknown-variants">Unknown variants</a></li>
  <li><a href="#gene-dosage" id="markdown-toc-gene-dosage">Gene dosage</a></li>
  <li><a href="#drug-indication-1" id="markdown-toc-drug-indication-1">Drug indication</a></li>
  <li><a href="#a-large-scale-example-summary" id="markdown-toc-a-large-scale-example-summary">A large scale example summary</a></li>
  <li><a href="#more-questions" id="markdown-toc-more-questions">More questions</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>With the popularisation of commercial genetics services, more and more people are now able to “decode” their genetic data.
Questions that might arise from this information include “do I have potentially disease-causing variants that can be treated with a drug?”, or “am I taking a drug that will be affected by my genetics?”.
To tackle such questions with an example, we use public data in combination with pharmacogenomics.
Outside of genotype data (offered by <a href="https://www.23andme.com">23andMe</a> for example), the most common file type will be VCF:
<a href="https://gatkforums.broadinstitute.org/gatk/discussion/1268/what-is-a-vcf-and-how-should-i-interpret-it">What is a vcf and how should I interpret it?</a>.</p>

<p>Here is a data source with different genetic data files.
<a href="https://my.pgp-hms.org/public_genetic_data">https://my.pgp-hms.org/public_genetic_data</a>.
To check that it works OK, I tried a quick version of this challenge.
I picked the first whole genome VCF file that I saw (hu24385B 2019-04-07.vcf.g_z)
<a href="https://my.pgp-hms.org/profile/hu24385B">https://my.pgp-hms.org/profile/hu24385B</a>. 
The VCF has 3,461,639 variants.
VCF files can contain a large range of information for each variant, however only the first 7 column are strictly neccessary; Chromosome, position, ID, Reference, Alternate, Qulaity, Filter, info. 
<a href="https://gatkforums.broadinstitute.org/gatk/discussion/1268/what-is-a-vcf-and-how-should-i-interpret-it">The details are explained on this GATK forum post.</a>.
Annotation information about the gene name (or related diseases) is often not present when the VCF is generated and only added later.
Therefore the most common input source may be lacking gene symbols.
To get the gene names of a single file, the simplest way was is to upload a VCF (or a part of it) to <a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor</a> to get the gene symbol (and any other information that you wish).
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.45.png" width="80%" /><br />
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.58.png" width="80%" /></p>

<p>To reduce the time and output you can limit the options.
Split the file and run in batches to save time.
Here I tried the first ~1800 variants.</p>

<p><br />
<code class="highlighter-rouge">head -2000 56001801068861_WGZ.snp.vcf &gt; test.vcf</code></p>

<p><br />
And then annotated that extract with <a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor.</a>
The results would be retained by a URL address such as this, for a few days, but will be deleted by the time your read this.<br />
<br />
<a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/Results?db=core;tl=jNYYW5ONeVFYnaMM-5265700">http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/Results?db=core;tl=jNYYW5ONeVFYnaMM-5265700</a> <br />
<br />
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.10.png" width="80%" /><br />
<br />
Make certain that you use the same reference genome as used on the input data.
The VCF file was made using reference genome GRCh37.
Therefore the Ensembl/VEP website URL should be for that genome build (grch37, not the default GRCh38).</p>

<h1 id="running-a-real-example">Running a real example</h1>
<p>If you would like to try this using a whole genome using the Ensembl web interface you will need to split your VCF into smaller block first.
For routine usage the command-line version of VEP and it’s databases should be installed on run locally.
There are several bioinformatics tools that are commonly used for manipulating genetic file formats such as VCFtools. 
To get a real understanding of the data type, I inlcude here a method using command line bash to split a VCF file into smaller blocks. 
A bash script is printed below where I use very mainstream traditional command-line tools to wrangle data, including 
<a href="https://en.wikipedia.org/wiki/Gzip">gunzip</a>
to unzip compressed files, 
<a href="https://en.wikipedia.org/wiki/wc_(Unix)">wc</a>
to could lines, 
<a href="https://en.wikipedia.org/wiki/Cat_(Unix)">cat</a>
to print a file, 
<a href="https://en.wikipedia.org/wiki/Head_(Unix)">head</a>
to read the top of a file, 
<a href="https://en.wikipedia.org/wiki/Sed">sed</a>
to edit lines, 
<a href="https://en.wikipedia.org/wiki/AWK">awk</a>
for data extraction, and
<a href="https://en.wikipedia.org/wiki/Grep">grep</a>
which is not used here but it fits well with these other tools - for text search.
Creating a file containing the code below and ending with the filename extension with “.sh” will allow it to be run by your terminal, in this case using the bash language.
I encourage you to read each line and figure out what should happen. If it makes sense then it is reasonable to swap such a manual method with a more efficient specialised tool.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># VEP accept files of &lt;50MB size.</span>
<span class="c"># We will split our large VCF into smaller files.</span>
<span class="c"># Each file requires the same original </span>
<span class="c"># headers and file extension ".vcf"</span>

<span class="c"># Unzip the VCF.gz</span>
<span class="nb">gunzip </span>56001801068861_WGZ.snp.vcf.gz

<span class="c"># Count the number of lines in vcf</span>
<span class="nb">wc</span> <span class="nt">-l</span> 56001801068861_WGZ.snp.vcf

<span class="c"># How should a vcf file look?</span>
<span class="c"># See the links posted in this tutorial above</span>

<span class="c"># Take a look at the header</span>
<span class="c"># This VCF has 140 lines of header metadata (beginning with "#")</span>
<span class="c"># Line 141 shows the column headers: CHROM	POS	ID	REF	ALT...</span>
<span class="c"># Line 142 starts with the first variant</span>
<span class="nb">head</span> <span class="nt">-142</span> 56001801068861_WGZ.snp.vcf

<span class="c"># Print the header to a new file for later</span>
<span class="nb">head</span> <span class="nt">-141</span> 56001801068861_WGZ.snp.vcf <span class="o">&gt;</span> header
<span class="c"># Print everything else (the body) to </span>
<span class="c"># a new file that we will then split.</span>
<span class="nb">sed</span> <span class="s1">'1,141d'</span> 56001801068861_WGZ.snp.vcf <span class="o">&gt;</span> body.vcf

<span class="c"># Make a new directory for the next step</span>
<span class="nb">mkdir </span>split_files
<span class="c"># Move the large file inside</span>
<span class="nb">mv </span>body.vcf split_files/
<span class="nb">cd </span>split_files
<span class="c"># Now split the body.vcf into smaller </span>
<span class="c"># files of 200,000 lines each</span>
<span class="nb">split</span> <span class="nt">-l</span> 150000 body.vcf

<span class="c"># You will now how ~10 files "xaa, xab, etc."</span>
<span class="c"># Add the header back onto all of these files to make them VCFs again.</span>
<span class="c"># This "for loop" will do the following for each file:</span>
<span class="c"># Print the header and the vcf body to </span>
<span class="c"># a file with the same name, </span>
<span class="c"># adding a file extension ".vcf".</span>
<span class="c"># Then remove the vcf body file that </span>
<span class="c"># does not have the ".vcf" extension</span>
<span class="c"># leaving you with the original whole genome VCF split</span>
<span class="c"># into smaller files, each with the same headers.</span>

<span class="k">for </span>file <span class="k">in</span> ./x<span class="k">*</span> <span class="p">;</span> 
    <span class="k">do </span><span class="nb">cat</span> ../header <span class="nv">$file</span> <span class="o">&gt;&gt;</span> <span class="nv">$file</span>.vcf <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nv">$file</span> <span class="p">;</span>
<span class="k">done</span>

<span class="c"># These should be small enough to run on VEP online.</span>
<span class="c"># You could edit the split command to make a </span>
<span class="c"># reasonable number of files, </span>
<span class="c"># uploading &gt;10 is not efficient.</span>

</code></pre></div></div>

<h1 id="comparing-annotated-genetic-data-to-drug-lists">Comparing annotated genetic data to drug lists</h1>
<p>Uploading either a small example or your split-whole-genome example, VEP will process the data and annotate it will whatever features you require, including gene names, variant consequence, pahtnogenicy prediction, etc.
The next aim will be to see if any of your output genes are also present in your drug-gene database.
The method will require merging both dataset (gene and drug datasets) based on shared features. 
A simple sanity test would be useful first:</p>

<h2 id="small-example-check-of-gene-list-versus-drug-gene-list">Small example check of gene list versus drug-gene list</h2>
<p>From the VEP output I extracted the gene symbols column and compared against a list of druggable target genes from 
<a href="https://www.drugbank.ca">DrugBank</a>
(because I happen to have their data on hand, FDA may be more reliable).
I used another command-line method to do this efficienctly:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cut</span> <span class="nt">-f1</span> <span class="nt">-d</span> <span class="s2">","</span> vep_output_file.csv | <span class="nb">uniq</span> <span class="o">&gt;</span> unique.genes.txt
</code></pre></div></div>
<ul>
  <li>Cut column 1 (f1)</li>
  <li>with a delimiter comma (,)</li>
  <li>from the vep output csv file (or tsv, or text file)</li>
  <li>then pipe (|) that result into another program (sort) to sort the result in alphabetic order</li>
  <li>pipe (|) again this result into(uniq)</li>
  <li>so that only one unique gene name is output</li>
  <li>then (&gt;) write the output into the new file “unique.genes.txt”.<br />
<br />
Repeat the same type of method on the DrugBank dataset to get a list of the gene names contained in DrugBank into a file called “unique.druggable.txt”
The next command will then compare both lists.
<br />
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sort </span>unique.genes.txt unique.druggable.txt | <span class="nb">uniq</span> <span class="nt">-c</span> <span class="nt">-i</span> | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'1 '</span>
</code></pre></div>    </div>
    <p>This command also used “uniq -c” to count how many times each gen name occurs and then “grep -v ‘1 ‘” meaning ignore genes that are only present 1 time. 
We want the genes that are present twice, once in each list.
The genes which were in present in both the variant list and DrugBank list are:<br />
From a 2,000 line VCF file = 
<a href="https://www.drugbank.ca/bio_entities/BE0003599">GABRD</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0004895">PRKCZ</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0000495">SCNN1D</a><br />
From a 10,000 VCF line file = 
<a href="https://www.drugbank.ca/bio_entities/BE0003599">GABRD</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0004895">PRKCZ</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0000495">SCNN1D</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0008994">TP73</a>.</p>
  </li>
</ul>

<h1 id="a-real-example-of-merging-genetic-and-pharmacogenomic-data">A real example of merging genetic and pharmacogenomic data</h1>
<p>Now that a small example has shown us the logic of the process, we can try a more complex real-world example. 
The following R language script is used to merge the VEP annotated VCF file with a DrugBank database based on the gene names that are common to both datasets.
Read each line and try to understand the process. 
The are many alternative ways to do the same thing in different programming languages. 
This example is done using R. 
I recommending installing R and then installing R studio to edit and run your commands.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comment lines are ignored because of "#" symbol.</span><span class="w">
</span><span class="c1"># Command lines are run by clicking "Run" or "command+enter" on Mac</span><span class="w">

</span><span class="c1"># csv = comma sep file</span><span class="w">
</span><span class="c1"># tsv = tab spaced</span><span class="w">
</span><span class="c1"># txt = white space</span><span class="w">

</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># To do</span><span class="w">
</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># Files: vepfile, drugs</span><span class="w">
</span><span class="c1"># merge based on Gene symbol</span><span class="w">

</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># import the VEP file</span><span class="w">
</span><span class="c1">#///////////////////////////////</span><span class="w">

</span><span class="c1"># Important note:</span><span class="w">
</span><span class="c1"># The VEP file will start with a header line </span><span class="w">
</span><span class="c1"># that begins with "#" symbol. But this is being ignored by R.</span><span class="w">
</span><span class="c1"># Open the file with text edit and remove that symbol.</span><span class="w">
</span><span class="c1"># Maybe there is an R command to do this on import, whatever is faster.</span><span class="w">

</span><span class="c1"># Split vcf into ~ 10 files. </span><span class="w">
</span><span class="c1"># file 1 "xaa" was anylised on VEP, </span><span class="w">
</span><span class="c1"># download the TXT version, </span><span class="w">
</span><span class="c1"># or unzip my provided example one - cfKJCLRm0eKXsaEG.txt.zip</span><span class="w">
</span><span class="c1"># https://grch37.ensembl.org/Homo_sapiens/Tools/VEP/</span><span class="w">

</span><span class="c1"># Import the VEP output</span><span class="w">
</span><span class="n">vepfile</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="w">
  </span><span class="n">file</span><span class="o">=</span><span class="s2">"cfKJCLRm0eKXsaEG.txt"</span><span class="p">,</span><span class="w">
  </span><span class="n">na.strings</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="s2">"NA"</span><span class="p">),</span><span class="w">
  </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w">
  </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Import drugbank table</span><span class="w">
</span><span class="c1"># the "fill=TRUE" is needed because not all </span><span class="w">
</span><span class="c1"># file lines have the same number of elements.</span><span class="w">
</span><span class="n">drugs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="w">
  </span><span class="n">file</span><span class="o">=</span><span class="s2">"all.txt"</span><span class="p">,</span><span class="w">
  </span><span class="n">na.strings</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="s2">"NA"</span><span class="p">),</span><span class="w">
  </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w">
  </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w">
  </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w"> </span><span class="p">)</span><span class="w">


</span><span class="c1"># We can merge these two files based on 1 common column.</span><span class="w">
</span><span class="c1"># However, the gene name column does not have the same name.</span><span class="w">
</span><span class="c1"># One of them needs to be renamed:</span><span class="w">
</span><span class="c1"># vepfile="SYMBOL"</span><span class="w">
</span><span class="c1"># drugs="Gene.Name"</span><span class="w">

</span><span class="c1"># colnames(df)[colnames(df) == 'oldName'] &lt;- 'newName'</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">vepfile</span><span class="p">)[</span><span class="n">colnames</span><span class="p">(</span><span class="n">vepfile</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"SYMBOL"</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"Gene.Name"</span><span class="w">

</span><span class="c1"># Merge keeping only matches</span><span class="w">
</span><span class="n">merged</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merge</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vepfile</span><span class="p">,</span><span class="w">
                </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">drugs</span><span class="p">,</span><span class="w">
                </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gene.Name"</span><span class="p">,</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">


</span><span class="c1"># Remove empty data "NA"</span><span class="w">
</span><span class="c1"># Install packeges once (comment out then)</span><span class="w">
</span><span class="c1"># Load library each time to use "%&gt;%" (command join) and filtering</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"tidyr"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"dplyr"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">df1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merged</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">drop_na</span><span class="p">(</span><span class="n">Drug.IDs</span><span class="p">)</span><span class="w">

</span><span class="c1"># Make a list of benign variant types that should be removed</span><span class="w">
</span><span class="n">filter_out</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> 
    </span><span class="s1">'synonymous_variant|UTR|NMD_transcript|non_coding|downstream|upstream|intron|mature_miRNA_variant'</span><span class="w">

</span><span class="c1"># Then filter out anything matching these terms.</span><span class="w">
</span><span class="n">df2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df1</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">filter_all</span><span class="p">(</span><span class="n">all_vars</span><span class="p">(</span><span class="o">!</span><span class="n">grepl</span><span class="p">(</span><span class="n">filter_out</span><span class="p">,</span><span class="n">.</span><span class="p">)))</span><span class="w">

</span><span class="c1"># Save an output tsv file for Excel, etc.</span><span class="w">
</span><span class="n">write.table</span><span class="p">((</span><span class="n">df2</span><span class="p">),</span><span class="w"> </span><span class="n">file</span><span class="o">=</span><span class="s1">'./output.tsv'</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w">  </span><span class="n">quote</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>If you complete this process the output will contain a perfectly merged dataset.
So in this simple example it takes just 5 minutes to get from a real genome VCF to possibly druggable target genes (see further note on <em>drug indication</em> below).
The difficulty lies downstream in interpreting which variants can have an effect that would justify the use of the drug.
Anyone implementing a usable version of this method will incur several obstacles;
e.g. are non-coding or synonymous variants worth reporting?, genes have multiple transcripts which means one variant can be both coding and non-coding depending on transcript splicing, etc.
Other sources of sequence data, including the sequences of Watson and Venter;<br />
<a href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/">http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/</a><br />
23andMe open snp data; <a href="https://opensnp.org/genotypes">https://opensnp.org/genotypes</a>.
There are many layers to a this problem to create a usable product.
For example, how to integrate pharmacodynamics, covariats to drug response, contraindications, variant pathogenicity, etc.
However, this is a good start as a learning experience.</p>

<h1 id="annotation">Annotation</h1>
<p><a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor</a> is very useful.
Note: For a real product, the code can run offline (a perl program with a few local library dependencies).
The databases/cache that it uses are a bit too large to include on in a user software.
In the real world you would have to send <em>anonymised</em> packets from the user via an API for accessing the genomic databases hosted on your servers.
Make sure to check their license to see if you can use oftware and databases in a commercial product.<br />
<a href="http://www.ensembl.org/info/about/legal/code_licence.html">http://www.ensembl.org/info/about/legal/code_licence.html</a></p>

<h1 id="drug-indication">Drug indication</h1>
<p>My example used <a href="https://www.drugbank.ca">DrugBank</a> for pharmacogenomic information.
However, it may be safest to use the <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">FDA information</a> as the primary source, but including <a href="https://www.drugbank.ca">DrugBank</a> info is no problem.
Drugs might be either a treatment for a genetic determinant, or a warning for drug usage in someone who also has a genetic variation that might effect their treatment.
The “Labelling Section” listed by FDA might offer the best information.
<a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling </a>
For example, if we go and check the Prescribing Information PDF to compare two drugs we see that</p>

<p><br />
<strong>(1)</strong> One is used to directly block a gene product,<br />
<strong>(2)</strong> The second warns about use with certain genetic complications.</p>

<p><br />
<strong>Drug 1</strong>: <a href="https://www.accessdata.fda.gov/scripts/cder/daf/index.cfm?event=overview.process&amp;varApplNo=761034">Atezolizumab</a> (1),<br />
<strong>Gene</strong>: <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling"><em>CD274</em></a> <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">(PD-L1)</a><br />
<strong>Labeling</strong>: Indications and Usage<br />
<strong>PRESCRIBING</strong> <strong>INFORMATION</strong>: TECENTRIQ (Atezolizumab) is a programmed death-ligand 1 (PD-L1) blocking antibody indicated for the treatment of patients with…
<a href="https://www.accessdata.fda.gov/drugsatfda_docs/label/2016/761034Orig1s000lbl.pdf">linked PDF</a>.<br />
<strong>Explained</strong>: Genetic disorder and the drug to treat it, exactly what you want.</p>

<p><br />
<strong>Drug 2</strong>: Avatrombopag (3)<br />
<strong>Gene</strong>: <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling"><em>PROC</em></a><br />
<strong>Labeling</strong>: Warnings and Precautions<br />
<strong>PRESCRIBING INFORMATION</strong>: Thrombotic/Thromboembolic Complications: DOPTELET is a thrombopoietin (TPO) receptor agonist… Monitor platelet counts and for thromboembolic events
<a href="https://www.accessdata.fda.gov/drugsatfda_docs/label/2019/210238s002lbl.pdf">linked PDF</a>.<br />
<strong>Explained</strong>: Atezolizumab is used to treat thrombocytopenia (low levels of thrombocytes).<br />
You <em>do not want to give</em> this to someone who has <a href="https://omim.org/entry/176860?search=proc&amp;highlight=proc"><em>PROC</em></a><a href="https://omim.org/entry/176860?search=proc&amp;highlight=proc"> deficiency</a>;
their disease is <a href="https://en.wikipedia.org/wiki/Thrombophilia">Thrombophilia</a> (hypercoagulability, or <a href="https://en.wikipedia.org/wiki/Thrombosis">thrombosis</a>).
With this in mind, perhaps an application doing this job could work two ways.
(1) If someone has a genetic disorder, the drug, gene, and Indicated usage appears.
(2) If someone is prescribed a drug a suggestion appears to check their genetics, with a link to the gene and Warnings and Precautions.</p>

<h1 id="optimising-vcf-annotation">Optimising VCF annotation</h1>
<p>The slowest part of the method is VCF annotation.
You can significantly increase the speed by first reducing the input to contain only regions of interst.
That is, prepare a list of coordinates for each gene, and select for those regions in your input VCF or genotype data before annotation (VEP).</p>

<h1 id="how-to-get-coordinates-for-a-gene-list">How to get coordinates for a gene list</h1>
<p>Use Biomart.
Their main server was down when I tried, so I went via Ensembl, data access section:<br />
<a href="http://www.ensembl.org/info/data/biomart/index.html">http://www.ensembl.org/info/data/biomart/index.html</a><br />
Then to use the BioMart data mining tool<br />
<a href="http://www.ensembl.org/biomart/martview/28fdaf82da6c02dc5892f99b757e2c44">http://www.ensembl.org/biomart/martview/28fdaf82da6c02dc5892f99b757e2c44</a><br />
I actually needed the positions using GRCh37 (rather than 38), so I switched to the old Ensembl using<br />
<a href="http://www.ensembl.org/info/website/tutorials/grch37.html">http://www.ensembl.org/info/website/tutorials/grch37.html</a><br />
to get to <a href="http://grch37.ensembl.org/index.html">http://grch37.ensembl.org/index.html</a> 
then the Biomart section<br />
<a href="http://grch37.ensembl.org/biomart/martview/04f009257dadbafbe595155ba910eb5e">http://grch37.ensembl.org/biomart/martview/04f009257dadbafbe595155ba910eb5e</a></p>

<p>Choose DataBase: Genes 93 Dataset: Human Filter -&gt; Gene -&gt; Input external ref ID list -&gt; (change dropdown) Gene
Name paste your list.
e.g. VPS45 PSMB8 BLNK NEFL NLRP7 SMAD4 PSMB9<br />
To set the output type: Attributes -&gt; Gene -&gt; select “gene start”, “gene stop”, “gene name”, or anything extra.
Select the “Results” button at the top and export.
The results can be tsv or csv.
You would have to figure out how to extract the regions from the vcf (sed, grep, awk, R code, etc.).
When I needed this, I used my own tools which required converting to format like this “X:1-2000”, and ordered by number and alphabetic (some positions in the reference genome were patches added later and have an alphanumeric instead of the normal chromosome).
If you use this list to extract regions from a VCF, remember to include all the original VCF header information.</p>

<h1 id="extracting-regions-from-a-vcf-using-a-bed-file">Extracting regions from a VCF using a bed file</h1>
<p>The early part of this tutorial shows how old-school command line tools can be used to extract data. 
Indeed, this may be computationally most efficient but there are some specialised tools that make the process easier in general.
You can use VCFtools to extract specified regions.<br />
<a href="https://vcftools.github.io/man_latest.html">https://vcftools.github.io/man_latest.html</a><br />
You could use a list of defined genome position to reduce the size of your dataset. 
The defined genomic coordinates are generally supplied in a file format called the (BED file](https://en.wikipedia.org/wiki/BED_(file_format))
Note that sometimes the bed file “chrom” ID - the name of the chromosome (e.g. chr3) does not match if the VCF file uses “3” instead of “chr3”.
You might need to edit the bed.
My bed file was like this:<br />
(tab spaced), ref.bed<br />
<br />
chrom    chromStart    chromEnd<br />
1    3549    13555<br />
<br />
And this command ran OK for me to give “output_prefix.recode.vcf”<br /></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$:</span>~/tools/vcftools_0.1.13/bin <span class="se">\</span>
./vcftools <span class="se">\</span>
<span class="nt">--vcf</span> ~/input.vcf <span class="se">\</span>
<span class="nt">--bed</span> ~/ref.bed <span class="se">\</span>
<span class="nt">--out</span> output_prefix <span class="se">\</span>
<span class="nt">--recode</span> <span class="nt">--keep-INFO-all</span>
</code></pre></div></div>
<p>This new VCF will now only contain gene regions that are potentially “druggable”, or at least included on the FDA list.
VCF annotation will be <em>much faster</em> than annotation of the whole genome.</p>

<h1 id="unknown-variants">Unknown variants</h1>
<p>In the majority of situations you will be stuck with <em>variants of unknown significance</em>.
In the absence of tailored analysis and interpretation of each invidual variant, one must often rank unknown variants based on a predicted pathogenicity.
Carefully consider that predictions can be completed wrong and address how such an annotation should be presented. 
One can rank unknown variants based on PHRED-scaled CADD score, highest being more predicted pathogenic.
https://cadd.gs.washington.edu/info<br />
<a href="http://genetics.bwh.harvard.edu/pph2/">Polyphen</a> gives a predicted outcome label and a probability score 0-1 from benign to probably damaging.
See what other pathogenicity prediction tools you can find and estimate how widespread/accepted their usage is.</p>

<h1 id="gene-dosage">Gene dosage</h1>
<p>An important cosideration of variant effect depends on gene dosage.
A <a href="https://en.wikipedia.org/wiki/Dominance_(genetics)">dominant gene</a> may be affected by a single heterozgous variant while a recessive gene may be able to compensate against the negative effect of a heterozyous variant due the presence of a second functional gene copy.
Therefore, the presence of heterozygous or homozygous allele is an important consideration.
Some genes may be sensitive to a <a href="https://en.wikipedia.org/wiki/Zygosity">hemizygous</a> effect, low frequency <a href="https://en.wikipedia.org/wiki/Somatic_(biology)">somatic variants</a>,
<a href="https://en.wikipedia.org/wiki/Mosaic_(genetics)">mosaisism</a>, etc.
<a href="https://en.wikipedia.org/wiki/SNV_calling_from_NGS_data">SNV calling in NGS</a> is a broad topic, but it is safe to say that at least the allele dosage (generally heterozygous or homozygous) should be included in result summary.
If possible, when a gene is linked to a specific disease then the <a href="https://en.wikipedia.org/wiki/Heredity">inheritance type</a> associated with that gene-disease should also be included. 
For example, <a href="https://www.omim.org">https://www.omim.org</a> is a good place to see examles.
The genetic disease <a href="https://www.omim.org/entry/219700?search=cystic%20fibrosis&amp;highlight=cystic%20fibrosi">cystic fibrosis</a> is shown with an inheritance type AR (autosomal recessive) meaning that damaging variants on both gene alleles are required to cause disease. 
The gene <em>cftr</em>, which is the genetic determinant of cystic fibrosis, also has an <a href="https://www.omim.org/entry/602421?search=cftr&amp;highlight=cftr">OMIM page <em>cftr</em></a> that also lists AR inheritance.
An excellent resource for matching gene to disease is the <a href="https://panelapp.genomicsengland.co.uk">https://panelapp.genomicsengland.co.uk</a>.
Individual genes can be explored, or “panels” of disease-specific gene lists can be explored. For example, here is the “<a href="https://panelapp.genomicsengland.co.uk/panels/545/">Bleeding and platelet disorders</a>” panel. 
This shows the “Mode of inheritance” and colour-coded confidence in the disease-gene relationship.
Integrating this type of expert-curated open datasets can be extremely useful.</p>

<h1 id="drug-indication-1">Drug indication</h1>
<p>The indication or warning can be difficult to automate.
For the example drug<br />
<a href="https://www.drugbank.ca/drugs/DB11595">https://www.drugbank.ca/drugs/DB11595</a><br />
the section “Pharmacology” “Indication” has the Indication info.<br />
The FDA label is contained as a PDF attachment in the section “REFERENCES” FDA label Download (245 KB).
If I had to automate the process I would add a URL link for each drug:<br />
for gene name CD274<br />
the drugbank column Drug IDs has these:<br />
DB11595; DB11714; DB11945<br />
and for each ID you could append the ID onto the drugbank URL to link to the webpage
<a href="https://www.drugbank.ca/drugs/">https://www.drugbank.ca/drugs/</a>.
You can do this in R with some technical how-to reading, or do it manually for a quick example like this and removing space to create a web URL.<br />
URL				Drug IDs<br />
https://www.drugbank.ca/drugs/	DB00303<br />
https://www.drugbank.ca/drugs/	DB00114<br />
https://www.drugbank.ca/drugs/	DB00142<br />
https://www.drugbank.ca/drugs/	DB01839<br />
https://www.drugbank.ca/drugs/	DB00125</p>

<h1 id="a-large-scale-example-summary">A large scale example summary</h1>
<p>I do not suggest this for a small project, but if I was to automate subsection requests for real:</p>
<ul>
  <li>[1] Download the whole database (probably a big table sized &gt;100MB) and</li>
  <li>[2] For every query (the Drug ID) extract the sections of interest (indication,  Biologic Classification, Description,  FDA label, etc.)</li>
  <li>[3] Display each section as additional columns in candidate genes table.<br />
<br /></li>
  <li>[1] Would be here: <a href="https://www.drugbank.ca/releases/latest">https://www.drugbank.ca/releases/latest</a></li>
  <li>[2] Would be like this: <a href="https://www.w3schools.com/xml/default.asp">https://www.w3schools.com/xml/default.asp</a><br />
Look at example 2, your database request might be something like:
[get food name = Belgian Waffles, description] or
[get drug ID = DB11595, indication,  Biologic Classification, Description,  FDA label.]
The database request problem can be tricky to optimise but not especially difficult with some experience in SQL-type management.</li>
  <li>[3] For every line in the gene candidate table, do this query request and output the result into the same row.<br />
The final table would be something that includes colunm headers like:<br />
Gene, consequence, variant, amino acid, genome position, CADD, DrugBank ID, Description, Indication, FDA label PDF link, etc.
This table could be ranked based on consequence, CADD score.
The top couple of rows then might be converted into a more readable format like a PDF.</li>
</ul>

<h1 id="more-questions">More questions</h1>

<p>Q: Do we have to infer that in the future everybody will have his genome sequenced ?<br />
You do not have to assume this. It may become true. There could be privacy concerns or social problems arising from genetic prejudice, etc.</p>

<p>Q: Before using our algorithm, patient will have to sequence a part of their genome and thus this is a weakness of our algorithm?<br />
Your tool will provide a service based on genetics.<br />
Option [1] One has their genetics already and will use it for personal medicine.<br />
Option [2] they are prescribed a drug and want to only sequence the genes of interest that could affect this drug.<br />
Option [3] they do not want any genetic info and therefor your product is irrelevant.<br />
Option [4] they do not want their personal genetics, but are willing to estimate their relatedness to others in a genetic database and therefore calculate a probability of accuracy for this drug-gene information. e.g. both parents are Swiss and therefore based on the population they have probability of X that their genotype is Y.</p>

<p>Q: Is it realistic to assume that it will be feasible based on the fact that the sequencing cost is decreasing ?<br />
Irrelevant in this case, but yes, whole genome seq is sometimes &lt; 200CHF will likely be common soon.</p>

<p>Q:  We plan to use polyphen and/or sift in order to discriminate between those types of variants. Is that a good idea?<br />
That is a good start. CADD score is also pretty well known among physicians.
In my opinion, I do not trust the scores often.
However, it is common that for processing a large amount of data, such prediction tools are useful in general.
For example, I might [1] rank first on VEP variant “consequences”; stop mutations with most importance.
[2] Then rank secondly with these values since you cannot interpret most with consequence = missense variant.</p>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling</a></li>
  <li><a href="https://www.pharmgkb.org/view/drug-labels.do">https://www.pharmgkb.org/view/drug-labels.do</a></li>
  <li>Mary V. Relling &amp; William E. Evans. Pharmacogenomics in the clinic. <em>Nature</em> 2015; 526, 343–350. doi: 10.1038/nature15817</li>
  <li>Yip VL, Hawcutt DB, Pirmohamed M. Pharmacogenetic Markers of Drug Efficacy and Toxicity. <em>Clin Pharmacol Ther.</em> 2015;98(1):61-70. doi: 10.1002/cpt.135.</li>
  <li>David R. Adams, M.D., Ph.D.,  and Christine M. Eng, M.D. Next-Generation Sequencing to Diagnose Suspected Genetic Disorders N Engl J Med Oct 2018 doi: 10.1056/NEJMra1711801</li>
</ul>

</p>

  <h2> - </h2>
  <p><h1 id="how-latex-is-used">How LaTeX is used</h1>
<p class="meta">28 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#how-latex-is-used" id="markdown-toc-how-latex-is-used">How LaTeX is used</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>This page simply demonstrates how LaTeX format is included on this website.
The LaTeX equation is written as normal
The code is then converted to SVG by http://latex.codecogs.com
The resulting file is saved into site.baseurl/equations</p>

<p>code cogs for latex
<img src="https://latex.codecogs.com/svg.latex?\Large&space; x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" title="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" /></p>

<p><img src="https://latex.codecogs.com/svg.latex?x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D" alt="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" /></p>

<p><img src="/equations/svg.latex.svg" width="40%" /></p>

</p>

  <h2> - </h2>
  <p><h1 id="linear-regression">Linear regression</h1>
<ul id="markdown-toc">
  <li><a href="#linear-regression" id="markdown-toc-linear-regression">Linear regression</a></li>
  <li><a href="#simple-linear-regression" id="markdown-toc-simple-linear-regression">Simple linear regression</a></li>
  <li><a href="#drawing-the-line" id="markdown-toc-drawing-the-line">Drawing the line</a></li>
  <li><a href="#accuracy-of-the-coefficient-estimates" id="markdown-toc-accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</a></li>
  <li><a href="#se-in-hypothesis-tests-for-coefficients" id="markdown-toc-se-in-hypothesis-tests-for-coefficients">SE in hypothesis tests for coefficients</a></li>
  <li><a href="#model-accuracy" id="markdown-toc-model-accuracy">Model accuracy</a>    <ul>
      <li><a href="#residual-standard-error" id="markdown-toc-residual-standard-error">Residual Standard Error</a></li>
      <li><a href="#r2-statistic" id="markdown-toc-r2-statistic">\(R^2\) Statistic</a></li>
    </ul>
  </li>
</ul>

<p><br />
This page is being formulated currently by plagerising “An Introduction to Statistical Learning”. If you find this page before a lot of changes are complete, then keep this in mind. Once major changes/completion occurs, this message will be updated to references instead.</p>

<h1 id="simple-linear-regression">Simple linear regression</h1>

<p>Predict a quantitative response \(Y\) using a predictor variable \(X\); regressing \(Y\) onto \(X\).
The <em>intercept</em> and <em>splope</em> are written as \(\beta_0\) and \(\beta_1\), respectively.
These are unknwon constants and together are knwon as the model <em>coefficients</em> or <em>parameters</em>.
The simple linear regression is written as</p>

<p>[\label{eq1}\tag{1} 
Y \approx \beta_0 + \beta_1 X]</p>

<p>Values that are estimated are labelled with a “hat”, e.g. 
\(\hat y\) - a prediction of \(Y\) based on \(X = x\).
With some sample data, one can begin to predict \(Y\) based on the predictor \(X\) using the estimated model coefficients \(\hat \beta_0\) and \(\hat \beta_1\);</p>

<p>[\label{eq2}\tag{2} 
\hat y \approx \hat\beta_0 + \hat\beta_1 x]</p>

<p>In this case, the estimated response \(\hat y\) equals the estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) according to a sample of the predictor values (\(x\)).</p>

<h1 id="drawing-the-line">Drawing the line</h1>

<p>The estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) are unknown, therefore we need to get these values to predict \(Y\) based on \(X\).
A number (\(n\)) of obersevations are made where we measure \(X\) and \(Y\). Measurements could be recorded as: (measure 1, x = 5, y = 10), (measure 2, x = 10, y = 20), and so on up to \(n\) obersavations;</p>

<p>[\label{eq3}\tag{3} 
(5,10), (10,20),…, (x_n,y_n)]</p>

<p>[(x_1,y_1), (x_2,y_2),…, (x_n,y_n)]</p>

<p>We want to combine each measurment on a plot so that the line drawn through the data fits well and produces coefficient estimates \(\hat \beta_0\) and \(\hat \beta_1\).
Each measurement (\(i\)) is represented with \(y_i \approx \hat \beta_0 + \hat \beta_1 x_i\) for \(i = 1,2,...,n\).
The ideal result will be a line that fits all points closely. 
The measure of <em>closeness</em> has many topics of interest, but the most common method is to minimise the <em>least squares</em>.</p>

<p>\(e_i = y_i - \hat y_i\) represents the \(i\)th <em>residual</em> - the difference between our \(i\)th response according to our model versus the true \(i\)th observed response.
The <em>residual sum of squares</em> (RSS) is written as
\(\label{eq4}\tag{4} 
RSS = e^2 _1 + e^2 _2 +...+ e^2 _n\)</p>

<p>[RSS = (y_1-\hat\beta0-\hat\beta_1x_1)^2+(y_2-\hat\beta_0-\hat\beta_1x_2)^2+…+(y_n-\hat\beta_0-\hat\beta_1x_n)^2.]</p>

<p>The least squares method uses \(\hat\beta_0 and \hat\beta_1\) such that RSS is minimised. The minimisers are as follows</p>

<p>[\label{eq5}\tag{5} 
 \hat \beta_1  = 
\frac{ 
\sum_{i=1}^{n}	(	xi -\bar{x} )	(yi - \bar{y}	)	} 
{\sum_{i=1}^{n}	(	xi -\bar{x} )^2
}]</p>

<p>[\hat\beta_0 - \bar{y} - \hat\beta_1 \bar{x}]</p>

<p>where the sample means are
\(\bar{y} \equiv \frac{1}{n}\sum_{i=1}^{n} y_i\)
and
\(\bar{x} \equiv \frac{1}{n}\sum_{i=1}^{n} x_i\)
so the equation above (\ref{eq5}) defines the least squares coefficient estimates for simple linear regression.</p>

<h1 id="accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</h1>

<p>We will want to account for error \(\epsilon\) to draw an accurate line. 
\(\epsilon\) would be the mean-zero random error for the relationship between \(Y\) and \(X\) for the unknown function \(f\).</p>

<p>[\label{eq6}\tag{6} 
Y = \beta_0 + \beta_1 X + \epsilon]</p>

<p>This equation represents the <em>population regression line</em>.
We assume that the error term will be independet of \(X\), and the model is a best approximation of the \(X - Y\) relationship.
Equation 5 shows the least squares regression coefficient estimates to model the <em>least squares line</em>.
In real data, the population regression line is not observed but from the sample observations we can calculate the least squares line.</p>

<p>We would like to know how closely these two lines could be from our sampled data. 
The mean of a random variable \(Y\) would be written as \(\mu\).
If we have \(n\) number of observations of \(Y\) (\(y_1, y_2,...,y_n\)) then we can get the estimate of \(\mu\). 
So, the estimate \(\hat \mu = \hat y\), where the sample mean is \(\hat y = \frac{1}{n} \sum_{i=1}^{n} y_i\). 
Obviously the sample mean and true population mean may not be the very same, but they should be close. 
Similary, we want to know the true \(\beta_0\) and \(\beta_1\) in a linear regression so we will estimate the \(\hat\beta_0\) and \(\hat\beta_1\) as shown in eqn \ref{eq5}, the coefficient estimates that define the least squares line.</p>

<p>Likewise, \(\hat\mu\) is used to estimate \(\mu\) in an unbiased way; each estimate might be over- or underestimated but there is no particular bias in one direction from the random samples. 
The least squares coefficient estimates for \(\beta_0\) and \(\beta_1\) will also be unbiased. 
Over a large number of observations the estimate should be very accurate but since we will have a limited number of samples the estimate accuracy estimate should be established. 
This is gerneally done using the <em>standard error</em>.
When considering the estimate mean \(\hat\mu\), the formula for SE(\(\hat\mu\)) will be</p>

<p>[\label{eq7}\tag{7} 
Var( \hat\mu ) = SE( \hat\mu )^2 = \frac{\sigma^2}{n}]</p>

<p>\(\sigma\) (sigma) is the standard deviation for every \(y_i\) of \(Y\) (approximately; further reading on Gaussian error assumption and number of observations).
This SE will shirk with more frequent observations.
In the same direction we can measure the standard errors for \(\hat\beta_0\) and \(\hat\beta_1\)</p>

<p>[\label{eq8}\tag{8} 
SE(\hat\beta_0)^2 = \sigma\left[  \frac{1}{n}</p>
<ul>
  <li>\frac {\bar x^2}{ \sum_{i=1}^{n} (x_i - \bar x)^2 } \right]]</li>
</ul>

<p>[SE(\hat\beta_1)^2 =  \frac{1}{n}</p>
<ul>
  <li>\frac {\sigma^2}{ \sum_{i=1}^{n} (x_i - \bar x)^2 }]</li>
</ul>

<p>and \(\sigma^2 = Var(\epsilon)\). (To be strictly valid, the errors \(\epsilon_i\) for each observation should be uncorrelated with common variance \(\sigma^2\))
The estimate of \(\sigma\) is the <em>residual standard error</em>, and shown by \(RSE = \sqrt{ \frac{RSS}{(n-2)} }\).</p>

<p>In turn, the SE can be used to calculate the <em>confidence interval</em>.
A range of values that will contain the true unknown value of the parameter with 95% probability is the 95% confidence interval. 
This interval for \(\hat\beta_1\) is approximately</p>

<p>[\label{eq9}\tag{9} 
\hat\beta_1 \pm 2 \cdot SE(\hat\beta_1)]</p>

<p>a 95% chance that the true \(\beta_1\) value is in this range.
The same fomula is true for \(\beta_0\), swapping terms.</p>

<h1 id="se-in-hypothesis-tests-for-coefficients">SE in hypothesis tests for coefficients</h1>
<p>The SE can be used for a <em>hypothesis test</em> on the corfficients, i.e. testing the <em>null hypothesis</em> and <em>alternative hypothesis</em>,<br />
\(\label{eq10}\tag{10} 
H_0 - there is not relationship between X and Y\)<br />
\(H_0 : \beta_1 = 0\)
\(H_0 - there is not relationship between X and Y\)
\(H_a : \beta_1 = 0\),
respectively.</p>

<p>Therefore if \(\beta_1 = 0\), according to
\ref(eqn 6)
this equation reduces to 
\(Y = \beta+0 + \epsilon\), and an association is not found between \(X\) and \(Y\).
For the null hypothesis to be true (no association) we must determine if the slope \(\beta_1\), or rather our estimate \(\hat\beta_1\) is non-zero.
If the SE\(\hat\beta_1\) is small then we are confident that \(\hat\beta_1\) is accurate, if for example \(\beta_1 \neg 0\) indicating an 
\(X - Y\) relationship.
If our estimate SE (SE \(\hat\beta_1\)) is large, then we will need to see a much greater 
\(X - Y\) association before rejecting the null hypothesis (association is true).
A <em>t</em>-statistic is used to measure how many <em>standard deviations</em> (SD) 
\(\hat\beta_1\) is from 0.
T-statistic:
\(\label{eq11}\tag{11} 
t = \frac{
\hat\beta_1 - 0}{
SE(\hat\beta_1)}\)
No relationship will give <em>t</em>-distribution \(n - 2\) degrees of freedom.
(bell curve shape, for values of n greater than \(\approx\) 30 it is similar to normal distribution).</p>

<p>The probability of observing any number equal to |<em>t</em>| or larger, assuming \(\beta_1 = 0\), is the <em>p-value</em>.
A very low p-value indicates that it is unlikely that an assocation will be measured by chance, and therefore is like a true association between predictor and response, and we can <em>reject the null hypothesis</em>.</p>

<h1 id="model-accuracy">Model accuracy</h1>
<p>After rejecting the null we want to check if the line fits well - does the model fit the data?
Ttwo related quantities can asses the fit of linear regression; <em>residual standard error</em> (RSE) and \(R^2\)
statistic.</p>
<h2 id="residual-standard-error">Residual Standard Error</h2>

<p>Because of the error term \(\epsilon\),
the LR cannot perfectly predict \(Y\) from \(X\).
RSE is an estimate of the SD of \(\epsilon\).</p>

<p>[\label{eq12}\tag{12} 
RSE = \sqrt{
\frac{1}{n-2} RSS
}
=
\sqrt{
\frac{1}{n-2}
\sum_{i=1}^n 
(y_i - \hat y_i)^2
}]</p>

<p>The RSS part is expanded on the right hand side of the equation since:
$$</p>

<p>RSS = 
\sum_{i=1}^n 
(y_i - \hat y_i)^2</p>

<p>$$.</p>

<p>The RSS is a measure of the <em>lack of fit</em> of the model equation \ref{eq6} to
the dataset.
The value calculated in eqn \ref{eq12} will become smaller when the estimate and true value become closer; 
\(\hat y_1 \approx y_1\) for 
\(i = 1,2,...,n\), in which case the model will fit the data well.</p>

<h2 id="r2-statistic">\(R^2\) Statistic</h2>
<p>We use this all the time; it will be worth getting into the discussion from 
Cosma Rohilla Shalizi in
<a href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/">Advanced Data Analysis from an Elementary Point of View</a>
<em>Chapter 2 The Truth about Linear Regression</em>
``2.2.1.1 R2: Distraction or Nuisance?’’. 
The short summary following just covers the basic “normal” usage, rather getting into the statistica weeds.</p>

<p>[\label{eq }\tag{ }]</p>

<p>[\label{eq }\tag{ }]</p>

<p>[\label{eq }\tag{ }]</p>

</p>

  <h2> - </h2>
  <p><h1 id="statistics-books">Statistics books</h1>
<p>Statistical Inference, Casella &amp; Berger<br />
<a href="https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf">https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf</a><br />
Advanced Data Analysis from an Elementary Point of View by Cosma Rohilla Shalizi. <br />
<a href="http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/">http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/</a>
Mathematical statistics and data analysis<br />
<a href="https://epdf.tips/mathematical-statistics-and-data-analysis65096.html">https://epdf.tips/mathematical-statistics-and-data-analysis65096.html</a><br />
 The Probability and Statistics Cookbook<br />
<a href="http://statistics.zone">http://statistics.zone</a><br />
 The Elements of Statistical Learning Data Mining, Inference, and Prediction<br />
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print11.pdf">https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print11.pdf</a><br />
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a><br />
Core Statistics, Simon N. Wood<br />
<a href="https://people.maths.bris.ac.uk/~sw15190/core-statistics.pdf">https://people.maths.bris.ac.uk/~sw15190/core-statistics.pdf</a><br />
Calculus Made Easy Thompson<br />
<a href="http://www.gutenberg.org/ebooks/33283?msg=welcome_stranger">http://www.gutenberg.org/ebooks/33283?msg=welcome_stranger</a><br />
 Bayesian Approaches to Clinical Trials and Health-Care Evaluation<br />
<a href="http://the-eye.eu/public/Books/Medical/texts/Bayesian%20Approaches%20to%20Clinical%20Trials%20and%20HealthCare%20Eval.%20-%20D.%20Spiegelhalter%20%282004%29%20WW.pdf">http://the-eye.eu/public/Books/Medical/texts/Bayesian%20Approaches%20to%20Clinical%20Trials%20and%20HealthCare%20Eval.%20-%20D.%20Spiegelhalter%20%282004%29%20WW.pdf</a><br />
Bayesian Data Analysis<br />
<a href="https://www.academia.edu/32086149/Bayesian_Data_Analysis_Third_Edition_Gelman_.pdf">https://www.academia.edu/32086149/Bayesian_Data_Analysis_Third_Edition_Gelman.pdf</a><br />
Doing Bayesian data analysis a tutorial with R and BUGS.pdf<br />
<a href="http://www.users.csbsju.edu/~mgass/robert.pdf">http://www.users.csbsju.edu/~mgass/robert.pdf</a></p>

</p>

 -->
</feed>
