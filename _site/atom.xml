<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

 <title>Tom Preston-Werner</title>
 <link href="https://tom.preston-werner.com/atom.xml" rel="self"/>
 <link href="https://tom.preston-werner.com/"/>
 <updated>2020-09-14T17:25:39+02:00</updated>
 <id>https://tom.preston-werner.com/</id>
 <author>
   <name>Tom Preston-Werner</name>
   <email>tom@mojombo.com</email>
 </author>

 
 <entry>
   <title>Unnecessary complexity in precision medicine</title>
   <link href="https://tom.preston-werner.com/2020/05/05/singhgupta_genes.html"/>
   <updated>2020-05-05T00:00:00+02:00</updated>
   <id>https://tom.preston-werner.com/2020/05/05/singhgupta_genes</id>
   <content type="html">&lt;h1 id=&quot;unnecessary-complexity-in-precision-medicine&quot;&gt;Unnecessary complexity in precision medicine&lt;/h1&gt;
&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-in-precision-medicine&quot; id=&quot;markdown-toc-unnecessary-complexity-in-precision-medicine&quot;&gt;Unnecessary complexity in precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#complexity-in-living-systems&quot; id=&quot;markdown-toc-complexity-in-living-systems&quot;&gt;Complexity in living systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#is-complexity-necessary&quot; id=&quot;markdown-toc-is-complexity-necessary&quot;&gt;Is complexity necessary?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#evolution-of-complexity-genic-genomic-and-developmental&quot; id=&quot;markdown-toc-evolution-of-complexity-genic-genomic-and-developmental&quot;&gt;Evolution of complexity: genic, genomic, and developmental&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-in-precision-medicine-1&quot; id=&quot;markdown-toc-unnecessary-complexity-in-precision-medicine-1&quot;&gt;Unnecessary complexity in precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-and-the-etiology-of-cancer&quot; id=&quot;markdown-toc-unnecessary-complexity-and-the-etiology-of-cancer&quot;&gt;Unnecessary complexity and the etiology of cancer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unnecessary-complexity-beyond-precision-medicine&quot; id=&quot;markdown-toc-unnecessary-complexity-beyond-precision-medicine&quot;&gt;Unnecessary complexity beyond precision medicine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusions&quot; id=&quot;markdown-toc-conclusions&quot;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;
Singh_and_Gupta.2020.GenoMed;&lt;br /&gt;
Singh, R.S., Gupta, B.P. Genes and genomes and unnecessary complexity in precision medicine. npj Genom. Med. 5, 21 (2020). https://doi.org/10.1038/s41525-020-0128-1 Open Access
&lt;a href=&quot;https://doi.org/10.1038/s41525-020-0128-1&quot;&gt;https://doi.org/10.1038/s41525-020-0128-1&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Risk factors, gene to phenotype, are more complex that anticipated in early genomics. 
Genotype - phenotype
Molecular and evolutionary complexity.
Molecular contigency - chance driven mutation, redundancy, molectular pathway cross-over (shared phenotypes).
Necessary complexity versus unnecessary - evolutionary baggage due to molecular constraint and blind evolution.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;“Two individuals bearing the same set of risk factors may not present (or exhibit the symptoms of) the same disease. This is the kind of problem that precision medicine is expected to overcome.”
From familial studies, we often see that even clear monogenic, dominant disease does not always manifest equally among family members. “The lack of genetic determinism in disease is the result of molecular contingency.”&lt;/p&gt;

&lt;p&gt;An interesting point is made that I haven’t though much about; “Whether evolution is deterministic, i.e., predictable and repeatable, or contingent and unpredictable is an interesting topic in evolutionary biology (ref 1) (for a recent review see ref. 2)”
Although, this is just a smaller version of a question in physics that I do think about frequently: if every irreducibly small particle/force in the universe could be detected with a mapped trajectory, then we could “see” both forward and reverse in time; causal determinism. Since the particle physics answer will not come anytime soon, a discussion on the evolutionary version is reasonable.&lt;/p&gt;

&lt;p&gt;”..phenotypic evolutionary ‘repeatability’ is common when the founding populations are closely related, perhaps resulting from shared genes and developmental pathways, whereas different outcomes become more likely as historical divergences increase.”&lt;/p&gt;

&lt;p&gt;I believe that in some cases we can produces actionable levels of confidence in mutation prediction to make this a worthwhile endeavour. Knowing that a mutation is likely to occur (and be evolutionarily selected against) and result in disease means that we can prepare a treatment / cure for when it does occur naturally. 
For example, each nucleotide within each individual cell will have a particular probability for the energy required to mutate. 
Many factors affect this value; nucleotide type, methylation, DNA/RNA folding, bound proteins, chemical/UV insult, etc. 
Some of these factors can be estimated based on large studies, and some may never be quantifiable in the real world.
However, we can at least quantify some values to produce mutation predictor value X and and response Y and therefore quantify estimate coefficients, and therefore predict that mutation 1 is more likely to occur than mutation 2.&lt;/p&gt;

&lt;p&gt;Reference is made to “‘Evolution and Tinkering,’ Jacob (ref 4)” who compares natural selection to a tinkerer as opposed to an engineer.
Singh and Gupta propose “a theory of molecular complexity, consisting of necessary and unnecessary complexity in living systems”, that “exceedingly high level of unnecessary complexity” is more due to blind evolution rather than randomness of mutation. 
“Unecessary and unnecessary complexity is relevant to current discussions on genomics and precision medicine.”&lt;/p&gt;

&lt;h1 id=&quot;complexity-in-living-systems&quot;&gt;Complexity in living systems&lt;/h1&gt;
&lt;h1 id=&quot;is-complexity-necessary&quot;&gt;Is complexity necessary?&lt;/h1&gt;
&lt;h1 id=&quot;evolution-of-complexity-genic-genomic-and-developmental&quot;&gt;Evolution of complexity: genic, genomic, and developmental&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-in-precision-medicine-1&quot;&gt;Unnecessary complexity in precision medicine&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-and-the-etiology-of-cancer&quot;&gt;Unnecessary complexity and the etiology of cancer&lt;/h1&gt;
&lt;h1 id=&quot;unnecessary-complexity-beyond-precision-medicine&quot;&gt;Unnecessary complexity beyond precision medicine&lt;/h1&gt;
&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

</content>
 </entry>
 
 <entry>
   <title>Websites for basic genetic variant information</title>
   <link href="https://tom.preston-werner.com/2020/04/27/genetic_mutation_websites.html"/>
   <updated>2020-04-27T00:00:00+02:00</updated>
   <id>https://tom.preston-werner.com/2020/04/27/genetic_mutation_websites</id>
   <content type="html">&lt;h1 id=&quot;websites-for-basic-genetic-variant-information&quot;&gt;Websites for basic genetic variant information&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Identifying pathogenic variants with whole genome and whole exome sequencing is not simple.
Determining the correct filtering method can take some time but it is not the most difficult task.
Validating genetic factors is generally the most time consuming part of this type of research. 
Here is a compilation of some of the websites and resources that I use constantly.
I will begin this simply as a list but continue to contribute information on how to use all of these over time.
I use most of the listed resources daily.
There are several steps in assessing if a gene variant is a good candidate to explain a clinical phenotype.
Often a clear story can be made between the genetic mutation and the resulting phenotype.
Other times (usually) a genetic finding (particularly biallelic mutations) seem to have a direct link to the clinical phenotype but it can take  weeks-months to functionally validate such a finding.
 With that in mind, it is good to have some sort of routine way to quickly assess the possible pathogenicity of a mutation by hand.
I will mostly discuss these in the context of rare mutations which are likely to be under selective pressure and occur at very low frequencies in a healthy population.
 Sites and tools for getting basic genetic information For assessing rare variants Ensembl and Exac (now gnomAD) are my bread and butter.
I haven’t done it yet but I need to set up a hotkey to open a browser with both of these sites simultaneously.
To demonstrate how I like to use these we could use an example. Lets say we have NGS results for a patient with immunodeficiency with coding variants in the gene RAG2.
OK, well known gene, important for antibody production as wells as TCR and BCR development.
Looks good so far.
Let’s see if the variants are common SNPs or could they be likely to cause damage if they are not reported (of course this is in your pipeline automatically but it’s good practice and takes less than 60 seconds; valuable if there is a real person affected by your results).
 After a long time getting confused about transcripts and coordinates, I now know how important it is to accurately report coordinates so there is no confusion if collaborating or reporting the mutations etc. Jump over to Exac.
&lt;a href=&quot;http://exac.broadinstitute.org&quot;&gt;Exac.org&lt;/a&gt;
 This is in a nutshell, the exomes of ~ 60,000 individuals which can be used to view how frequently mutations occur in the general population (unfortunately it is mostly just European but there is some global representation). Exac is vital for checking coding variants.
It covers some of the intronic regions (exon intron splice sites) and some of the upstream and downstream regions.
This is typical for anyone who does whole exome sequencing.
 We mentioned confusion about transcripts and coordinates, Exac automatically loads the coverage as shown for the canonical transcript. Stick with this transcript for reporting or at least for your own notes.
When you head over to Ensembl grab the same one in the transcript table. Update! &lt;a href=&quot;http://gnomad.broadinstitute.org/about&quot;&gt;The Genome Aggregation Database (gnomAD) is online&lt;/a&gt;. This data set is the combination of “123,136 exomes and 15,496 genomes from unrelated individuals” which has “removed individuals known to be affected by severe pediatric disease, as well as their first-degree relatives.”
This is n extremely exciting resource. If you are familiar with Exac then you will know the value of this expansion into gnomAD. &lt;a href=&quot;https://youtu.be/_uRuFZv4JaU&quot;&gt;youtube.com&lt;/a&gt; &lt;a href=&quot;http://www.ensembl.org/index.html&quot;&gt;Ensembl&lt;/a&gt; Any good pipeline will have annotation of the details for any coding variants but it can be pretty valuable to go and look at these again by hand.
It doesn’t take long but can end up saving time in the long run.
If you do it often, the first check on Exac take less than 60 seconds.
The next check on Ensemble will only take 2-3 minutes.
In the quick search I plug in the gene name, luckily for my stuff the top hit is always the human gene (sorry Alpaca researchers).
When you get to the gene page first click is always “Show transcript table.”
If you are lucky there is only one coding transcript like for RAG2.
Most of the time there are about 6 transcripts of wildly varying lengths just to confuse matters.
Go for the transcript ID of the canonical transcript which you noted on Exac.
If you do so then life will be easier when you go to check the coordinates. 
On the left hand side in the table “Transcript-based displays” click “cDNA” shown under “Sequence”.
You can then search through to find the variant and amino acid to see if everything lines up.
You see the cDNA position and amino acid positions overlaid. If you were to pick a different transcript then of course the coordinates are likely to be different.
From here I usually go back to the table on the left of the screen to search Exons.
This obviously just lays out the exon sequences  in blocks along with useful information.
Only a small segment of the introns are displayed.
If you want reference sequences of multiple types just find the down load sequence button and chose FASTA and decide which type you want to display. You would likely have the information based on the annotated NGS data but you may want to look at the different transcripts and Ensembl is the best option. So far (in just a couple of minutes) you could have looked up the allele frequencies, affect of mutation on different transcripts and check that everything that should be reported from the NGS output matches up.
My next step is to check if these variants a already reported.
Everyone has their favourite method, searching PubMed etc. For my topics OMIM often produces good results and a quick search. &lt;a href=&quot;https://www.omim.org/&quot;&gt;Online Mendelian Inheritance in Man&lt;/a&gt;
This is a curated database and is generally very good.
Hopefully it continues to grow for a long time into the future.
Depending on how much you already know about your gene it is sometimes helpful to jump straight down to the “Allelic variants” section (if one is present).
You may find a few variants already reported with a similar phenotype being described as your case.
You may find the exact mutations already reported.
If this is the case then it is likely that it would have taken a few minutes longer to find the same cases on one of the other databases.
Whether you have found that there are many mutations reported similar to those that interest you or if you have found nothing reported so far, my next step is always to run through UniProt.
&lt;a href=&quot;http://www.uniprot.org&quot;&gt;UniProt&lt;/a&gt;
UniProt is so rich in information that there is no need to expnad on it here.
If you have never used it then just pick your favourite protein and go look it up now.
There is (usually) a combination of nearly everything you need to get a quick overview of a protein.
Gene function, functional domains, known variants, reported knockouts/mutagenesis studies, protein structures, expression, localisation, the list goes on.
Actually, as much as I love PDB, I find that using UniProt is usually quicker to check for available PDB protein structures before actually going to PDB to download from the source.
With these four websites one would likely be able to decide how confident you are about a candidate mutation/s.
At least if you are just looking coding variants.
Assessing non-coding regions is much messier business.
From here on in validation of a mutation can require a widely variable amount of functional work.
One thing is certain however, Sanger sequencing will be needed to confirm your NGS finding. &lt;a href=&quot;https://www.youtube.com/watch?v=3amsDkyiMu8&quot;&gt;youtube.com&lt;/a&gt; &lt;a href=&quot;https://github.com/gantzgraf/autoprimer3/releases/tag/v3.0.2&quot;&gt;Autoprimer3&lt;/a&gt;
Autoprimer3 is an excellent application that you can use to design primers for a gene of interest.
It is super quick for producing primers to be used on genomic DNA for “any UCSC genome and design PCR/sequencing primers to genes or genome coordinates”.
As an example I timed myself to see how long it takes to get a primer list for all exons of the gene RAG2 and a reference sequence from default genomic coordinates on hg38 while avoiding SNPs based on dbsnp142.
It took me 46 seconds to open the application and produce a primer list and reference sequence.
Less than 1% of the time I may have to go and redesign a primer manually because of an awkward sequence or a patient’s DNA may have some uncommon variant at the primer site. 
Depending on which supplier you order oligos from, Sanger sequencing to confirm a variant by found during NGS can be done within 3 days; about 90 seconds to design and order the oligos, a day or 2 until they are delivered,  and a day to PCR and sequence.
The explanation may be a bit long winded here but this app is excellent.
Just give it a try if you do any routine PCR or sequencing for coding variant.
As the name suggests, it is a simple version of Primer3 but super quick.
&lt;a href=&quot;https://software.broadinstitute.org/gatk/&quot;&gt;Genome Analysis Toolkit: Variant Discovery in High-Throughput Sequencing Data.&lt;/a&gt;
GATK most useful to jump straight to: &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/tooldocs/&quot;&gt;Tool Documentation Index&lt;/a&gt; Genome hg38 &lt;a href=&quot;http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=chr7:142299011,142813287&quot;&gt;(TCR region as example)&lt;/a&gt;
&lt;a href=&quot;https://gpgtools.org&quot;&gt;GPGtools&lt;/a&gt;
for sending sensitive patient info.
&lt;a href=&quot;https://www.gnupg.org&quot;&gt;GnuPG&lt;/a&gt; is GPL licensed alternative to the PGP suite for sending sensitive patient info.
See also Pretty good privacy for academic data. &lt;a href=&quot;http://www.umd.be/HSF3/HSF.html&quot;&gt;Human splice finder&lt;/a&gt; Illumina-Pipeline-V2 (“Version 2 of Illumina pipeline that incorporates &lt;a href=&quot;https://github.com/nirav99/Illumina-Pipeline-V2/blob/master/IlluminaPipelineCASAVA1_8.pdf&quot;&gt;CASAVA 1.8”)&lt;/a&gt; Sequence Manipulation Suite http://www.coccidia.icb.usp.br/sms2/index.html
Sequence Ontology http://www.sequenceontology.org
UCSC Genome Bioinformatics FAQ https://genome.ucsc.edu/FAQ/FAQformat
UCSC Table Browser https://genome.ucsc.edu/cgi-bin/hgTables
MutScan - https://github.com/OpenGene/MutScan
Detect and visualise target mutations by scanning FastQ files directly. Very useful if you are interested in some certain mutations but saves the time it would take to normally through your pipeline. &lt;/p&gt;
&lt;h2 id=&quot;communities-and-learning&quot;&gt;Communities and learning&lt;/h2&gt;
&lt;p&gt;No need to reinvent the wheel here. Stephen Turner has a better list of resources than I will produce with his post ”Staying Current in Bioinformatics &amp;amp; Genomics: 2017 Edition.” 
http://www.gettinggeneticsdone.com/2017/02/staying-current-in-bioinformatics-genomics-2017.html
Essentially it boils down to the journals, Twitter, some expert blogs, and several genomics communities.
The journals and other sites I like to follow are detailed here. When all directed into a single feed I think it produces an essential resource for most genetics/bioinformatics scientists.
Literature of Interest - In this post I show the use of Feedly to condense all the litereature that I follow into a single source and allow the option to view by category.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-and-cloud-computing&quot;&gt;Machine learning and cloud computing&lt;/h2&gt;
&lt;p&gt;In this post I have started to gather some of the resources I like to use and topics that I find interesting.
Some other links tagged on at the end:
BioStarts - Bioinformatics academic community https://www.biostars.org
Useful bash Bioinformatics one-liners&lt;br /&gt;
https://github.com/stephenturner/oneliners&lt;br /&gt;
Efficient R programming https://csgillespie.github.io/efficientR/
Cheat sheets for data.   science http://www.datasciencecentral.com/…
RStudio Cheat Sheets  
https://www.rstudio.com/resources/cheatsheets/#515&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Test - make a new md file</title>
   <link href="https://tom.preston-werner.com/2020/04/26/tester.html"/>
   <updated>2020-04-26T00:00:00+02:00</updated>
   <id>https://tom.preston-werner.com/2020/04/26/tester</id>
   <content type="html">&lt;h1 id=&quot;test---make-a-new-md-file&quot;&gt;Test - make a new md file&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;10 Nov 2016 - San Francisco&lt;/p&gt;

&lt;p&gt;This is a story about a company called
&lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk&lt;/a&gt; (pronounced “sneak”), their
founder Guy Podjarny, my decision to become one of their advisors, and how they
are going to help save you from malevolent agents trying to steal your digital
stuff.&lt;/p&gt;

&lt;p&gt;If you’re anything like me, you’re simultaneously terrified and in awe of the
increasing commonality of large corporate security breaches. Even big names like
Ebay, Home Depot, Anthem, JP Morgan Chase, Target, LinkedIn, Dropbox, and Yahoo
are falling victim to sophisticated attacks. If you spend even a few minutes
looking into it, you’ll be shocked at how frequently these breaches are
happening now. The fine folks at Information is Beautiful have an excellent
interactive visualization of the &lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;World’s Biggest Data
Breaches&lt;/a&gt;
over the last twelve years, in case you want to read all the gory details and
never get a restful night of sleep ever again:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;
  &lt;img src=&quot;/images/posts/2016-11-10/breaches.png&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve used a fair number of emotionally charged words above that might be
triggering your FUD detectors right about now. But be advised: it’s not paranoia
when they really are out to get you. If recent, extremely high profile (and
subsequently weaponized) breaches like those of the Clinton Campaign and the DNC
aren’t enough to make you want to air gap your entire life, then I envy your
steely-eyed mettle and implore you to teach me your meditation techniques.&lt;/p&gt;

&lt;p&gt;The fact is, security is hard. And it’s getting harder every day. To win, you
have to get it right every single time. To lose (and lose big), you only have to
screw it up once.&lt;/p&gt;

&lt;p&gt;During my years at GitHub, I spent a lot of time assembling a dedicated security
team, managing security audits and penetration tests, and working to establish a
culture of security awareness amongst our development team. All of this is
challenging and expensive, especially for a young company. Even worse, it’s the
kind of investment that’s totally invisible when it’s working, making it hard to
sustain until that crucial and terrible moment you end up on the front page of
Hacker News as the latest victim.&lt;/p&gt;

&lt;p&gt;A year ago I was contemplating this, especially the difficult proposition of
having developers, furious at work on new features, constantly maintain
awareness of security vulnerabilities they might be inadvertently weaving into
the product. Web application developers are generally not security experts, and
though I would love to live in a world where that wasn’t true, it’s just not a
realistic expectation. Meanwhile, modern development means an increasing
reliance on 3rd party code. Even a small Rails app will probably have 300 or
more gem dependencies after a few months of development. It’s even more in the
nodejs world. This level of modularization and code reuse, driven by the
explosion of high quality open source over the last decade, is amazing and I
absolutely love it, but it comes at a security expense.&lt;/p&gt;

&lt;p&gt;Open source projects are not known for their excellent security records.
Vulnerabilities like &lt;a href=&quot;http://heartbleed.com/&quot;&gt;Heartbleed&lt;/a&gt; and
&lt;a href=&quot;https://blog.cloudflare.com/inside-shellshock/&quot;&gt;Shellshock&lt;/a&gt; painfully
demonstrate the idea that “given enough eyeballs, all bugs are shallow” is
completely false. In fact, due to a flaw in YAML, Rails had a &lt;a href=&quot;http://blog.codeclimate.com/blog/2013/01/10/rails-remote-code-execution-vulnerability-explained/&quot;&gt;pretty extreme
remote code execution
vulnerability&lt;/a&gt;
for years. If you were running any version of Rails prior to the fix, you were
vulnerable. This stuff is real, and as responsible developers, we need to be
more proactive about it.&lt;/p&gt;

&lt;p&gt;Luckily, at the time I was pondering these matters, I ran into Guy Podjarny. As
a former cofounder of Blaze.io and then CTO of Web Experience at Akamai (which
acquired Blaze.io), Guy intimately understands the impact of security on today’s
web developers. He was working on an automated tool to scan and fix security
vulnerabilities in 3rd party dependencies. I was intrigued. They already had a
way to scan nodejs projects and look for known security vulnerabilities in the
dependency tree and automatically upgrade or patch affected libraries. I thought
this was pretty cool, but it was his vision for what automated security tooling
could be that sold me on him and his company. I can’t talk much about that
now, but just know that what Snyk is today is just the tip of what will
become an intelligent and proactive bodyguard for your entire codebase.&lt;/p&gt;

&lt;p&gt;A few months ago, Snyk released GitHub integration to make it fantastically
simple to hook up your repos to Snyk and, my favorite feature: the ability to
monitor your repo for future vulnerabilities and then &lt;strong&gt;automatically submit a
pull request&lt;/strong&gt; with the suggested package upgrade or hotfix patch (nodejs only for
now).&lt;/p&gt;

&lt;p&gt;Today, &lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk announced support for
Ruby&lt;/a&gt;. Take a look at that blog post,
it does an awesome job of explaining how simple it is to set up and what the
generated pull requests look like. It’s totally free for open source projects,
and extremely cheap insurance for your important projects.&lt;/p&gt;

&lt;p&gt;Make no mistake, 3rd party code is a clear and present danger to your business.
If you don’t know if you’re vulnerable, then you must assume that you are and
take steps to protect yourself. Snyk makes it easy.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Cryptography</title>
   <link href="https://tom.preston-werner.com/2020/04/26/cryptography.html"/>
   <updated>2020-04-26T00:00:00+02:00</updated>
   <id>https://tom.preston-werner.com/2020/04/26/cryptography</id>
   <content type="html">&lt;h1 id=&quot;cryptography&quot;&gt;Cryptography&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;26 Apr 2020&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#breaking&quot;&gt;Breaking crypto&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aes&quot;&gt;AES is the most important current encryption method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#quantum&quot;&gt;Quantum computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thoughts&quot;&gt;Some thoughts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reading&quot;&gt;Reading list on more advanced topics&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Like most of the posts on this blog, this will be a work in progress. Cryptography is a topic which I stumbled upon and really enjoy.
For the reading list skip to the end of this page.
There is a long interesting history which would appeal to a casual reader.
Most people would be familiar with stories about crypto during WWII, particularly because of movies like
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.imdb.com/title/tt2084970/&quot;&gt;The Imitation Game&lt;span id=&quot;titleYear&quot;&gt;(2014).
&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;
Cracking of Enigma falls into the espionage theme along with stories like that of books from  &lt;span class=&quot;author notFaded&quot; style=&quot;color: #0000ff;&quot;&gt;&lt;span class=&quot;a-declarative&quot;&gt;&lt;a href=&quot;https://www.amazon.co.uk/Ben-Macintyre/e/B001H6WAL8/ref=dp_byline_cont_book_1&quot;&gt;Ben Macintyre.&lt;/a&gt; &lt;/span&gt;&lt;/span&gt;One of my favorites is: &lt;span id=&quot;productTitle&quot; class=&quot;a-size-large&quot;&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.amazon.co.uk/d/Books/Operation-Mincemeat-True-Story-Changed-Course-World/1408809214&quot;&gt;Operation Mincemeat&lt;/a&gt;:&lt;/span&gt; The True Spy Story That Changed the Course of World War II. &lt;/span&gt;
Reading so much about the non-fiction side of this topic ultimately led me to the &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_James_Bond_novels_and_short_stories&quot;&gt;Ian Flemming novels&lt;/a&gt;.&lt;/span&gt; Of course I had seen all the movies as a kid, and like most, loved them. Reading the novels in their order of release ended up being much more fun than I have with most fictional book series perhaps because of Flemming’s true involvements during WWII.&lt;/p&gt;

&lt;p&gt;Gentle brushing against the topic of cryptography with these classical stories  eventually lead me to an interest in modern crypto. Real, crypto! Like most sciences portrayed in popular culture, it really only gets interesting when you get into the technical reading.
Computerphile has several good videos on cryptographic topics. This video describes SHA1 in a way that I find quite interesting. This is just about hashing methods but it is a lovely introduction to crypto.
&lt;a href=&quot;https://www.youtube.com/watch?v=DMtFhACPnTY&quot;&gt;www.youtube.com&lt;/a&gt;
Another video from the series now gets to actual crypto  in the same entertaining way; &lt;a href=&quot;https://www.youtube.com/watch?v=jkV1KEJGKRA&quot;&gt;End to End Encryption (E2EE)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;breaking&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;breaking-crypto&quot;&gt;Breaking crypto&lt;/h1&gt;
&lt;p&gt;Learning the basics of crypto and how it’s broken is best done at the same time. Of course actually breaking the crypto is difficult. But understanding it doesn’t have to be. To learn this you can quickly get the main points about modular arithmetic, exponentiation, and periods in this video.
&lt;a href=&quot;https://www.youtube.com/watch?v=12Q3Mrh03Gk&quot;&gt;Shor’s algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think getting a clear grasp on the topic relies on getting used to modular arithmetic. For example on a clock we use Mod 12. If you get up at 12am and the time is now 1pm well then obviously you have been up for 13 hours. &lt;strong&gt;13 mod 12 = 1&lt;/strong&gt;.
You know just as well that if you get up at 7am and it is now 8pm you have also been up for 13 hours. We can do this in our head very easily, and can do other examples easily too if you get over the initial confusion. &lt;strong&gt;A/B = Q remainder R&lt;/strong&gt;. In some cases we only care about the remainder R. In that case we say: &lt;strong&gt;A modulo B is equal to R&lt;/strong&gt;. Where B is referred to as the modulus (or mod for short).
The only difficulty is when the numbers become quite large.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/what-is-modular-arithmetic&quot;&gt;Here is a page that describes this very well. &lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This video is summed up with the 4 steps. The reason that RSA works is because Step 2, finding the period, takes a very long time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png&quot; alt=&quot;rsa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Quantum computing is expected to dramatically speed up this step.
Another good intro video that has some interesting discussion on Diffie-Hellman key exchange was given at the Chaos Communication Congress:
J. Alex Halderman, Nadia Heninger: Logjam: Diffie-Hellman, discrete logs, the NSA, and you.&lt;/p&gt;

&lt;p&gt;“Earlier this year, we discovered that Diffie-Hellman key exchange – cornerstone of modern cryptography – is less secure in practice than the security community believed. In this talk, we’ll explain how the NSA is likely exploiting this weakness to allow it to decrypt connections to at least 20% of HTTPS websites, 25% of SSH servers, and 66% of IPsec VPNs.”
&lt;a href=&quot;https://www.youtube.com/watch?v=mS8gm-_rJgM&quot;&gt;www.youtube.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Applied Cypto Handbook is a very good technical introduction and probably as far as a general reader will ever want to go.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cacr.uwaterloo.ca/hac/&quot;&gt;http://cacr.uwaterloo.ca/hac/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;aes&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;aes-is-the-most-important-current-encryption-method&quot;&gt;AES is the most important current encryption method&lt;/h1&gt;
&lt;p&gt;This lecture is the perfect intro if you already know what methods are out there.
&lt;a href=&quot;https://www.youtube.com/watch?v=x1v2tX4_dkQ&quot;&gt;www.youtube.com&lt;/a&gt;
The accompanying book is worth the money if you’re looking for a textbook. The table of contents is available on amazon.
&lt;span style=&quot;color: #0000ff;&quot;&gt; &lt;a href=&quot;http://www.crypto-textbook.com&quot;&gt;http://www.crypto-textbook.com.&lt;/a&gt;&lt;/span&gt;
Here is a link to &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Évariste_Galois#Galois_theory&quot;&gt;Galois’ wiki.&lt;/a&gt;&lt;/span&gt;
This might lead you down a wiki rabbit hole learning about interesting maths.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;quantum&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;quantum-computing&quot;&gt;Quantum computing&lt;/h1&gt;
&lt;p&gt;Here are simply two videos from PBS that will be more entertaining and succinct at discussing this really interesting topic than I.
&lt;a href=&quot;https://www.youtube.com/watch?v=IrbJYsep45E&quot;&gt;How quantum computing works&lt;/a&gt;
How might quantum computing destroy computer security?
&lt;a href=&quot;https://www.youtube.com/watch?v=wUwZZaI5u0c&quot;&gt;By utilising Shor’s algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A fun little topic brought up in this videos is: that quantum Fourier transform uses resonance to amplify the basic state associated with the correct period.
If you’re reading this site then it’s likely that you are a biologist.
If that is the case you may be more familiar with protein structures than quantum mechanics.
I first became introduced to the practical application of Fourier transformation while learning nuclear magnetic resonance (NMR) spectroscopy for protein structuring.
Of course, you don’t actually have to learn it to do NMR.
It happens automatically during data analysis but most people in the field surely would still like to know the details.
Wiki has a great page: &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;&gt;https://en.wikipedia.org/wiki/Fourier_transform&lt;/a&gt;
&lt;img src=&quot;https://dylanlawlessblog.files.wordpress.com/2017/02/ft.png&quot; alt=&quot;ft&quot; /&gt;
“In NMR an exponentially shaped free induction decay (FID) signal is acquired in the time domain and Fourier-transformed to a Lorentzian line-shape in the frequency domain.”&lt;/p&gt;

&lt;p&gt;The next main point addressed in this video is: Complex roots of unity.
This is introduced quite well in the video.
If you have never seen anything like this before then I highly recommend the short book by Feynman;
&lt;a href=&quot;https://www.amazon.co.uk/dp/B00BR40XJ6?ref_=k4w_oembed_ICZkE7ckZ2ZUfR&amp;amp;tag=kpembed-20&amp;amp;linkCode=kpd&quot;&gt;QED: The Strange Theory of Light and Matter&lt;/a&gt;
&lt;!--
(https://www.amazon.com/QED-Strange-Princeton-Science-Library/dp/0691164096/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1494067439&amp;sr=1-1&amp;keywords=qed+the+strange+theory+of+light+and+matter)
--&gt;
In no way does this little book talk about quantum computing.
If fact it is pretty old now and is not the kind of thing that professionals will be using for reference.
Why would I suggest this for someone who is new to the topic? Well it is an extremely fun introduction to the topic of QED and lays the foundation of ideas that have become mainstream over the next 30 years.
Understanding some basic ideas will leave you open to recognise more complex applications, especially important if you want to only look at the basics of quantum computing.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;thoughts&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;some-thoughts&quot;&gt;Some thoughts&lt;/h1&gt;
&lt;p&gt;This talk at Google by Peter Warren Singer based on his book,
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Cybersecurity-Cyberwar-Everyone-Needs-Know%C2%AE/dp/0199918112&quot;&gt;Cybersecurity and Cyberwar&lt;/a&gt;&lt;/span&gt;,
may be a pretty interesting watch for anyone into technology security in some way. This is not a technical talk, more of something to get you into the mindset up why this topic may be interesting.
&lt;a href=&quot;https://www.youtube.com/watch?v=h0SXO5KUZIo&quot;&gt;www.youtube.com&lt;/a&gt;
&lt;a href=&quot;https://www.cl.cam.ac.uk/~rja14/book.html&quot;&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;https://www.cl.cam.ac.uk/~rja14/book.html&lt;/span&gt;&lt;/a&gt; Security Engineering — The Book&lt;/p&gt;

&lt;p&gt;The cryptopals crypto challenges are a fun way to learn some hands on application of cryptographic techniques. &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.cryptopals.com/&quot;&gt;http://www.cryptopals.com/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Announcing the first SHA1 collision on February 23, 2017.
This was a really big event in the crypto community.
I think many people in the cyber security field assume that experiments and findings in public and academic research are a few years behind government capabilities.
Take from that what you will.
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html&quot;&gt;https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;There are countless reasons why crypto is interesting.
The applications range from the most mundane day to day requirements in the modern world such as banking, personal communication, the use of medical data (which I post about here &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://dylanlawlessblog.wordpress.com/2017/02/21/pretty-good-privacy-for-academic-data/&quot;&gt;Pretty good privacy for academic data&lt;/a&gt;&lt;/span&gt;) all the way out to the most hypothetical academic applications.
An interesting point to think about is the journey that each data packet makes across the mystical &lt;em&gt;internet&lt;/em&gt;.
Most electronic communications travel across a number of boarders and further distances than most people will travel in their entire life.
Our world would not run very smoothly if all communication was sent in a readable format with no protection.
Here is some basic info on the infrastructure require for modern electronic communication:
&lt;a href=&quot;https://www.youtube.com/watch?v=DKHZKTRyzeg&quot;&gt;www.youtube.com&lt;/a&gt;,
&lt;a href=&quot;https://www.youtube.com/watch?v=0TZwiUwZwIE&quot;&gt;www.youtube.com&lt;/a&gt;
And the &lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.submarinecablemap.com&quot;&gt;Submarine Cable Map&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;While we’re on the topic, I found this video on the Cornwall cable landing station.
The physical infrastructure and engineering requirements of global communication are sometimes easy to forget if one spends more time on computer programming or mathematics
&lt;a href=&quot;https://www.youtube.com/watch?v=K_nnUbX7uuQ&quot;&gt;www.youtube.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;reading&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;reading-list-on-more-advanced-topics&quot;&gt;Reading list on more advanced topics&lt;/h1&gt;
&lt;p&gt;/r/crypto wiki&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/crypto/wiki/index&quot;&gt;https://www.reddit.com/r/crypto/wiki/index&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Textbook: An Introduction to Mathematical Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Cryptology ePrint Archive&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://eprint.iacr.org&quot;&gt;http://eprint.iacr.org&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Handbook of Applied Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cacr.uwaterloo.ca/hac/&quot;&gt;http://cacr.uwaterloo.ca/hac/&lt;/a&gt;&lt;/span&gt;
Goldreich: The Foundations of Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html&quot;&gt;http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Handbook of Elliptic and Hyperelliptic Curve Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.hyperelliptic.org/HEHCC/&quot;&gt;http://www.hyperelliptic.org/HEHCC/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
eBACS: ECRYPT Benchmarking of Cryptographic Systems&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://bench.cr.yp.to&quot;&gt;http://bench.cr.yp.to&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Mihir Bellare and Shafi Goldwasser’s Lecture Notes&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf&quot;&gt;http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Charm: A tool for rapid cryptographic prototyping&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.charm-crypto.com/index.html&quot;&gt;http://www.charm-crypto.com/index.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
eHash Wiki&lt;br /&gt;
&lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_Hash_Function_Zoo&quot;&gt;Hash Function Zoo&lt;/a&gt;&lt;br /&gt;
and the &lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo&quot;&gt;SHA-3 Zoo&lt;/a&gt;&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page&quot;&gt;http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Cryptology ePrint Archive&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://eprint.iacr.org&quot;&gt;http://eprint.iacr.org&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
IACR Conferences (Crypto, Eurocrypt, Asiacrypt)&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.iacr.org/conferences/&quot;&gt;http://www.iacr.org/conferences/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
IEEE Symposium on Security and Privacy (There are loads of papers and talks on YouTube under Program of past events)&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.ieee-security.org/TC/SP2017/past.html&quot;&gt;https://www.ieee-security.org/TC/SP2017/past.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Crypto Stack Exchange&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://crypto.stackexchange.com&quot;&gt;https://crypto.stackexchange.com&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Blogpost so-you-want-to-crypto&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.seancassidy.me/so-you-want-to-crypto.html&quot;&gt;https://www.seancassidy.me/so-you-want-to-crypto.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Authenticated Encryption Zoo&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo&quot;&gt;https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Helger Lipmaa Cryptology Pointers&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://kodu.ut.ee/~lipmaa/crypto/&quot;&gt;http://kodu.ut.ee/~lipmaa/crypto/&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Free Course: Applied Cryptography&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.udacity.com/course/applied-cryptography--cs387&quot;&gt;https://www.udacity.com/course/applied-cryptography–cs387&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Kerckhoffs’s principle&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle&quot;&gt;https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Schneier’s Law&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://www.schneier.com/blog/archives/2011/04/schneiers_law.html&quot;&gt;https://www.schneier.com/blog/archives/2011/04/schneiers_law.html&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
crypto blogs from David Wong’s github&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;https://github.com/mimoo/crypto_blogs&quot;&gt;https://github.com/mimoo/crypto_blogs&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
Shor in Haskell The Quantum IO Monad&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf&quot;&gt;http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;
The Quipper Language: programming language for quantum computing&lt;br /&gt;
&lt;span style=&quot;color: #0000ff;&quot;&gt;&lt;a href=&quot;http://www.mathstat.dal.ca/%7Eselinger/quipper/&quot;&gt;http://www.mathstat.dal.ca/%7Eselinger/quipper/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;!-- 
![encrpytdata](https://i.imgur.com/UubXs0H.gif)
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Placeholder post</title>
   <link href="https://tom.preston-werner.com/2015/06/19/replicated.html"/>
   <updated>2015-06-19T00:00:00+02:00</updated>
   <id>https://tom.preston-werner.com/2015/06/19/replicated</id>
   <content type="html">&lt;h1 id=&quot;placeholder-post&quot;&gt;Placeholder post&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;19 Jun 2015 - San Francisco&lt;/p&gt;
&lt;p&gt;Placeholder text.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ViN-qkcovL0&amp;amp;feature=youtu.be&quot;&gt;youtube video link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;email &lt;a href=&quot;mailto:contact@replicated.bla&quot;&gt;contact@replicated.bla&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Example weblink
&lt;a href=&quot;https://snyk.io/blog/welcome-ruby-users/&quot;&gt;Snyk&lt;/a&gt;.
Video link &lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;World’s Biggest Data
Breaches&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Embedded image from other source
&lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/&quot;&gt;
  &lt;img src=&quot;/images/posts/2016-11-10/breaches.png&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 

 <!--
 
  <h2> - </h2>
  <p><h1 id="cryptography">Cryptography</h1>
<p class="meta">28 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#cryptography" id="markdown-toc-cryptography">Cryptography</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#breaking-crypto" id="markdown-toc-breaking-crypto">Breaking crypto</a></li>
  <li><a href="#aes-is-the-most-important-current-encryption-method" id="markdown-toc-aes-is-the-most-important-current-encryption-method">AES is the most important current encryption method</a></li>
  <li><a href="#quantum-computing" id="markdown-toc-quantum-computing">Quantum computing</a></li>
  <li><a href="#some-thoughts" id="markdown-toc-some-thoughts">Some thoughts</a></li>
  <li><a href="#reading-list-on-more-advanced-topics" id="markdown-toc-reading-list-on-more-advanced-topics">Reading list on more advanced topics</a></li>
</ul>

<!--
cross link and label
1. [Introduction](#introduction)
<a name="introduction"></a>
-->
<h1 id="introduction">Introduction</h1>
<p>Like most of the posts on this blog, this will be a work in progress. Cryptography is a topic which I stumbled upon and really enjoy.
For the reading list skip to the end of this page.
There is a long interesting history which would appeal to a casual reader.
Most people would be familiar with stories about crypto during WWII, particularly because of movies like
<span style="color: #0000ff;"><a href="http://www.imdb.com/title/tt2084970/">The Imitation Game<span id="titleYear">(2014).
</span></a></span>
Cracking of Enigma falls into the espionage theme along with stories like that of books from  <span class="author notFaded" style="color: #0000ff;"><span class="a-declarative"><a href="https://www.amazon.co.uk/Ben-Macintyre/e/B001H6WAL8/ref=dp_byline_cont_book_1">Ben Macintyre.</a> </span></span>One of my favorites is: <span id="productTitle" class="a-size-large"><span style="color: #0000ff;"><a href="https://www.amazon.co.uk/d/Books/Operation-Mincemeat-True-Story-Changed-Course-World/1408809214">Operation Mincemeat</a>:</span> The True Spy Story That Changed the Course of World War II. </span>
Reading so much about the non-fiction side of this topic ultimately led me to the <span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/List_of_James_Bond_novels_and_short_stories">Ian Flemming novels</a>.</span> Of course I had seen all the movies as a kid, and like most, loved them. Reading the novels in their order of release ended up being much more fun than I have with most fictional book series perhaps because of Flemming’s true involvements during WWII.</p>

<p>Gentle brushing against the topic of cryptography with these classical stories  eventually lead me to an interest in modern crypto. Real, crypto! Like most sciences portrayed in popular culture, it really only gets interesting when you get into the technical reading.
Computerphile has several good videos on cryptographic topics. This video describes SHA1 in a way that I find quite interesting. This is just about hashing methods but it is a lovely introduction to crypto.
<a href="https://www.youtube.com/watch?v=DMtFhACPnTY">www.youtube.com</a>
Another video from the series now gets to actual crypto  in the same entertaining way; <a href="https://www.youtube.com/watch?v=jkV1KEJGKRA">End to End Encryption (E2EE)</a>.</p>

<p><a name="breaking"></a></p>
<h1 id="breaking-crypto">Breaking crypto</h1>
<p>Learning the basics of crypto and how it’s broken is best done at the same time. Of course actually breaking the crypto is difficult. But understanding it doesn’t have to be. To learn this you can quickly get the main points about modular arithmetic, exponentiation, and periods in this video.
<a href="https://www.youtube.com/watch?v=12Q3Mrh03Gk">Shor’s algorithm</a></p>

<p>I think getting a clear grasp on the topic relies on getting used to modular arithmetic. For example on a clock we use Mod 12. If you get up at 12am and the time is now 1pm well then obviously you have been up for 13 hours. <strong>13 mod 12 = 1</strong>.
You know just as well that if you get up at 7am and it is now 8pm you have also been up for 13 hours. We can do this in our head very easily, and can do other examples easily too if you get over the initial confusion. <strong>A/B = Q remainder R</strong>. In some cases we only care about the remainder R. In that case we say: <strong>A modulo B is equal to R</strong>. Where B is referred to as the modulus (or mod for short).
The only difficulty is when the numbers become quite large.
<span style="color: #0000ff;"><a href="https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/what-is-modular-arithmetic">Here is a page that describes this very well. </a></span></p>

<p>This video is summed up with the 4 steps. The reason that RSA works is because Step 2, finding the period, takes a very long time:</p>

<!-- ![rsa](https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png) -->
<p><img src="https://dylanlawlessblog.files.wordpress.com/2017/02/rsa.png" width="40%" /></p>

<p>Quantum computing is expected to dramatically speed up this step.
Another good intro video that has some interesting discussion on Diffie-Hellman key exchange was given at the Chaos Communication Congress:
J. Alex Halderman, Nadia Heninger: Logjam: Diffie-Hellman, discrete logs, the NSA, and you.</p>

<p>“Earlier this year, we discovered that Diffie-Hellman key exchange – cornerstone of modern cryptography – is less secure in practice than the security community believed. In this talk, we’ll explain how the NSA is likely exploiting this weakness to allow it to decrypt connections to at least 20% of HTTPS websites, 25% of SSH servers, and 66% of IPsec VPNs.”
<a href="https://www.youtube.com/watch?v=mS8gm-_rJgM">www.youtube.com</a></p>

<p>Applied Cypto Handbook is a very good technical introduction and probably as far as a general reader will ever want to go.
<span style="color: #0000ff;"><a href="http://cacr.uwaterloo.ca/hac/">http://cacr.uwaterloo.ca/hac/</a></span></p>

<p><a name="aes"></a></p>
<h1 id="aes-is-the-most-important-current-encryption-method">AES is the most important current encryption method</h1>
<p>This lecture is the perfect intro if you already know what methods are out there.
<a href="https://www.youtube.com/watch?v=x1v2tX4_dkQ">www.youtube.com</a>
The accompanying book is worth the money if you’re looking for a textbook. The table of contents is available on amazon.
<span style="color: #0000ff;"> <a href="http://www.crypto-textbook.com">http://www.crypto-textbook.com.</a></span>
Here is a link to <span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/Évariste_Galois#Galois_theory">Galois’ wiki.</a></span>
This might lead you down a wiki rabbit hole learning about interesting maths.</p>

<p><a name="quantum"></a></p>
<h1 id="quantum-computing">Quantum computing</h1>
<p>Here are simply two videos from PBS that will be more entertaining and succinct at discussing this really interesting topic than I.
<a href="https://www.youtube.com/watch?v=IrbJYsep45E">How quantum computing works</a>
How might quantum computing destroy computer security?
<a href="https://www.youtube.com/watch?v=wUwZZaI5u0c">By utilising Shor’s algorithm</a></p>

<p>A fun little topic brought up in this videos is: that quantum Fourier transform uses resonance to amplify the basic state associated with the correct period.
If you’re reading this site then it’s likely that you are a biologist.
If that is the case you may be more familiar with protein structures than quantum mechanics.
I first became introduced to the practical application of Fourier transformation while learning nuclear magnetic resonance (NMR) spectroscopy for protein structuring.
Of course, you don’t actually have to learn it to do NMR.
It happens automatically during data analysis but most people in the field surely would still like to know the details.
Wiki has a great page: <a href="https://en.wikipedia.org/wiki/Fourier_transform">https://en.wikipedia.org/wiki/Fourier_transform</a></p>

<p><img src="https://dylanlawlessblog.files.wordpress.com/2017/02/ft.png" width="50%" /><br />
“In NMR an exponentially shaped free induction decay (FID) signal is acquired in the time domain and Fourier-transformed to a Lorentzian line-shape in the frequency domain.”</p>

<p>The next main point addressed in this video is: Complex roots of unity.
This is introduced quite well in the video.
If you have never seen anything like this before then I highly recommend the short book by Feynman;
<a href="https://www.amazon.co.uk/dp/B00BR40XJ6?ref_=k4w_oembed_ICZkE7ckZ2ZUfR&amp;tag=kpembed-20&amp;linkCode=kpd">QED: The Strange Theory of Light and Matter</a>
<!--
(https://www.amazon.com/QED-Strange-Princeton-Science-Library/dp/0691164096/ref=sr_1_1?s=books&ie=UTF8&qid=1494067439&sr=1-1&keywords=qed+the+strange+theory+of+light+and+matter)
-->
In no way does this little book talk about quantum computing.
If fact it is pretty old now and is not the kind of thing that professionals will be using for reference.
Why would I suggest this for someone who is new to the topic? Well it is an extremely fun introduction to the topic of QED and lays the foundation of ideas that have become mainstream over the next 30 years.
Understanding some basic ideas will leave you open to recognise more complex applications, especially important if you want to only look at the basics of quantum computing.</p>

<p><a name="thoughts"></a></p>
<h1 id="some-thoughts">Some thoughts</h1>
<p>This talk at Google by Peter Warren Singer based on his book,
<span style="color: #0000ff;"><a href="https://www.amazon.com/Cybersecurity-Cyberwar-Everyone-Needs-Know%C2%AE/dp/0199918112">Cybersecurity and Cyberwar</a></span>,
may be a pretty interesting watch for anyone into technology security in some way. This is not a technical talk, more of something to get you into the mindset up why this topic may be interesting.
<a href="https://www.youtube.com/watch?v=h0SXO5KUZIo">www.youtube.com</a>
<a href="https://www.cl.cam.ac.uk/~rja14/book.html"><span style="color: #0000ff;">https://www.cl.cam.ac.uk/~rja14/book.html</span></a> Security Engineering — The Book</p>

<p>The cryptopals crypto challenges are a fun way to learn some hands on application of cryptographic techniques. <span style="color: #0000ff;"><a href="http://www.cryptopals.com/">http://www.cryptopals.com/</a></span></p>

<p>Announcing the first SHA1 collision on February 23, 2017.
This was a really big event in the crypto community.
I think many people in the cyber security field assume that experiments and findings in public and academic research are a few years behind government capabilities.
Take from that what you will.
<span style="color: #0000ff;"><a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html">https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html</a></span></p>

<p>There are countless reasons why crypto is interesting.
The applications range from the most mundane day to day requirements in the modern world such as banking, personal communication, the use of medical data (which I post about here <span style="color: #0000ff;"><a href="https://dylanlawlessblog.wordpress.com/2017/02/21/pretty-good-privacy-for-academic-data/">Pretty good privacy for academic data</a></span>) all the way out to the most hypothetical academic applications.
An interesting point to think about is the journey that each data packet makes across the mystical <em>internet</em>.
Most electronic communications travel across a number of boarders and further distances than most people will travel in their entire life.
Our world would not run very smoothly if all communication was sent in a readable format with no protection.
Here is some basic info on the infrastructure require for modern electronic communication:
<a href="https://www.youtube.com/watch?v=DKHZKTRyzeg">www.youtube.com</a>,
<a href="https://www.youtube.com/watch?v=0TZwiUwZwIE">www.youtube.com</a>
And the <span style="color: #0000ff;"><a href="http://www.submarinecablemap.com">Submarine Cable Map</a></span>.</p>

<p>While we’re on the topic, I found this video on the Cornwall cable landing station.
The physical infrastructure and engineering requirements of global communication are sometimes easy to forget if one spends more time on computer programming or mathematics
<a href="https://www.youtube.com/watch?v=K_nnUbX7uuQ">www.youtube.com</a>.</p>

<p><a name="reading"></a></p>
<h1 id="reading-list-on-more-advanced-topics">Reading list on more advanced topics</h1>
<p>/r/crypto wiki<br />
<span style="color: #0000ff;"><a href="https://www.reddit.com/r/crypto/wiki/index">https://www.reddit.com/r/crypto/wiki/index</a></span><br />
Textbook: An Introduction to Mathematical Cryptography<br />
<span style="color: #0000ff;"><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5F72903FBACA6DF57799612526CC437F?doi=10.1.1.182.9999&amp;rep=rep1&amp;type=pdf</a></span><br />
Cryptology ePrint Archive<br />
<span style="color: #0000ff;"><a href="http://eprint.iacr.org">http://eprint.iacr.org</a></span><br />
Handbook of Applied Cryptography<br />
<span style="color: #0000ff;"><a href="http://cacr.uwaterloo.ca/hac/">http://cacr.uwaterloo.ca/hac/</a></span>
Goldreich: The Foundations of Cryptography<br />
<span style="color: #0000ff;"><a href="http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html">http://www.wisdom.weizmann.ac.il/%7Eoded/foc-drafts.html</a></span><br />
Handbook of Elliptic and Hyperelliptic Curve Cryptography<br />
<span style="color: #0000ff;"><a href="http://www.hyperelliptic.org/HEHCC/">http://www.hyperelliptic.org/HEHCC/</a></span><br />
eBACS: ECRYPT Benchmarking of Cryptographic Systems<br />
<span style="color: #0000ff;"><a href="http://bench.cr.yp.to">http://bench.cr.yp.to</a></span><br />
Mihir Bellare and Shafi Goldwasser’s Lecture Notes<br />
<span style="color: #0000ff;"><a href="http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf">http://cseweb.ucsd.edu/%7Emihir/papers/gb.pdf</a></span><br />
Charm: A tool for rapid cryptographic prototyping<br />
<span style="color: #0000ff;"><a href="http://www.charm-crypto.com/index.html">http://www.charm-crypto.com/index.html</a></span><br />
eHash Wiki<br />
<a href="http://ehash.iaik.tugraz.at/wiki/The_Hash_Function_Zoo">Hash Function Zoo</a><br />
and the <a href="http://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo">SHA-3 Zoo</a><br />
<span style="color: #0000ff;"><a href="http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page">http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page</a></span><br />
Cryptology ePrint Archive<br />
<span style="color: #0000ff;"><a href="http://eprint.iacr.org">http://eprint.iacr.org</a></span><br />
IACR Conferences (Crypto, Eurocrypt, Asiacrypt)<br />
<span style="color: #0000ff;"><a href="http://www.iacr.org/conferences/">http://www.iacr.org/conferences/</a></span><br />
IEEE Symposium on Security and Privacy (There are loads of papers and talks on YouTube under Program of past events)<br />
<span style="color: #0000ff;"><a href="https://www.ieee-security.org/TC/SP2017/past.html">https://www.ieee-security.org/TC/SP2017/past.html</a></span><br />
Crypto Stack Exchange<br />
<span style="color: #0000ff;"><a href="https://crypto.stackexchange.com">https://crypto.stackexchange.com</a></span><br />
Blogpost so-you-want-to-crypto<br />
<span style="color: #0000ff;"><a href="https://www.seancassidy.me/so-you-want-to-crypto.html">https://www.seancassidy.me/so-you-want-to-crypto.html</a></span><br />
Authenticated Encryption Zoo<br />
<span style="color: #0000ff;"><a href="https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo">https://aezoo.compute.dtu.dk/doku.php?id=AE%20Zoo</a></span><br />
Helger Lipmaa Cryptology Pointers<br />
<span style="color: #0000ff;"><a href="http://kodu.ut.ee/~lipmaa/crypto/">http://kodu.ut.ee/~lipmaa/crypto/</a></span><br />
Free Course: Applied Cryptography<br />
<span style="color: #0000ff;"><a href="https://www.udacity.com/course/applied-cryptography--cs387">https://www.udacity.com/course/applied-cryptography–cs387</a></span><br />
Kerckhoffs’s principle<br />
<span style="color: #0000ff;"><a href="https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle">https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle</a></span><br />
Schneier’s Law<br />
<span style="color: #0000ff;"><a href="https://www.schneier.com/blog/archives/2011/04/schneiers_law.html">https://www.schneier.com/blog/archives/2011/04/schneiers_law.html</a></span><br />
crypto blogs from David Wong’s github<br />
<span style="color: #0000ff;"><a href="https://github.com/mimoo/crypto_blogs">https://github.com/mimoo/crypto_blogs</a></span><br />
Shor in Haskell The Quantum IO Monad<br />
<span style="color: #0000ff;"><a href="http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf">http://www.cs.nott.ac.uk/%7Epsztxa/publ/qio.pdf</a></span><br />
The Quipper Language: programming language for quantum computing<br />
<span style="color: #0000ff;"><a href="http://www.mathstat.dal.ca/%7Eselinger/quipper/">http://www.mathstat.dal.ca/%7Eselinger/quipper/</a></span></p>

<!-- 
![encrpytdata](https://i.imgur.com/UubXs0H.gif)
-->
<p><small></small></p>

<p>&lt;/small&gt;</p>
</p>

  <h2> - </h2>
  <p><h1 id="rag">RAG</h1>
<ul id="markdown-toc">
  <li><a href="#rag" id="markdown-toc-rag">RAG</a></li>
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#methods" id="markdown-toc-methods">Methods</a>    <ul>
      <li><a href="#population-genetics-and-data-sources" id="markdown-toc-population-genetics-and-data-sources">Population genetics and data sources</a></li>
      <li><a href="#data-processing" id="markdown-toc-data-processing">Data processing</a></li>
      <li><a href="#raw-data-availability-and-analysis-script" id="markdown-toc-raw-data-availability-and-analysis-script">Raw data availability and analysis script</a></li>
      <li><a href="#data-visualisation" id="markdown-toc-data-visualisation">Data visualisation</a></li>
      <li><a href="#validation-of-mrf-against-functional-data" id="markdown-toc-validation-of-mrf-against-functional-data">Validation of MRF against functional data</a></li>
    </ul>
  </li>
  <li><a href="#results" id="markdown-toc-results">Results</a>    <ul>
      <li><a href="#rag1-and-rag2-conservation-and-mutation-rate-residue-frequency" id="markdown-toc-rag1-and-rag2-conservation-and-mutation-rate-residue-frequency">RAG1 and RAG2 conservation and mutation rate residue frequency</a></li>
      <li><a href="#mrf-scores-select-for-confirmed-variants-in-human-disease" id="markdown-toc-mrf-scores-select-for-confirmed-variants-in-human-disease">MRF scores select for confirmed variants in human disease</a></li>
      <li><a href="#top-candidate-variants-require-validation" id="markdown-toc-top-candidate-variants-require-validation">Top candidate variants require validation</a></li>
      <li><a href="#false-positives-in-transib-domains-do-not-negatively-impact-prediction" id="markdown-toc-false-positives-in-transib-domains-do-not-negatively-impact-prediction">False positives in <em>Transib</em> domains do not negatively impact prediction</a></li>
      <li><a href="#mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants" id="markdown-toc-mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants">MRF predicts RAG deficiency amongst PID patients harbouring rare variants</a></li>
      <li><a href="#mrf-supplements-pathogenicity-prediction-tools-for-translational-research" id="markdown-toc-mrf-supplements-pathogenicity-prediction-tools-for-translational-research">MRF supplements pathogenicity prediction tools for translational research</a></li>
      <li><a href="#figure" id="markdown-toc-figure">Figure</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ul>
      <li><a href="#main-supplemental-data-table" id="markdown-toc-main-supplemental-data-table">Main supplemental data table</a></li>
      <li><a href="#clinical-relevance-of-top-candidates" id="markdown-toc-clinical-relevance-of-top-candidates">Clinical relevance of top candidates</a></li>
      <li><a href="#supplemental-analysis-tables" id="markdown-toc-supplemental-analysis-tables">Supplemental analysis tables</a></li>
      <li><a href="#protein-structure-application" id="markdown-toc-protein-structure-application">Protein structure application</a></li>
      <li><a href="#median-cadd-score-per-residue" id="markdown-toc-median-cadd-score-per-residue">Median CADD score per residue</a></li>
      <li><a href="#supplemental-file" id="markdown-toc-supplemental-file">Supplemental file</a></li>
      <li><a href="#genome-wide-and-disease-specific-application" id="markdown-toc-genome-wide-and-disease-specific-application">Genome-wide and disease-specific application</a></li>
      <li><a href="#bayesian-probability" id="markdown-toc-bayesian-probability">Bayesian probability</a></li>
    </ul>
  </li>
</ul>

<h1 id="abstract">Abstract</h1>
<p>While widespread genome sequencing ushers in a new era of preventive medicine, the tools for predictive genomics are still lacking.
Time and resource limitations mean that human diseases remain uncharacterised because of an inability to predict clinically relevant genetic variants.
A strategy of targeting highly conserved protein regions is used commonly in functional studies. 
However, this benefit is lost for rare diseases where the attributable genes are mostly conserved. 
An immunological disorder exemplifying this challenge occurs through damaging mutations in <em>RAG1</em> and <em>RAG2</em> which presents at an early age with a distinct phenotype of life-threatening immunodeficiency or autoimmunity. 
Many tools exist for variant pathogenicity prediction but these cannot account for the probability of variant occurrence. 
Here, we present a method that predicts the likelihood of mutation for every amino acid residue in the RAG1 and RAG2 proteins. 
Population genetics data from approximately 146,000 individuals was used for rare variant
analysis. 
Forty-four known pathogenic variants reported in patients and recombination activity measurements from 110 RAG1/2 mutants were used to
validate calculated scores. 
Probabilities were compared with 98
currently known human cases of disease. 
A genome sequence dataset of 558
patients who have primary immunodeficiency but that are negative for RAG
deficiency were also used as validation controls. 
We compared the
difference between mutation likelihood and pathogenicity prediction. 
Our
method builds a map of most probable mutations allowing pre-emptive
functional analysis. 
This method may be applied to other diseases with
hopes of improving preparedness for clinical diagnosis.</p>

<p>Authors:
Dylan Lawless,
Hana Lango Allen,
James Thaventhiran,
‘NIHR BioResource–Rare Diseases Consortium’,
Flavia Hodel,
Rashida Anwar,
Jacques Fellay,
Jolan E. Walter’,
Sinisa Savic.</p>

<h4 id="ethics-statement">Ethics statement</h4>

<p>The study was performed in accordance with the Declaration of Helsinki.
The NIHR BioResource projects were approved by Research Ethics
Committees in the UK and appropriate national ethics authorities in
non-UK enrolment centres.</p>

<h4 id="abbreviations">Abbreviations</h4>

<p>BCR (B-cell receptor), CADD (combined annotation dependent depletion),
CID-G/A (combined immunodeficiency with granuloma and/or autoimmunity),
GWAS (genome-wide association studies), HGMD (human gene mutation
database), \({M}_{r}\) (mutation rate), MRF (mutation rate residue
frequency), PID (primary immunodeficiency), pLI (probability of being
loss-of-function intolerant), <em>RAG1</em> (recombination activating gene 1),
\({R}_{f}\) (residue frequency), RNH (RNase H), RSS (recombination signal
sequence), SCID (severe combined immunodeficiency), TCR (T-cell
receptor).</p>

<h1 id="introduction">Introduction</h1>

<p>Costs associated with genomic investigations continue to reduce [payne2018cost] while the richness of data generated increases.
Globally, the adoption of wide scale genome sequencing implies that all new-born infants may receive screening for pathogenic genetic variants
in an asymptomatic stage, pre-emptively  [kwan2014newborn]. 
The one dimensionality of individual genomes is now being expanded by the possibility of massive parallel sequencing for somatic variant analysis and by single-cell or lineage-specific genotyping; culminating in a genotype spectrum. 
In whole blood, virtually every nucleotide position may be mutated across \(10^5\) cells 
[Liggett208066]. 
Mapping one’s genotype across multiple cell types and at several periods during a person’s life may soon be feasible 
[clark2018scnmt]. 
Such genotype snapshots might allow for prediction and tracking of somatic, epigenetic, and transcriptomic profiling.</p>

<p>The predictive value of genomic screening highly depends on the computation tools used for data analysis and its correlation with functional assays or prior clinical experience. 
Interpretation of that data is especially challenging for rare human genetic disorders;
candidate disease-causing variants that are predicted as pathogenic
often require complex functional investigations to confirm their significance. 
There is a need for predictive genomic modelling with aims to provide a reliable guidance for therapeutic intervention for patients harbouring genetic defects for life-threatening disease before the illness becomes clinically significant.</p>

<p>The study of predictive genomics is exemplified by consideration of gene essentiality, accomplished by observing intolerance to loss-of-function variants. 
Several gene essentiality scoring methods are available for both the coding and non-coding genome  [bartha2017human]. 
Approximately 3,000 human genes cannot tolerate the loss of one allele
[bartha2017human]. 
The greatest hurdle in monogenic disease is the interpretation of variants of unknown significance while functional validation is a major time and cost investment for laboratories investigating rare disease.</p>

<p>Severe, life-threatening immune diseases are caused by genetic
variations in almost 300 genes
[picard2018international conley2014discovery] however, only a small percentage of disease causing variants have been characterised using functional studies. 
Several robust tools are in common usage for predicting variant pathogenicity.  Compared to methods for pathogenicity prediction, a void remains for predicting mutation probability, essential for efficient pre-emptive validation. 
Our investigation aims to apply predictive genomics as a tool to identify genetic variants that are most likely to be seen in patient cohorts.</p>

<p>We present the first application of our novel approach of predictive genomics using Recombination activating gene 1 (RAG1) and RAG2 deficiency as a model for a rare primary immunodeficiency (PID) caused by autosomal recessive variants. 
<em>RAG1</em> and <em>RAG2</em> encode lymphoid-specific proteins that are essential for V(D)J recombination.
This genetic recombination mechanism is essential for a robust immune response by diversification the T and B cell repertoire in the thymus and bone marrow, respectively  [schatz1989v oettinger1990rag].
Deficiency of RAG1  [mombaerts1992rag] and RAG2  [shinkai1992rag] in mice causes inhibition of B and T cell development. 
[schwarz1996rag ]
formed the first publication reporting that RAG mutations in humans causes severe combined immunodeficiency (SCID), and deficiency in peripheral B and T cells. 
Patient studies identified a form of immune dysregulation known as Omenn syndrome
[de1991restricted rieux1998highly]. 
The patient phenotype includes multi-organ infiltration with oligoclonal, activated T cells. 
The first reported cases of Omenn syndrome identified infants with hypomophic RAG variants which retained partial recombination activity
[villa1998partial]. 
RAG deficiency can be measured by in vitro quantification of recombination activity
[lawless2018prevalence lee2014systematic tirosh2018recombination].
Hypomorphic <em>RAG1</em> and <em>RAG2</em> mutations, responsible for residual V(D)J recombination activity (on average 5-30%), result in a distinct phenotype of combined immunodeficiency with granuloma and/or autoimmunity (CID-G/A)
[kwan2014newborn walter2015broad schuetz2008immunodeficiency].</p>

<p>Human RAG deficiency has traditionally been identified at very early ages due to the rapid drop of maternally-acquired antibody in the first six months of life. 
A loss of adequate lymphocyte development quickly results in compromised immune responses. 
More recently, we have found that RAG deficiency is also found for some adults living with PID
[lawless2018prevalence].</p>

<p><em>RAG1</em> and <em>RAG2</em> are highly conserved genes but disease is only reported with autosomal recessive inheritance. 
Only 44% of amino acids in RAG1 and RAG2 are reported as mutated on GnomAD and functional validation of candidate variants is difficult 
[lek2016analysis].
Pre-emptive selection of residues for functional validation is a major challenge; 
a selection based on low allele frequency alone is infeasible since the majority of each gene is highly conserved. 
A shortened time between genetic analysis and diagnosis means that treatments may be delivered earlier. 
RAG deficiency may present with diverse phenotypes and treatment strategies vary. 
With such tools, early intervention may be prompted. 
Some patients could benefit from hematopoietic stem cell transplant  [john2016unrelated] when necessary while others may be provided mechanism-based treatment  [casanova2014guidelines]. 
Here, we provide a new method for predictive scoring that was validated against
groups of functional assay values, human disease cases, and population genetics data. 
We present the list of variants most likely seen as future determinants of RAG deficiency, meriting functional investigation.</p>

<h1 id="methods">Methods</h1>
<h2 id="population-genetics-and-data-sources">Population genetics and data sources</h2>

<p>GnomAD (version r2.0.2)  [lek2016analysis] was queried for the canonical transcripts of <em>RAG1</em> and <em>RAG2</em> from population genetics data of approximately 146,000 individuals; ENST00000299440 (<em>RAG1</em>) 1586 variants, GRCh37 11:36532259-36614706 and ENST00000311485 (<em>RAG2</em>) 831 variants, GRCh37 11:36597124 - 36619829. 
Data was filtered to contain the variant effect identifiers: frameshift, inframe deletion, inframe
insertion, missense, stop lost, or stop gained. 
Reference transcripts were sourced from Ensembl in the FASTA format amino acid sequence for transcript RAG1-201 ENST00000299440.5 HGNC:9831 and transcript RAG2-201 ENST00000311485.7 HGNC:9832.
These sequences were converted to their three-letter code format using <em>One to Three</em> from the Sequence Manipulation Suite (SMS2) 
[stothard2000sequence]. 
Combined Annotation Dependent Depletion (CADD) scores were sourced from (Nov 2018) and are reported by 
[kircher2014general ]. 
The dataset used was “All possibleSNVs” from whole genome data, from which we extracted the data for coding regions of <em>RAG1</em> and <em>RAG2</em>. 
We used the Human Gene Mutation Database (HGMD) from the Institute of Medical Genetics in Cardiff as a pre-defined source of known RAG deficiency cases (Feb 2019, free access
version to NM_000448.2.) 
[stenson2014human]. 
Data was formatted into CSV and imported into R for combined analysis with PHRED-scaled CADD scores and the main dataframe. 
The crystal structure render of DNA bound RAG complex was produced with data from RCSB Protein Data Bank
(3jbw.pdb)  [ru2015molecular]. 
Structures were visualised using the software VMD from the Theoretical and Computational Biophysics Group
[Humphrey1996vmd], imaged with Tachyon rendering 
[Stone1998Ane], and colour mapped using our scoring method.</p>

<h2 id="data-processing">Data processing</h2>

<p>The population genetics input dataset used GnomAD variant allele frequencies and reference sequences processed as CSV files, cleaned and sorted to contain only amino acid codes, residue numbers, alternate residues, alternate allele frequencies, and a score of 0 or 1 to indicate presence or absence of variants where 1 represented none reported. 
An annotation column was also provided to label where multiple alternate variants existed. 
Statistics and calculation steps are listed in order in Supplemental Tables E3-E8.</p>

<p>The percentage of conserved residues was calculated (55.99% of amino acids contained no reported variants in RAG1, 55.98% in RAG2 (
Table E4). 
Basic protein statistics were generated using canonical reference transcript sequences of <em>RAG1</em> and <em>RAG2</em> with the SMS2 tool <em>Protein Stats</em> 
[stothard2000sequence]. 
The resulting pattern percentage value was converted to a frequency (decimal 0-1) based on the number of residues per protein to generate the residue frequency
(\({R}_{f}\)). 
The \({R}_{f}\) values were found for both proteins as shown in <strong>Table E5</strong> and summarised in <strong>Table E6</strong>.</p>

<p>The count of variants per residue were found for both proteins and the mutation rates (\({M}_{r}\)) per residue were calculated as shown in
** Table E7**. 
\({M}_{r}\) was found by counting the number of mutations per residue in a window, sized to contain each protein
individually. 
For genome-wide application the window size may be increased or decreased. 
In this case the window consisted of only the coding regions. 
The \({M}_{r}\) values were then converted to the frequencies based on the number of residues per protein. 
Separate, and overlapping windows could also be used based on genome phase data and regions of linkage disequilibrium to account for non-random association of alleles at different loci; this might be particularly important for disorders with multiple genetic determinants.</p>

<p>The \({M}_{r}\) and \({R}_{f}\) multiply to give the raw mutation rate residue frequency (MRF) value (**Table E8
). 
This value is also shown in **Tables mrf.assay] and 
Table:1
. 
Our investigation used a Boolean score \(C\) to account for the presence or absence of a mutation in the general population; 0 for any variant existing in the population and 1 for conserved residues.
\(C \times {M}_{r} \times {R}_{f}\), in our case, produced the MRF score for conserved residues. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a></p>

<p>(ii)** illustrates the raw MRF as a histogram and the MRF, after applying \(C\), as a heatmap.</p>

<p>An important consideration for future application is whether to use this Boolean score or instead use a discrete variable which accounts for the
true allele frequency in the general population.<br />
In the clinical setting, the likelihood of <em>de novo</em> mutations and inherited mutations have different impacts when considering recessive and dominant diseases.
A patient is more likely to inherit a variant that exists even at a very low frequency than to acquire a random <em>de novo</em> mutation. 
Therefore, a value representing an allele frequency may be used to replace \(C\) in many investigations, particularity when considering variants that exist
at low rates. 
PRHED-scaled CADD score data consisted of nucleotide level values. 
For comparison with MRF, the median CADD scores were averaged per codon as demonstrated in text. 
A summary of data processing and analysis is illustrated in 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">method_map</a>.</p>

<h2 id="raw-data-availability-and-analysis-script">Raw data availability and analysis script</h2>

<p>The supplemental files 
<em>“Raw_data_R_analysis_for_figures”</em> 
contains all raw data and analysis methods used to produce figures (except illustrations in Figures 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>\ and 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>).
<em>“data_analysis.R”</em> 
is an R script that contains the methods used to produce figures. 
Each of the input data CSV files are explained on first usage within the analysis script. 
Running 
<em>“data_analysis.R”</em> 
from within the same directory as the associated input data CSV files will replicate analysis.</p>

<h2 id="data-visualisation">Data visualisation</h2>

<p>For our visualisation of MRF scores, small clusters of high MRF values were of more appealing than individual highly conserved residues.
Therefore, we applied a 1% average filter where values were averaged over a sliding window of N number of residues (10 in the case of RAG1, 6
in the case of RAG2). 
For a clear distinction of MRF clusters, a cut-off threshold was applied at the 75\(^{th}\) percentile (e.g. 
0.0168 in RAG1) as shown in heatmaps in  FIGURE
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii)** and</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>. 
The gene heatmaps for coding regions in <em>RAG1</em>
and <em>RAG2</em> (Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a> 
) were populated with (i)
Boolean \(C\) score from population genetics data, (ii) raw MRF scores, and (iii) MRF clusters with 1% average and cut-off threshold. 
GraphPad Prism was used for heatmaps. 
The data used for heatmaps is available in
TABLE Table:1
and in the supplemental R source to allow for alternative visualisations. 
An example of alternative output for non-R users is shown in
&lt;FIGURE RAG_MRF_map. 
Adobe Illustrator and Photoshop were used for protein domain illustrations in Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iv).</p>

<h2 id="validation-of-mrf-against-functional-data">Validation of MRF against functional data</h2>

<p>The recombination activity of RAG1 and RAG2 was previously measured on known or candidate pathogenic variants
[lee2014systematic lawless2018prevalence tirosh2018recombination].
Briefly, the pathogenicity of variants in <em>RAG1</em> and <em>RAG2</em> was measured functionally <em>in vitro</em> by either expression of RAG1 and RAG2 in
combination with a recombination substrate plasmid containing recombination signal sequence (RSS) sites which are targeted by RAG complex during normal V(D)J recombination, or Abelson virus-transformed Rag2-/- pro-B cells with an RSS-flanked inverted GFP cassette.
Recombination events were assessed by quantitative real-time PCR using comparative CT or expression of GFP evaluated by flow cytometry,
respectively. 
The inverse score of recombination activity (0-100%) was used to quantify pathogenicity of variants in our study. 
Comparison between known pathogenicity scores and MRF was done by scaling MRF scores from 0-100% (100% being highest probability of occurring as
damaging). 
A data and analysis is summarised in Figure
FIGURE 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">method_map</a></p>

<h1 id="results">Results</h1>
<h2 id="rag1-and-rag2-conservation-and-mutation-rate-residue-frequency">RAG1 and RAG2 conservation and mutation rate residue frequency</h2>

<p>Variant probability prediction is dependent on population genetics data.
Our study queried GnomAD 
[lek2016analysis] 
to identify conserved residues using a Boolean score \(C\) of 0 (present in population) or 1 (conserved). 
The gene-specific mutation rate \({M}_{r}\) of each residue was calculated from allele frequencies. 
The gene-specific residue frequency \({R}_{f}\) represented the frequency of a residue occurring per gene, acquired by converting gene residue percentage (from the SMS2 tool
<em>Protein stats</em>) to a frequency (decimal 0-1) 
[stothard2000sequence].
Together the values were used to calculate the most probable disease-causing variants which have not yet been identified in patients.
We termed the resulting score a mutation rate residue frequency, where \(MRF = {C} \times {M}_{r} \times {R}_f\). 
This score represents the likelihood that a clinically relevant mutation will occur.</p>

<p>Figure 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a> 
presents the most probable unidentified
disease-causing variants in <em>RAG1/2</em>. 
Variants with a low MRF may still be damaging but resources for functional validation are best spent on
gene regions with high MRF. 
Clusters of conserved residues are shown in
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
 (i)** and are generally considered important for protein structure or function. 
However, these clusters do not predict the likelihood of mutation. 
Raw MRF scores are presented in
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(ii)
Histograms illustrates the MRF without Boolean scoring applied and 
FIGURE
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii) provides a clearer illustration of top MRF score clusters. 
For visualisation, a noise reduction method was applied; a sliding window was used to find the average MRF per 1% interval of each gene. 
The resulting scores displayed in 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 1</a>
(iii)**
contain a cut-off threshold to highlight the top scoring residues (using
the 75\(^{th}\) percentile). 
Variant sites most likely to present in disease cases are identified by high MRF scoring. 
This model may be expanded by the addition of phenotypic or epigenetic data
(<strong>Supplemental;</strong> ).</p>

<p>![RAG1 (red, left) and RAG2 (blue, right) conservation and mutation rate
residue frequency. 
(i) Gene conservation score; non-conserved 0 and
conserved 1. 
Colour indicates no known mutations in humans. 
(ii)
Histogram; raw MRF score. 
Heatmap; MRF prediction for conserved residues, graded 0 to 0.05 (scale of increasing mutation likelihood with human disease). 
(iii) Coloured bars indicate most likely clinically relevant variant clusters. 
MRF score averaged with 1% intervals for each gene and cut-off below 75th percentile, graded 0 to 0.03 (noise
reduction method). 
(iv) Gene structure with functional domains. 
Full list of residues and scores available in 
TABLE Table:1
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure_main</a></p>

<p>Table [Table:1
 provides all MRF scores for both proteins. 
Raw data used for calculations and the list of validated residues of RAG1 and RAG2 are available in 
TABLE **Tables E3 - E8
. 
shows the MRF mutation likelihood score for mutations that have also been reported as tested for recombination activity in functional assays.
The likelihood of mutation does not correlate with pathogenicity;</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
and
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
show that most mutations tested had severe loss of protein function, while the likelihood each mutation occurring in humans varied significantly. 
Analysis-ready files are also available in Supplemental data along with the associated R source file to allow for alternative visualisations as shown in **Figure
FIGURE RAG_MRF_map
.</p>

<p>TABLE ref</p>

<h2 id="mrf-scores-select-for-confirmed-variants-in-human-disease">MRF scores select for confirmed variants in human disease</h2>

<p>We have applied MRF scores to known damaging mutations from other extensive reports in cases of human disease
[schuetz2008immunodeficiency lee2014systematic villa2001v abolhassani2014hypomorphic kutukculer2012novel sobacchi2006rag villa1998partial noordzij2002immunophenotypic crestani2014rag1 dalal2005evolution kuijpers2011idiopathic gruber2009clinical de2010hypomorphic buchbinder2015identification felgentreff2011clinical schwarz1996rag reiff2013exome corneo2001identical asai2011analysis kato2015rag1 yu2014human de2005novel zhang2005novel henderson2013expanding avila2010highly riccetto2014compound walter2015broad gomez2000mutations chou2012novel]
[originally compiled by [notarangelo2016human ]]. 
This dataset compares a total of 44 variants. 
We expected that functionally damaging variants (resulting in low recombination activity in vitro) that have the highest probability of occurrence would be identified with high MRF scores. 
MRF prediction correctly identified clinically relevant mutations in <em>RAG1</em> and <em>RAG2</em>
(Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i)).
Variants reported on GnomAD which are clinically found to cause disease had significantly higher MRF scores than variants which have not been
reported to cause disease. 
We observed that rare and likely mutations provided high scores while rare but unlikely or common variants had low scores
(Fig. 
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i)).</p>

<p>![RAG1 and RAG2 MRF score predict the likelihood of mutations that are clinically relevant. 
(i) Known damaging variants (clinically diagnosed with genetic confirmation) reported on GnomAD have significantly higher
MRF scores than unreported variants. 
(ii) GnomAD rare variant allele frequency &lt;0.0001. 
No significant difference in allele frequency is found between known damaging and non-clinically reported variants.
Unpaired t-test. 
RAG1 P-value 0.002** RAG2 P-value 0.0339*. 
MRF; mutation rate residue frequency, ns; non-significant.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 2</a></p>

<p>Allele frequency is generally the single most important filtering method for rare disease in whole genome (and exome) sequencing experiments.
Variants under pressure from purifying selection are more likely to cause disease than common variants. 
However, most RAG mutations are rare. 
Therefore, allele frequencies of rare variants reported on GnomAD cannot differentially predict the likelihood of causing disease 
(<strong>Fig.
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(ii)</strong>). 
As such, we found no significant difference between known damaging variants and those that have not yet
been reported as disease-causing. 
The comparison between 
<strong>Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Human_cases</a>
(i) and (ii)</strong> 
illustrates the reasoning for the
design of our method.</p>

<p>Many non-clinically-reported rare variants may cause disease; the MRF score identifies the top clinically relevant candidates. 
Based on the frequency of protein-truncating variants in the general population, 
<em>RAG1</em> and <em>RAG2</em> are considered to be tolerant to the loss of one allele, as indicated by their low probability of being loss-of-function
intolerant (pLI) scores of 0.00 and 0.01, respectively 
[lek2016analysis]. 
This is particularly important for recessive diseases such as RAG deficiency where most new missense variants will be of unknown significance until functionally validated.</p>

<h2 id="top-candidate-variants-require-validation">Top candidate variants require validation</h2>

<p>Functionally characterising protein activity is both costly and time consuming. 
RAG1 and RAG2 have now been investigated by multiple functional assays for at least 110 coding variants
[lee2014systematic tirosh2018recombination lawless2018prevalence].
In each case, researchers selected variants in <em>RAG1</em> and <em>RAG2</em> that were potentially damaging or were identified from PID patients as the
most probable genetic determinant of disease. 
Functional assays for RAG deficiency in those cases, and generally, measured a loss of recombination activity as a percentage of wild type function (0-100%).</p>

<p>Pre-emptively performing functional variant studies benefits those who will be identified with the same variants in the future, before the
onset of disease complications. 
While more than 100 variants have been assayed in vitro, we calculated that only one-quarter of them are most
probable candidates for clinical presentation. 
<em>*Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
 illustrates that while functional work targeted “hand picked” variants that were ultimately
confirmed as damaging, many of them may be unlikely to arise based on population genetics data. 
**Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
 presents, in increasing order, the number of potential variants based on likelihood of presentation and
stacked by the number of variants per score category. 
Variants that have been measured for their loss of protein activity are coloured by
severity. 
Potential variants that remain untested are coloured in grey.
Only 21 of the top 66 most probable clinically relevant variants have been assayed in *RAG1</em>.</p>

<p>![RAG1 and RAG2 MRF score categories and variants assayed to date.
Protein residues are ranked and stacked into categories based on their MRF score. 
High scores (0.043 and 0.038 in RAG1 and RAG2, respectively) represent a greater mutation likelihood. 
Functional assays have measured recombination activity (as its inverse; % loss of activity) in a total
of 110 mutants. 
The severity of protein loss of function is represented by a red gradient. 
Residues that have not been functionally tested are shown in grey. 
While many protein residues are critical to protein function, their mutation is less probable than many of the top MRF
candidates. 
Data further expanded in Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>. 
MRF; mutation rate residue
frequency.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a></p>

<p>Supplemental Figure **
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
 further
illustrates the individual variants which have been tested functionally (the coloured <em>recombination activity</em> subset of Fig</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>]). 
We compared predicted MRF scores to assay measurements for 71 <em>RAG1</em> and 39 <em>RAG2</em> mutants. 
Most mutations tested showed severe loss of protein function (bottom panel of
Supplemental Figure **
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a>
), while the likelihood each mutation occurring in humans varied significantly (top panels).</p>

<p>If MRF scoring was used in the same cases pre-emptively, the loss of investment would be minimal; only 8 variants out of 71 mutants tested
had an above-average MRF score while being measured as functionally benign (a rate of 11.27%). 
RAG2 had only 3 out of 39 variants (7.69%) with an above-average MRF score while functionally benign. 
For the expended resources, approximately 30% more top candidates would have been tested in place of unlikely and functionally non-damaging
mutations. 
However, the true measurement of accuracy is limited in that very few of the most likely clinically relevant variants predicted by
MRF scoring have been tested to date.</p>

<h2 id="false-positives-in-transib-domains-do-not-negatively-impact-prediction">False positives in <em>Transib</em> domains do not negatively impact prediction</h2>

<p>Adaptive immunity is considered to have evolved through jawed vertebrates after integration of the RAG transposon into an ancestral
antigen receptor gene  [agrawal1998transposition hiom1998dna]. 
The <em>Transib</em> transposon is a 600 amino acid core region of RAG1 that targets RSS-like sequences in many invertebrates. 
A linked <em>RAG1/RAG2</em> was shown in the lower dueterosome (sea urchin), indicating an earlier common ancestor than the invertebrate  [fugmann2006ancient], and more
recently, a recombinatorially active RAG transposon (ProtoRAG) was found in the lower chordate amphioxus (or lancelet); the most basal extant
chordate and a “living fossil of RAG”  [huang2016discovery].</p>

<p>![False positives in <em>Transib</em> domains do not worsen probability prediction. 
The <em>Transib</em> domains contain critical conserved protein residues. 
(i) False positives were simulated by scoring <em>Transib</em> domain MRF without omitting Boolean conservation weight \(C=0\). 
(ii) Allele frequencies on GnomAD had conservation levels inversely proportional to
simulated false-positive MRF scoring. 
(iii) When testing for all Boolean component \(C&gt;0\) after MRF calculation the effect of false positives remained non-significant, illustrating the non-negative impact of MRF for prediciting the mutation. 
Unpaired t-test, * P = 0.0195, *** P
&lt; 0.0001. 
MRF; mutation rate residue frequency, ns; non-significant.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 4</a></p>

<p>A set of conserved motifs in core <em>RAG1</em> are shared with the <em>Transib</em> transposase, including the critical DDE residue catalytic triad
(residues 603, 711, and 965)  [kapitonov2005rag1]. 
Ten <em>RAG1</em> core motifs are conserved amongst a set of diverse species including human
[kapitonov2005rag1]. 
This evolutionarily conserved region is considered as most important to protein function. 
Therefore, we chose this region to determine if MRF scoring would have a negative impact if mutations were falsely predicted as clinically important. 
To assess the influence of a false positive effect on prediction, the MRF scores for conserved residues in this group were compared to GnomAD allele frequencies.
Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">False_positives_in_Transip_does_not_worsen_probability</a>
(i) 
plots the MRF (without omitting the Boolean component \(C=0\)) for conserved <em>Transib</em> motif residues, non-conserved <em>Transib</em> motif
residues, and non-<em>Transib</em> residues. 
<strong>Figure
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">False_positives_in_Transip_does_not_worsen_probability</a>
(ii)</strong> 
shows the percentage of these which were reported as mutated on GnomAD. 
By accounting for unreported variants by applying \(C&gt;0\), the resulting effect on incorrectly scoring MRF in the conserved <em>Transib</em>
motifs remained neutral.</p>

<h2 id="mrf-predicts-rag-deficiency-amongst-pid-patients-harbouring-rare-variants">MRF predicts RAG deficiency amongst PID patients harbouring rare variants</h2>

<p>We have previously measured the recombination activity of RAG1 and RAG2
disease-causing variants in several patients  [lawless2018prevalence].
We have compiled our own and other functional assay data from
@lee2014systematic and @tirosh2018recombination to produce a panel of
recombination activity measurements for coding variants in both <em>RAG1</em>
and <em>RAG2</em>.</p>

<p>RAG deficiency was measured as the level of recombination potential produced by the protein complex. 
Each method of investigation simulated the efficiency of wild-type or mutant proteins expressed by patients for their ability to produce a diverse repertoire of T-cell receptor (TCR) and B-cell receptor (BCR) and coding for immunoglobulins.
In functional experiments, mutant proteins were assayed for their ability to perform recombination on a substrate which mimics the RSS of TCR and BCR in comparison to wild-type protein complex (as % SEM).</p>

<p>By gathering confirmed RAG deficiency cases, we compiled the MRF scores for 43 damaging <em>RAG1</em> variants in 77 PID cases and 14 damaging <em>RAG2</em> variants in 21 PID cases (MRF scores spanning over 22 categories). 
To test our method against a strong control group, we identified coding variants in patients with PID where RAG deficiency due to coding variants has been ruled out as the cause of disease. 
We obtained <em>RAG1/2</em> variants in 558 PID patients who had their genomes sequenced as part of the NIHR BioResource - Rare Diseases study
[lawless2018prevalence]. 
Filtering initially identified 32 variants in 166 people. 
This set was trimmed to contain only rare variants; 29 variants over 26 MRF scoring categories from 72 cases of
non-RAG-deficient PID. 
The scatterplot in Figure 5 shows that most PID cases had damaging variants with a high MRF score, while PID cases carried benign variants in RAG1/2 with lower MRF scores; i.e. 
an MRF &gt;0.04 was seen for 31 cases of a damaging variant and only 2 cases of a non-damaging variant. 
Linear regression on this control group produced negative or near-zero slopes for <em>RAG1</em> and <em>RAG2</em>, respectively. 
The same analysis for known-damaging mutations in disease cases had a
significant prediction accuracy for <em>RAG1</em>. 
Analysis for <em>RAG2</em> was not significant. 
However, the sample size to date may be too small to significantly measure <em>RAG2</em> MRF scoring although a positive correlation
was inferred in  FIGURE
FIGURE <a href="RAG\_mrf\_linear\_regression">RAG_mrf_linear_regression</a></p>

<p>[altman1995statistics]. 
R source and raw data can be found in supplemental material.</p>

<p>![A linear regression model of RAG1/2 MRF scoring in cases of primary
immune deficiency. 
MRF prediction correlates with clinical presentation.
Damaging variants identified in confirmed RAG deficiency cases.
Non-damaging variants sourced from cases of PID with rare variants but not responsible for disease. 
An MRF &gt;0.04 was seen for 31 cases of a damaging RAG1 variants. 
(Slopes of RAG1: Damaging: 0.0008* (\(\pm\)
0.0004) P&lt;0.05, intercept 5.82e-05 ***, Non-damaging: -0.0007
(\(\pm\) 0.001). 
Slopes of RAG2; Damaging: 0.0023 (\(\pm\) 0.0018), intercept 0.0312 *, Non-damaging 0.0001 (\(\pm\) 0.0008). 
Source data and script in supplemental material).
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 5</a></p>

<h2 id="mrf-supplements-pathogenicity-prediction-tools-for-translational-research">MRF supplements pathogenicity prediction tools for translational research</h2>

<p>CADD scoring  [kircher2014general] is an important bioinformatics tool that exemplifies pathogenicity prediction. 
While CADD is a valuable scoring method, its purpose is not to predict likelihood of variation.
Similarly, MRF scoring is not a measure of pathogenicity. 
MRF scoring may be complemented by tools for scoring variant deleteriousness. 
We compare MRF to the PHRED-scaled CADD scores for all possible SNV positions in <em>RAG1</em> (**Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a>
) illustrating that pathogenicity prediction cannot account for mutation probability.
Combining both methods allows researchers to identify highly probable mutations before querying predicted pathogenicity.</p>

<p>![<em>RAG1</em> PHRED-scaled CADD score versus GnomAD conservation rate and MRF score. 
Allele frequency conservation rate (<strong>top</strong>) is vastly important for identifying critical structural and functional protein regions. 
The impact of mutation in one of these conserved regions is often estimated using CADD scoring (<strong>middle</strong>). 
CADD score heatmap is aligned by codon and separated into three layers for individual nucleotide positions. 
The MRF score (<strong>bottom</strong>)(visualised using the 75\(^{th}\) percentile with 1% averaging) highlights protein regions which are most likely to present
clinically and may require pre-emptive functional investigation.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 6</a></p>

<p>To further develop this concept, we first annotated variants with MRF likelihood scores and pathogenic prediction PHRED-scaled CADD scores
( FIGURE FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
), and secondly, performed a manual investigation of the clinical relevance of top candidates
(<strong>Table hgmd_data
). 
We used HGMD as an unbiased source of known RAG deficiency cases in both instances. 
CADD score was very successful at predicting the pathogenicity of a variant, (a high-density cluster of variants with CADD scores &gt;25) as shown in **red</strong> in</p>
<h2 id="figure">Figure</h2>
<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(i)<strong>. 
At about the same rate, CADD score also predicted variants as pathogenic that are, to date,
unreported (as **pink</strong> in **Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>]
(i)<strong>). 
Indeed, those unreported variants may very well be pathogenic.
However, the likelihood of each mutation varies. 
As such, we developed the MRF score to account for that likelihood. 
As expected, the likelihood of mutations occurring that were unreported was low according
to MRF (</strong>Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(ii)<strong>, **pink</strong>), while the mutations which did occur were highly enriched in at high MRF scores
(**Fig.</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(ii)<strong>, **red</strong> high-density cluster &gt;0.043). 
Combining mutation prediction (MRF) with pathogenicity prediction (tools like CADD) increases the accuracy of pre-emptively targeting clinically relevant variants. 
**Figure</p>

<p>FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a>
(iii)** shows that while the number of variants presented to date is relatively small, they already account for 36% of the top MRF score candidates.</p>

<p>![<em>RAG1</em> PHRED-scaled CADD score versus MRF score against HGMD data. 
(i)
A high CADD score is a predictor of deleteriousness. 
Both reported (red) and non-reported residues (pink) have a high density of high CADD score.
(ii) MRF scores only show a high-density cluster for high-likelihood variants, reflected by the high MRF score observed for known RAG
deficiency variants. 
The number of pathogenic variants is outweighed by conserved residues; (i-ii) shows density of scores to normalise between
groups. 
AUC overlap difference in CADD score of 21.43% and MRF score of 74.28% (above intersects &gt;22.84 and &gt;0.0409, in <em>(i-ii)</em>
respectively). 
(iii) The number of residues per MRF category shows that disease reported on HGMD accounts for 36% of top MRF candidates. 
AUC;
area under curve, CADD; Combined Annotation Dependent Depletion, HGMD;
Human Gene Mutation Database.<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 7</a></p>

<h1 id="discussion">Discussion</h1>

<p>Determining disease-causing variants for functional analysis typically aims to target conserved gene regions. 
On GnomAD 56% of <em>RAG1</em> (approx. 246,000 alleles) is conserved with no reported variants. 
Functional validation of unknown variants in genes with this level purifying selection is generally infeasible. 
Furthermore, we saw that a vast number of candidates are “predicted pathogenic” by commonly used pathogenicity tools, which may indeed be damaging but unlikely to occur.
To overcome the challenge of manual selection we quantified the likelihood of mutation for each candidate variant.</p>

<p>Targeting clearly defined regions with high MRF scores allows for functional validation studies tailored to the most clinically relevant
protein regions. 
An example of high MRF score clustering occurred in the RAG1 catalytic RNase H (RNH) domain at p.Ser638-Leu658 which is also
considered a conserved <em>Transib</em> motif.</p>

<p>While many hypothetical variants with low MRF scores may be uncovered as functionally damaging, our findings suggest that human genomic studies
will benefit by first targeting variants with the highest probability of occurrence (gene regions with high MRF). 
**Table 
Table:1
 lists the values for calculated MRFs for RAG1 and RAG2.</p>

<p>We have presented a basic application of MRF scoring for RAG deficiency.
The method can be applied genome-wide. 
This can include phenotypically derived weights to target candidate genes or tissue-specific epigenetic features. 
In the state presented here, MRF scores are used for pre-clinical studies. 
A more advanced development may allow for use in single cases. 
During clinical investigations using personalised analysis of patient data, further scoring methods may be applied based on disease features. 
A patient phenotype can contribute a weight based on known genotype correlations separating primary immunodeficiencies or
autoinflammatory diseases  [picard2018international]. 
For example, a patient with autoinflammatory features may require a selection that favors genes associated with proinflammatory disease such as <em>MEFV</em> and
<em>TNFAIP3</em>, whereas a patient with mainly immunodeficiency may have preferential scoring for genes such as <em>BTK</em> and <em>DOCK8</em>. 
In this way, a check-list of most likely candidates can be confirmed or excluded by whole genome or panel sequencing. 
However, validation of these expanded implementations requires a deeper consolidation of functional studies than is currently available.</p>

<p>[Havrilla220814 ]
have recently developed a method with similar possible applications for human health mapping constrained coding regions. 
Their study employed a method that included weighting by sequencing depth.
Similarly, genome-wide scoring may benefit from mutation significance cut-off, which is applied for tools such as CADD, PolyPhen-2, and SIFT
[itan2016mutation]. 
We have not included an adjustment method as our analysis was gene-specific but implementation is advised when calculating genome-wide MRF scores.</p>

<p>The MRF score was developed to identify the top most probable variants that have the potential to cause disease. 
It is not a predictor of pathogenicity. 
However, MRF may contribute to disease prediction; a clinician may ask for the likelihood of RAG deficiency (or any other
Mendelian disease of interest) prior to examination (<em>**</em>).</p>

<p>Predicting the likelihood of discovering novel mutations has implications in genome-wide association studies (GWAS). 
Variants with low minor allele frequencies have a low discovery rate and low probability of disease association  [kido2018minor], an important
consideration for rare diseases such as RAG deficiency. 
An analysis of the NHGRI-EBI catalogue data highlighted diseases whose average risk allele frequency was low  [kido2018minor]. 
Autoimmune diseases had risk allele frequencies considered low at approximately 0.4. 
Without a method to rank most probable novel disease-causing variants, it is unlikely that GWAS will identify very rare disease alleles (with frequencies
&lt;0.001). 
It is conceivable that a number of rare immune diseases are attributable to polygenic rare variants. 
However, evidence for low-frequency polygenic compounding mutations will not be available until large, accessible genetics databases are available, exemplified by
the NIHR BioResource Rare Diseases study  [lawless2018prevalence]. 
An interesting consideration when predicting probabilities of variant frequency, is that of protective mutations. 
Disease risk variants are quelled at low frequency by negative selection, while protective variants may drift at higher allele frequencies  [chan2014excess].</p>

<p>The cost-effectiveness of genomic diagnostic tests is already outperforming traditional, targeted sequencing  [payne2018cost]. 
Even with substantial increases in data sharing capabilities and adoption of clinical genomics, rare diseases due to variants of unknown significance
and low allele frequencies will remain non-actionable until reliable predictive genomics practices are developed. 
Bioinformatics as a whole has made staggering advances in the field of genetics 
[libbrecht2015machine]. 
Challenges that remain unsolved, hindering the benefit of national or global genomics databases, include DNA data storage and random access retrieval  [Organick114553], data privacy management 
[Huang:224980], 
and predictive genomics analysis methods.
Variant filtration in rare disease is based on reference allele frequency, yet the result is not clinically actionable in many cases.
Development of predictive genomics tools may provide a critical role for single patient studies and timely diagnosis 
[casanova2014guidelines].</p>

<h1 id="conclusion">Conclusion</h1>
<p>We provide a list of amino acid residues for RAG1 and RAG2 that have not been reported to date, but are most likely to present clinically as RAG
deficiency.<br />
This method may be applied to other diseases with hopes of improving preparedness for clinical diagnosis.</p>

<p>Supplemental
sec:Supplemental_text</p>

<p>![Data analysis summary map. 
Raw data and analysis scripts are provided in the supplemental. 
Analysis steps and data sources for each procedure described in <em>methods</em>. 
MRF; mutation rate residue frequency, PID;
primary immunodeficiency.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 2</a></p>

<p>![An alternative visualisation of MRF scores for RAG1 and RAG2 proteins.
The data from Table 
Table:1
in column “Average over 1%” is displayed on both the y-axis and colour scale. 
An analysis-friendly long form CSV of the Table 
Table:1
data is also provided in the compressed supplemental R data
“mrf.csv”.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 3</a></p>

<p>![MRF likelihood score versus known functional activity. 
We compiled all variants that we know to have been assayed for protein function to date.
The inverse of functional assay measurements were used, where 0% activity represents 100% loss of activity. 
MRF scores are presented as a percentage of the maximum score per gene (i.e., for RAG1 \(MRF_{max} = 0.043\) (100%) and \(MRF_{min} = 0.0048\) (0%)). 
Top panels show how likely each mutation is predicted to occur in humans.
Bottom panels show the loss of protein activity as a percentage compared to wild-type (% SEM); most mutations tested produced severe loss of protein function, regardless of their mutation likelihood.
Subset of <em>Recombination activity</em> data from Figure</p>

<p>FIGURE [Figure 3](https://link.springer.com/article/10.1007/s10875-019-00670-z}.
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 4</a></p>

<h2 id="main-supplemental-data-table">Main supplemental data table</h2>

<h2 id="clinical-relevance-of-top-candidates">Clinical relevance of top candidates</h2>

<p>The top scoring candidates in RAG1 were assessed for potential clinical relevance (**Table hgmd_data
). 
HGMD was chosen as a reliable, curated source of identifying pathogenic variants. 
45% of RAG1 variants reported on HGMD (23 of 51) were predicted by our model as the most likely candidates seen clinically (the top scoring MRF group of had
66 residues total). 
The remaining variants in the top MRF group, which were not reported by HGMD (43 of 66), were assessed manually for their likelihood as potentially disease causing. 
21 (49%) were highly conserved, not reported on GnomAD, and would be considered probable RAG deficiency on presentation as homozygous or compound heterozygous with a
second damaging variant. 
The remainder had allele frequencies &lt;0.0006, were only found as low frequency heterozygous in the general population, and justify functional validation. 
We expect that none of the top candidate mutations are benign.</p>

<p><strong>Number variants</strong>
Top MRF score candidates total 66%
(i) Of which are reported on HGMD 23%
(ii) Not reported on HGMD to date 43%</p>

<p><strong>Number variants</strong> <strong>Unreported top candidate (%)</strong>%
<strong>GnomAD allele frequency</strong>%
Not found 21 of 43 49% 0%
Very rare 15 of 43 35% 0.00002*
Very rare 7 of 43 16% 0.00006**</p>

<h2 id="supplemental-analysis-tables">Supplemental analysis tables</h2>

<h2 id="protein-structure-application">Protein structure application</h2>

<p>With the availability of a structured protein complex, modelling can be carried out prior to functional assays. 
Residues with the highest MRF for both RAG1 and RAG2 were mapped in  FIGURE
FIGURE Structure
.</p>

<p>![The RAG1 (blue) and RAG2 (grey) protein structure with top candidate MRF scores. 
(i) Protein dimers and (ii=iv) monomers illustrating the three highest category MRF scores for predicted clinically relevant
variants. 
Increasing in score the top three MRF categories (illustrated in  FIGURE
FIGURE <a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">Figure 3</a>
) for each protein are highlighted; yellow, orange, red. 
DNA (green) is bound by the RAG protein complex at recombination signal sequences (PDB:3jbw).
<a href="https://link.springer.com/article/10.1007/s10875-019-00670-z">ESM 5</a></p>

<h2 id="median-cadd-score-per-residue">Median CADD score per residue</h2>

<p>The sourced PHRED-scaled CADD score data consisted of nucleotide level values. 
We were interested in CADD scores averaged per codon.<br />
For every nucleotide position there were three alternative variants to consider,
e.g.</p>

<p>Chrom Pos Ref Alt1 Alt2
Alt3 PHRED1 PHRED2 PHRED3
11 36594855 A C G T 22.3
18.81 22.4</p>

<p>The PHRED-scaled scores are listed here; raw CADD scores are also
included in the original database. 
To produce a working input we used
the median score per codon, that is three scores per nucleotide and
three nucleotides per codon. 
This produced median PHRED-scaled score per
codon / residue, e.g.:</p>

<p>Chrom   Pos PHRED1  PHRED2 PHRED3 <br />
11 36594855 22.3 18.81 22.4
11 36594856 25.3 23.6 24.6
11 36594857 24.8 24.3 24.5</p>

<p>Median PHRED = 24.3\</p>
<h2 id="supplemental-file">Supplemental file</h2>
<p>** <em>‘RAG1.cadd.amino.csv’</em> within the analysis data <em>‘Raw_data_R_analysis_for_figures’</em> contains the median values over
a three-nucleotide window, starting at nucleotide 1 to produce input data with the correct reading frame. 
The “PHRED-scaled” values are used as a normalised and externally comparable unit of analysis, rather than
raw CADD scores. 
The area under the curve was calculated for density plots to quantify the difference between pathogenic and unreported variants with high scores, above the intersects &gt;0.0409 and &gt;22.84 for MRF and CADD, respectively, using score value (\(x\)) versus density (\(y\)) (Fig. 
 (i-ii)) with
\(\int_a^b \! f(x) \, \mathrm{d}x \approx\ \left(b-a\right) \left[\frac{f\left(a\right)\ + f\left(b\right)}{2}\right].\)</p>

<h2 id="genome-wide-and-disease-specific-application">Genome-wide and disease-specific application</h2>

<p>Weighting data can also be applied to the MRF score model to amplify the selectivity. 
The mutation rate can be applied genome wide with a process common in the study of information retrieval; term frequency, inverse document frequency (\(tf-idf\)). 
In this case the “term” and “document” are replaced by amino acid residue \(r\) and gene \(g\) , respectively such that,</p>

<p>[{rf-igf}<em>{r,g} ={rf}</em>{r,g} \times {igf}_r]</p>

<p>We may view each gene as a vector with one component corresponding to each residue mutation in the gene, together with a weight for each component that is given by (1).
Therefore, we can find the overlap score measure with the \({rf-igf}\) weight of each term in \(g\), for a query \(q\);</p>

<p>[\mbox{Score}(q,g)=\sum_{r\in q} \mbox{rf-igf}_{r,g}.]</p>

<p>In respect to MRF scoring, this information retrieval method might be applied as follows; the \({rf-igf}\) weight of a term is the product of
its \(rf\) weight and its \(igf\) weight (\({W}_{r,g}={rf}_{r,g} \times \log \frac{N}{{gf}_{r}}\)) or
(
\({W}_{r,g}=(1 + \log {rf}_{r,g}) \times \log \frac{N}{{gf}_{r}}\)
).
That is, firstly, the number of times a residue mutates in a gene
(
\(rf={rf}_{r,g}\)
) and secondly, the rarity of the mutation genome-wide in \(N\) number of genes (\(igf=N/{gf}_{r}\)). 
Finally, ranking the score of genes for a mutation query \(q\) by;
\(\mbox{Score}(q,g)=\sum_{r\in q\cap g} \mbox{rf-igf}_{r,g}\) The score of the query (Score(\(q,g\))) equals the mutations (terms) that appear in
both the query and the gene (\(r\in q\cap g\)). 
Working out the \(rf-igf\) weight for each of those variants (\({rf.igf}_{r,g}\)) and then summing them (\(\sum\)) to give the score for the specific gene with respect to the query.</p>

<h2 id="bayesian-probability">Bayesian probability</h2>

<p>MRF score may provide a limiting component required for applying Bayesian probability to disease prediction. 
A clinician may ask for the likelihood of RAG deficiency (or any Mendelian disease of interest) for a patient given a set of gene variants \(P(H|E)\) using Bayes’ theorem,
\(P(H|E) = \frac{P(E|H) P(H)}{P(E)}\) where \(P(H)\) is the probability of a patient having RAG deficiency, \(P(E | H)\) is the probability of RAG
deficiency due to a set of variants that have been pre-emptively assayed, and \(P(E)\) is the probability of having a set of gene variants.</p>

<p>\(P(H)\) is known since the rate of RAG deficiency is estimated at an incidence of 1:181,000  [kumanovics2017estimated], SCID at a rate of 1:330,000  [kwan2014newborn], and we also recently show the rate of RAG deficiency in adults with PID  [lawless2018prevalence]. 
Being a recessive disease, \(P(E)\) must account for biallelic variants and is the most difficult value to determine. 
This may be found from population genetics data for (i) the rate of two separate, compound heterozygous variants, (ii) the rate of a homozygous variant or potential consanguinity, or (iii) the rate of de novo variation
[lek2016analysis]. 
\(P(E|H)\) would be identified where all variants are functionally validated. 
This requires a major investment, however the MRF score provides a good approximation.</p>

</p>

  <h2> - </h2>
  <p><h1 id="genomic-analysis-for-rare-disease">Genomic analysis for rare disease</h1>

<ul id="markdown-toc">
  <li><a href="#genomic-analysis-for-rare-disease" id="markdown-toc-genomic-analysis-for-rare-disease">Genomic analysis for rare disease</a></li>
  <li><a href="#abbreviations" id="markdown-toc-abbreviations">Abbreviations</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#exome-sequencing" id="markdown-toc-exome-sequencing">Exome sequencing</a>    <ul>
      <li><a href="#sample-preparation" id="markdown-toc-sample-preparation">Sample preparation</a></li>
      <li><a href="#capture-library" id="markdown-toc-capture-library">Capture library</a></li>
      <li><a href="#sequencing" id="markdown-toc-sequencing">Sequencing</a></li>
      <li><a href="#ultra-deep-sequencing" id="markdown-toc-ultra-deep-sequencing">Ultra-deep sequencing</a></li>
    </ul>
  </li>
  <li><a href="#genomic-analysis" id="markdown-toc-genomic-analysis">Genomic analysis</a></li>
  <li><a href="#sec:routine_analysis" id="markdown-toc-sec:routine_analysis">Routine analysis</a>    <ul>
      <li><a href="#analysis-workflow-structure" id="markdown-toc-analysis-workflow-structure">Analysis workflow structure.</a></li>
      <li><a href="#analysis-storage-structure" id="markdown-toc-analysis-storage-structure">Analysis storage structure</a></li>
      <li><a href="#sequence-alignment-to-reference-genome" id="markdown-toc-sequence-alignment-to-reference-genome">Sequence alignment to reference genome</a></li>
      <li><a href="#read-adaptor-trimming" id="markdown-toc-read-adaptor-trimming">Read adaptor trimming</a></li>
      <li><a href="#read-sorting" id="markdown-toc-read-sorting">Read sorting</a></li>
      <li><a href="#gatk-best-practices" id="markdown-toc-gatk-best-practices">GATK best practices.</a></li>
      <li><a href="#read-deduplication" id="markdown-toc-read-deduplication">Read deduplication</a></li>
      <li><a href="#subsec:text_realtar" id="markdown-toc-subsec:text_realtar">Read realignment and targets</a></li>
      <li><a href="#subsec:text_bsqr" id="markdown-toc-subsec:text_bsqr">Base quality score recalibration</a></li>
      <li><a href="#subsec:text_hc" id="markdown-toc-subsec:text_hc">Haplotype calling</a></li>
      <li><a href="#subsec:text_joint" id="markdown-toc-subsec:text_joint">Cohort joint genotyping</a></li>
    </ul>
  </li>
  <li><a href="#tailored-analysis" id="markdown-toc-tailored-analysis">Tailored analysis</a></li>
  <li><a href="#integrating-databases" id="markdown-toc-integrating-databases">Integrating databases</a>    <ul>
      <li><a href="#subsec:gnomad" id="markdown-toc-subsec:gnomad">Population genetics</a></li>
      <li><a href="#phenotype-genotype-and-function" id="markdown-toc-phenotype-genotype-and-function">Phenotype, genotype, and function</a></li>
    </ul>
  </li>
  <li><a href="#sec:cohort_network_analysis" id="markdown-toc-sec:cohort_network_analysis">Rare disease cohort network analysis</a>    <ul>
      <li><a href="#introduction-1" id="markdown-toc-introduction-1">Introduction</a></li>
      <li><a href="#exome-analysis" id="markdown-toc-exome-analysis">Exome analysis</a></li>
      <li><a href="#cluster-list-preparation" id="markdown-toc-cluster-list-preparation">Cluster list preparation</a></li>
      <li><a href="#sec:net_construction" id="markdown-toc-sec:net_construction">Network construction</a></li>
      <li><a href="#sec:random_sample" id="markdown-toc-sec:random_sample">Random sampling</a></li>
      <li><a href="#expanding-damaged-gene-mcl-clusters" id="markdown-toc-expanding-damaged-gene-mcl-clusters">Expanding damaged gene MCL clusters</a></li>
      <li><a href="#sec:burden_rank" id="markdown-toc-sec:burden_rank">Burden rank</a></li>
      <li><a href="#sec:number_test" id="markdown-toc-sec:number_test">Determining the number of tests <em>m</em></a></li>
      <li><a href="#significance-testing" id="markdown-toc-significance-testing">Significance testing</a></li>
      <li><a href="#sec:enrichment_test" id="markdown-toc-sec:enrichment_test">Enrichment testing</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#command-line-example-code" id="markdown-toc-command-line-example-code">Command line example code</a></li>
</ul>

<h1 id="abbreviations">Abbreviations</h1>
<p>BWA (Burrows-Wheeler transformation aligner), FDR (False discovery
rate), GO (Gene Ontology), GrCh38 (Genome Reference Consortium Human
Build 38), GVCF (Genomic Variant Call Format), KEGG (Kyoto Encyclopedia
of Genes and Genomes), LoF (loss-of-function), NCBI (National Center for
Biotechnology), Pfam (Protein families database), PPI (Protein-protein
interaction), VCF (variant call format).</p>

<h1 id="introduction">Introduction</h1>

<p>This chapter contains theory and examples for the investigation of rare
disease by exome sequencing used throughout this thesis. Each section is
generally self-contained with a brief introduction. A specific section
is devoted to a novel method of rare disease cohort network analysis in
Sec [sec:cohort_network_analysis]. A separate introduction is also
included to begin that section in context. This procedure was developed
to provide a statistical method for the detection of damaged protein
pathways that drive disease. The method is based on measuring variant
enrichment and clustering by protein-protein interactions (PPI).</p>

<p>A detailed overall analysis plan is illustrated in . A accompanying data
storage plan is also provided in the same section that directly maps to
the analysis plan. A rough overview “infographic” of a next generation
sequencing study is shown <strong>Figure [fig:NGS]</strong>. The general
requirements, personnel responsibilities, and cost-breakdown is shown.</p>

<p><strong>Whole exome sequencing experiment design.</strong> The general
requirements, personnel responsibilities, and cost-breakdown is shown
for a small NGS study of approximately ten participants. If library
preparation and sequencing is performed at a dedicated facility then
scaling up to very large cohorts (&gt;1,000) potential only differs in
one critical feature; implementing the bioinformatic methods used in
this chapter also requires a critical expertise in high-performance
computing. No methods have been included to demonstrate job scheduling
and parallelisation across large computer
clusters.
<strong>FigureLabel</strong>
NGS</p>

<p><img src="/images/bioinfo/NGS.png" width="80%" /></p>

<p><em>image_caption</em></p>

<h1 id="exome-sequencing">Exome sequencing</h1>
<h2 id="sample-preparation">Sample preparation</h2>

<p>For genomic investigations, a patient generally donates a small blood
sample (2-6mL) along with signed consent to use their biological
material and data in genetic and functional research. Patient DNA is
purified from peripheral blood monocytes. In most cases, the
purification is done using a commercial kit such as that from Qiagen
(51104 QIAamp DNA Blood Mini Kit). This protocol takes about 1 hour to
purify 1-10 patient samples. Sometimes patient DNA is provided from an
external source such as a local hospital where blood samples are
processed routinely by dedicated staff. In this case, the purification
method may be unknown so extra care should be taken when checking
suitability for sequencing experiments. Consideration should be given to
the possibility of sample mix up, that contamination could have
occurred, etc.</p>

<p>High-throughput sequencing experiments benefit from consistency during
sequencing library preparation. While there are several commercial
options available, the protocol used in this study was the SureSelect XT
target enrichment system for Illumina paired-end multiplexed sequencing
library. A detailed protocol is available from the manufacturer.
However, the process can be summarised in four main steps. After DNA
quality has been checked, the basic protocol consists of:<br />
(1) DNA fragmentation into 100-300 base pair strands, either (i) by
using an enzyme that digests the DNA or (ii) by breaking by sonication;
the DNA is suspended inside a small glass tube containing a glass rod
which is vibrated by sonic waves inside a water bath.<br />
(2) Another round of quality control checks to ensure that the DNA is
fragmented into the correct size range.<br />
(3) These fragments are bound by probes that specifically recognise the
coding sequences which collectively make up the exome.<br />
(4) The DNA that has been selectively purified is then tagged by adding
a tail of nucleotides in specific sequences that label each of the
individual samples with a unique code. When the sequencing step is
performed later, all of the samples will get mixed together. The unique
tag allows us to later re-identify which sequences belong to every
person included in the study.</p>

<p>While it is important that library preparation is performed accurately,
the individual steps could be replaced by alternative methods. The
crucial component is an end product of targeted DNA fragments that have
been tagged appropriately to allow the sequencing chemistry on the
chosen system and that fragment lengths are in the correct range. A more
detailed summary of the procedure is outlined;</p>

<p><strong>Preparation of sample</strong></p>
<ol>
  <li>DNA is sheared, the most frequently used methods are by enzymatic
digestion and sonication.</li>
  <li>Fragmented DNA is purified using AMPure XP beads.</li>
  <li>Quality assessment.</li>
  <li>End repair.</li>
  <li>Purify using AMPure XP beads.</li>
  <li>Adenylation at 3’ end.</li>
  <li>Purify using AMPure XP beads.</li>
  <li>Paired-end adaptor ligation.</li>
  <li>Purify using AMPure XP beads.</li>
  <li>Amplification.</li>
  <li>Purify using AMPure XP beads.</li>
  <li>Assess quality.</li>
</ol>

<p><strong>Hybridization and capture</strong></p>
<ol>
  <li>Hybridize capture library probes to DNA.</li>
  <li>Capture the hybridized DNA using streptavidin-coated beads.
Note: at this step, custom gene target libraries can be used.</li>
</ol>

<p><strong>Indexing and multiplexing</strong></p>
<ol>
  <li>Captured libraries are amplified with indexing primers.</li>
  <li>Purify using AMPure XP beads.</li>
  <li>Assess quality and concentration of indexed library DNA.</li>
  <li>Pool samples at equal concentrations.</li>
</ol>

<h2 id="capture-library">Capture library</h2>
<p>For targeted sequencing experiments, the most important step in library
preparation is the hybridization of capture library probes. Libraries of
probes that are complementary to exome coding sequences can be ordered
from a number of commercial suppliers. For a whole exome, this consists
of hundreds of thousands of short RNA oligonucleotide strands bound to
biotin. When the capture library hybridization mix is added to the DNA,
most of the short probes bind to their complementary DNA sequences over
12-16 hours. To separate these selected fragments from the remaining
bulk of unwanted DNA, streptavidin-coated magnetic beads are added. The
streptavidin attaches to the biotin and therefore the DNA-bound probe
can be pulled out using a strong magnet. Unbound DNA can then be washed
away. Experiments in this study have been performed using Agilent
capture library SureSelect Human All Exon V4-6, although several other
options are available.</p>

<p>Targeted panels can also be used to focus on smaller sets of genes. For
example, in some immunological conditions a panel of  50 genes might be
targeted rather than a library for all known genes (exome). Cancer
genetics screening services sometimes use a small panel of 40-100 genes.
These small panels cut down on cost and focus only on genes where
interpretation of variants would be possible. For the same price as
whole exome, less capture library is needed and more samples can be
sequenced.</p>

<p>As of 2018, all-exon capture library costs roughly £16,000 for enough
reagent to prepare 96 DNA samples. This accounts for about 50% of the
cost of the total library preparation materials. In total, the library
preparation costs about £200 per sample. Once the samples have been
prepared it cost about another £200 to sequence; approximately £400
total.</p>

<h2 id="sequencing">Sequencing</h2>
<p>The sequencing carried out in this study was performed on Illumina
platforms. These include the MiSeq for very small runs of a select set
of genes, HiSeq 3000, 4000, and HiSeq X for whole exome or whole genome
sequencing. The prepared libraries of patient DNA are pooled to contain
5-12 samples per pool. Since each sample has a unique identifier tag, it
is OK to pool them together and later separate out all the individual
data per person. On the HiSeq 3000 approximately 12 samples can be run
per lane with acceptable coverage. This provides about 30-50X reads per
nucleotide, sufficiently deep to confidently identify true germline
mutations. There are 8 lanes per sequencing flow cell. Therefore, a
single sequencing run can contain anything from 50-100 patient samples.
Depending on the sequencing platform the run can take up to 5 days to
complete.</p>

<h2 id="ultra-deep-sequencing">Ultra-deep sequencing</h2>
<p>Mendelian disorders can be successfully explained using exome and whole
genome sequencing. Both the interpretability and cost per sample are
improved in cases where a gene sequencing panel can be used. Some
conditions, particularly autoinflammatory disorders, can arise from low
frequency somatic variants that are capable of driving disease through
potent gain-of-function mechanisms. It is worth noting that a
“gain-of-function” can also be considered as a succinct description for
systems where a loss of inhibitory activity occurs that directly results
in increased signaling cascade activity that would otherwise rest in an
inactive state; a homeostatic pathway. E.g. loss of an autoinhibitory
feature for a single protein or loss of an inhibitory mechanism that is
responsible for direct repression in the absence of stimulation or
specific agonist. In such cases, a low frequency de novo variant will
escape detection with typical sequencing methods, but ultra-deep
sequencing offers a method for detection. This option uses a high
concentration of capture reagent to prepare a highly enriched library
and sequence at high-density on a flow cell to produce ultra-deep
sequencing reads (e.g. &gt;5,000x versus 50x, as typical for whole exome
sequencing). In this case, PCR-free preparation is ideal for somatic
variant detection, naturally.</p>

<h1 id="genomic-analysis">Genomic analysis</h1>
<p>Like any data science, bioinformatics is a discipline of data
manipulation. The majority of jobs could be accomplished simply with a
method for sequence alignment and data mining using grep, sed, and awk.
However, the development of specialised genomics-based tools allows us
to standardise procedures and expand the avenues of exploration. One of
the greatest single, collaborative, sources of genomics analysis tool is
the Genome Analysis Toolkit developed by The Broad Institute.</p>

<p>While not every tool was used in this study, a synopsis of analysis
options is worthwhile; an overview of GATK provides a good example of
the current trends. The software provided by GATK includes methods for
data manipulation. As of writing, there are 291 packages in this
software suite. 
These are divided into major topics of genomic data
handling that include:</p>

<ul>
  <li>
    <p>Tools dedicated to managing read data in SAM, BAM or CRAM formats.</p>
  </li>
  <li>
    <p>Diagnostics and QC to collect sequencing quality and comparative
metrics;</p>
  </li>
  <li>
    <p>Interval manipulation to process genomic intervals in various formats.
For example, converting a BED file to a Picard interval list;</p>
  </li>
  <li>
    <p>Metagenomics. For example, microbial community composition and pathogen
detection using read filtering, microbe reference alignment, and
abundance scoring;</p>
  </li>
  <li>
    <p>Tools that manipulate FASTA format references. For example, creating a
custom capture library relies on oligonucleotide baits for hybrid
selection reactions, or making BWA-MEM index image files, or a sequence
dictionary to accompany a reference.</p>
  </li>
  <li>
    <p>Variant calling and genotyping for variants such as SNVs, SNPs, and
Indels. For example, haplotype calling of germline SNPs and indels by
performing a local re-assembly of haplotypes, such HaplotypeCaller gVCF
files are generally merged into batches of single gVCFs to manage
databases, and joint genotyping is a common approach on these databases.
Some tools also specialise in calling somatic SNVs and indels also by
local assembly of haplotypes.</p>
  </li>
  <li>
    <p>Variant manipulation software for handling variant call format (VCF)
data.</p>
  </li>
  <li>
    <p>Base calling. This is software that is used at the early stage of
sequence data interpretation to process the raw data, i.e. base calls,
and other attributes such as the adapters used.</p>
  </li>
  <li>
    <p>Read filters which can be applied by the engine to select reads for
analysis.</p>
  </li>
  <li>
    <p>Variant annotations is a software that can be used during critical
stages of analysis by other tools, i.e. HaplotypeCaller, Mutect2,
VariantAnnotator and GenotypeGVCFs.</p>
  </li>
  <li>
    <p>Copy number variant discovery using read coverage to detect copy number
variants.</p>
  </li>
  <li>
    <p>Coverage analysis using allele depths as the metric.</p>
  </li>
  <li>
    <p>Structural variant discovery.</p>
  </li>
  <li>
    <p>Variant evaluation and refinement. For example, variant calls can be
further detailed using annotations which are not offered by the base
software.</p>
  </li>
  <li>
    <p>Variant filtering that allows annotation of the FILTER column in a
dataset.</p>
  </li>
</ul>

<h1 id="sec:routine_analysis">Routine analysis</h1>
<p>Routine analysis can be summarised in order as raw sequence data quality
control, read trimming, reference alignment, subsequently followed by
the GATK best practices for SNV and indels. 
Figure “analysis flow”
illustrates the basic analysis workflow
structure. Proceeding top to bottom, the procedure making up the left
side of fig. [fig:analysis_flow] contains the procedures for routine
analysis. Each rectangle box labels a program function, key input and
output data are shown with light slanted boxes. The most important data
retention steps are indicated with a “data store” symbol. The right-hand
side of the same figure illustrates the second phase of analysis used in
this study; tailored analysis, or cohort-specific analysis. The
annotation, filtering, and segregation of data here depends on the
project. A generally useful strategy will output gene candidate data
based on inheritance type to produce individual datasets for each (i)
functional heterozygous variants (including de novo, somatic, known
dominant genes, etc.), (ii) homozygous only variants, and (iii)
potential compound heterozygous variants, and (iv) a master version of
all variants that have completed the filtering pipeline. These datasets
are generally small (&lt;1MB per individual) and combined into all
individuals per sequence run or cohort.</p>

<p>Genome and exome analysis is an iterative process. Although there are
routine steps, different methods will be used depending on each
experiment. Data storage is a major factor in genetic analysis. Not only
are the initial files large in size, many intermediate files are
produced and may themselves be important to retain for a certain period.
Key output files are shown by light slanted boxes. As shown in <strong>Figure
[fig:analysis_flow_storage]</strong>, storage structure is divided between
long-term and short-term storage. A /work/ directory is used for
long-term storage and is backed up routinely. Short-term storage is used
for intermediate files which are held in /scratch/ directories and not
backed up. File sizes are represented by colour, dark orange indicating
large and light yellow indicating small sizes.</p>

<h2 id="analysis-workflow-structure">Analysis workflow structure.</h2>
<p>Tools used are shown in square boxes.
Reference data used secondary to inputs are shown as light boxes with
curved sides. Key output files are shown by light slanted boxes. Storage
structure is divided between long-term and short-term storage. The same
figure key is applied to Fig.
[fig:analysis_flow_storage].</p>

<p><strong>FigureLabel</strong>
analysis_flow</p>

<p><img src="/images/bioinfo/analysis_flow.pdf" width="80%" /></p>

<p><em>image_caption</em></p>

<h2 id="analysis-storage-structure">Analysis storage structure</h2>
<p>Storage structure is divided
between long-term and short-term storage. A /work/ directory is used for
long-term storage and is backed up routinely. Short-term storage is used
for intermediate files which are held in /scratch/ directories and not
backed up. File sizes are represented by colour, dark orange indicating
large and light yellow indicating small sizes. Figure key is shown in
Fig.
[fig:analysis_flow].
<strong>FigureLabel</strong>
analysis_flow_storage
<img src="/images/bioinfo/analysis_flow_storage.pdf" width="40%" /></p>

<h2 id="sequence-alignment-to-reference-genome">Sequence alignment to reference genome</h2>
<p>The analysis methods are normally run as a pipeline workflow. The basic
methods do not have major changes in theory, although there are usually
several methods or software options available for each step. Once a
working pipeline is established, most of a researcher’s time can be
spent on the tailored analysis at the end of the pipeline, which
requires more specialised steps. Each individuals’ exome sequence data
contains approximately 3-8 GB of raw data. This is output as  150bp raw
unmapped sequence fragments that must be aligned to the reference human
genome. The raw sequence data is normally collected into a fasta format
file called a “fastq” file (pronounced “fast” “q”).</p>

<p>An important consideration for sequence analysis is the reference genome
used for comparison. The coordinates for individual nucleotides vary
between reference versions. For example, aligning with one reference
version will produce a file that contains chromosome, position, and
variants specific to that genome reference. Annotation will be required
to interpret results, but if databases based on coordinates from
different reference versions are used during this step the results will
be incorrect.</p>

<p>The current human genome reference is a version of Genome Reference
Consortium Human Build 38 patch release 13 (GrCh38)
(https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39).
Because of the timing when next generation sequencing became popular,
many researchers tend to use genome build GrCh37 in their analysis<br />
(https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/)
However, it is preferable to use the more recent GrCh38. A lot of the
best standardised methods that are used in the field were developed
while genome build GrCh37 was the most recent version. Thousands of
database samples will be in storage which have been aligned with this
reference. Bioinformatic analysis is extremely more powerful when
comparing many samples than when looking at one sample individually.
Therefore, many people still tend to align their data to GrCh37 so that
they can use their reference databases without going back and realigning
all of their old samples again to GrCh38.
The most popular method for aligning short read data to the reference
human genome is “BWA-MEM” (a Burrows-Wheeler transformation aligner)
[@Li2009Fast]. BWA-MEM was used to align sequencing data in this study
to GrCh37 [subsec:text_alignment] (for an example usage see page ).</p>

<h2 id="read-adaptor-trimming">Read adaptor trimming</h2>
<p>Since Illumina-based sequencing technology relies on duplexed samples,
identification sequence tags were added to all sequence libraries.
During analysis these tag sequences can affect alignment and are
therefore removed from each read [subsec:text_cut]. The command line
usage is shown on page .</p>

<h2 id="read-sorting">Read sorting</h2>
<p>To allow downstream analyses to run efficiently, the sequences within
files are rearranged based on their coordinate position after alignment
with the reference genome. This process is carried out using SamTools
[subsec:text_sort] This software is part of the The Broad
Institute-maintained Genome Analysis Toolkit (GATK). Their standardised
pipeline is illustrated here in [fig:gatk]; a protocol familiar to
most bioinformaticians. And example of usage can be seen on page .</p>

<h2 id="gatk-best-practices">GATK best practices.</h2>
<p>Illustration from software.broadinstitute.org. Per-sample variant calling is used to
produce a file in GVCF format. GVCFs are consolidated from multiple
samples into a GenomicsDB datastore. Joint genotyping is carried out,
and finally, variant quality score recalibration filtering is used to
produce the final multi-sample callset with the desired balance of
precision and sensitivity. Further downstream analysis, including
annotation is not shown.
<strong>FigureLabel</strong>
gatk
<img src="/images/bioinfo/gatk.png" width="40%" /></p>

<h2 id="read-deduplication">Read deduplication</h2>
<p>Sequence library preparation may contain a PCR amplification step.
Individual fragments of genomic DNA will be amplified. If a read
contains a variant then, after amplification, we only want to count this
occurrence once so that we do not interpret an inflated allele depth.
Therefore, identical reads are marked as duplicates. Alternative
overlapping reads that also contain the same variant will result in
detection of a true germline variant. When no other overlapping reads
contain the variant then the allele depth will remain low and be
filtered out later by a frequency threshold, or flagged as potentially
somatic. [subsec:text_dedup] For command line usage examples of this
step, see page .</p>

<h2 id="subsec:text_realtar">Read realignment and targets</h2>
<p>After sequence alignment, regions of misalignments will inevitably
exist. To deal with this feature, a local realignment process is used
such that the number of mismatching bases is minimized across all the
reads. This main source of misalignments corrected in this step are due
insertions and deletions. Current versions of the GATK suite no longer
require this step as it is integrated into the downstream process of
haplotype assembly (via HaplotypeCaller or MuTect2). However, the step
is included here since it is a well known legacy feature and is a very
useful concept to understand for new users. As usage example is provided
on page .</p>

<h2 id="subsec:text_bsqr">Base quality score recalibration</h2>
<p>The alignment steps are difficult and computationally intensive. There
are methods to double check the alignment and see if more appropriate
corrections can be made. Once the quality control is all done, we are
left with a Bam file format which is ready for variant analysis. Most of
the bioinformatic community agrees on some best practices using the
tools maintained by the Broad Institute. The GATK is widely used for the
QC and variant analysis of genomic data.</p>

<p>Joint analysis of multiple samples increases the accuracy of our
methods. Not only are the algorithms checking for consistencies in the
data, but sometimes the sequence library preparation induces errors in
the sequences produced. For example, sometimes a particular nucleotide
position can be sequenced incorrectly. In isolation we would expect that
this patient has a true mutation in the gene, but when we compare the
whole cohort we see that it is just a common sequencing artefact.</p>

<p>When we look at the number of variants compared to the reference genome
there can be hundreds of thousands. The vast majority of these can be
ignored by [1] comparing the in-house database of false positive,
[2] comparing the unrelated samples sequenced on the same run to
remove library preparation errors, [3] compare to databases of common
polymorphisms.</p>

<p>In genome wide association studies, researchers are generally looking at
the mild effects of common polymorphisms which occur in the general
population and may associate with a particular phenotype. In rare
disease analysis we are focusing on the very rare variants that have a
strong effect to produce a severe phenotype. Therefore, another step for
pruning out the data is to compare to large cohorts of “healthy”
populations to leave only the very rare variants in our dataset. The
command line arguments can be see on page .</p>

<h2 id="subsec:text_hc">Haplotype calling</h2>
<p>The final output, illustrated in the GATK best practices figure above,
is stored in a Genomic Variant Call Format (GVCF). The GVCF file type
that now presents our data has one row for each nucleotide along the
genome. The row contains the DNA position, the nucleotide (either
wild-type (ref) or mutation (alt)) and lots of quality and metrics
information. We analyse variants against curated databases of known
mutations. We also analyse again separately for indels, since a shift in
the sequence position due to a indel could affect the alignment
accuracy. For an example see page .</p>

<h2 id="subsec:text_joint">Cohort joint genotyping</h2>
<p>We can merge 10-100s of samples together by combining the files to
simplify how we handle the data. Tracking hundreds of files is
exponentially more difficult than tracking 1. The GVCF contains a row of
data for every single nucleotide. We can condense the information by
converting to a VCF which instead only keeps information for every
variant but not every wild type nucleotide (since wild type is healthy
and of no interest to us). The GATK documentation provides a great
explanation of the shared features and differences between gVCF and VCF
files.</p>

<p>As our dataset becomes smaller we can double check to focus on just the
most likely disease-causing mutations. Often times, a research group or
clinical research team will collect genetic material from patients who
they would like to diagnose genetically, or even collect a great
database of patients with a shared phenotype. There are many of
facilities that will sequence the samples commercially. When one orders
exome or whole genome sequencing commercially, most facilities will also
provide data analysis.</p>

<p>The output of their analysis is usually this VCF file (mostly contain
the chromosome, nucleotide position, and a selection of quality control
information). This file is usually the end-point of routine analysis.
However, it does not really put one in a position for a genetic
diagnosis. Very good services will also provide lists of top candidate
genetic determinants along with information on each of the genes and
possible mechanisms of pathogenicity (although the number of companies
doing high-level tailored analysis is small but growing). There are
usually more hurdles in determining candidate variants of unknown
significance. An example of the command line arguments used can be found
on page .</p>

<h1 id="tailored-analysis">Tailored analysis</h1>
<p>Routine analysis typically takes up to a week, although it is usually
performed on a standardised pipeline that can run automatically on a
high-performance computing platform. A large part of custom filtering
begins when the routine analysis steps have been completed; downstream
analysis is adapted for each particular challenge. The discussion in
explains some foundational steps towards a fully automatic system that
relies only on some input features, such as clinical information. While
many software packages exist that claim to output tailored analysis,
these tend to either only tackle a specific niche or require lots of
curated auxiliary input data.</p>

<p>The output of non-routine analysis (outlined in this chapter) sometimes
takes only five minutes to interpret a cause of disease. In other cases,
data that has been sequenced years previously has not yet relented an
explanation for phenotypes that almost certainly should be explained by
coding variants present within the sequence data. For example, for a
dozen patients who share a similar and severe phenotype, it is often
likely that the same gene or related genes could cause their disease.
Unrelated patients with a rare dominant disease are not to all carry the
same disease-causing variant; they may have different variants in shared
gene, or variants among different genes which all contribute to a shared
pathway that would result in the same end-point phenotype.</p>

<p><strong>Shared pathways in candidate genes.</strong> Output of a STRING database
query for known and predicted protein-protein interactions. These
interactions include physical and functional associations. (STRING is an
SIB-run service of the ELIXIR core data open resources for publicly
funded research).</p>

<p><strong>FigureLabel</strong>
string_normal_image
<img src="/images/bioinfo/string_normal_image.png" width="40%" /></p>

<p><em>image_caption</em></p>

<p>For example, in <strong>Figure [fig:string_normal_image]</strong> above we see
that from a group of unrelated people, all of the candidate genes
carrying functional variants are joined by their shared functional
interactions. For an autoinflammatory phenotype, genes like <em>NLRP3</em>,
<em>NOD2</em>, <em>TNFAIP3</em>, <em>MyD88</em>, <em>IKBKB</em>, <em>FASLG</em>, or <em>TMEM173</em> might all
have different functions but damaging mutations in any of these could
result in phenotypes that, on the surface, appear related.</p>

<p>Another circumstance might be seen in a small cohort of patients with a
shared autoinflammatory phenotype. For example, the gene <em>NLRP7</em> has
relatively few publications examining it’s role in autoinflammatory
disease. One would not consider this a strong candidate gene if faced
with a variant of unknown significance in this gene from a single
patient. However, three or four very rare or novel mutations in
unrelated patients should be given consideration as producing an
autoinflammatory disease. Single case, or small cohorts lack the power
to measure significant associations. Therefore in the situation proposed
here, manual interpretation is required (biased as it may be).</p>

<p><em>NLRP7</em> variants not reported as producing disease, like <em>MEFV</em>, or
<em>TNFAIP3</em>. However, we must consider that genes plausibly responsible
for causing disease in a dominant manner and that are highly conserved
are generally under purifying selective pressure. Damaging mutations may
be not be compatible with viability and therefore we never see cases of
disease. Variants which are damaging to protein function but that do not
completely destroy all of the normal structure may produce a phenotype
that is pathogenic but viable with modern medical intervention.</p>

<p>In the example of NLRP7, the protein is known to Inhibit
CASP1/caspase-1-dependent IL-1$\beta$ secretion. The functional domains
of this protein are shared in other pro-inflammatory processes. Pyrin,
NACHT, and LRR, domain variants are all studied for autoinflammatory
diseases. The related gene, <em>NLRP3</em>, is probably the most widely
recognised gene where damaging variants in these functional domains
produce severe immune disorders. In cases where we have protein
structures, we can also model the effect.</p>

<p>In our example, <em>NLRP7</em> variants have been reported as the genetic
determinant of a condition that causes early neonatal death and ectopic
pregnancy. Many of the reported variants are stop mutations that will
either produce a truncated protein or prevent expression of the allele
altogether through nonsense-mediated decay. It is difficult predict the
mechanism of disease in cases like this where the two outcomes have
opposing paths. That is to say, a truncated protein may have an active
functional domain which can no longer be inhibited since the C-terminal
domains are missing, while haploinsufficiency would mean that cells
cannot perform their normal function for the pathway since 50% of the
protein is depleted (in heterozygous cases). Haploinsufficiency can
result in a disease that phenotypically resembles a gain-of-function
when the responsible protein normally acts as an inhibitor for an
inflammatory pathway [@Lawless2018acase]. This is not expected with
<em>NLRP7</em> and therefore heterozygous loss-of-function does not explain
disease.</p>

<p>For a candidate gene like this, we have some plausible evidence but
cannot really progress any further without new functional studies. The
first step involved confirmatory Sanger sequencing for all patients
identified through exome sequencing. Next, any close relatives that are
available might be also sequenced for the same variants. If the
mutations are disease-causing then other carriers would also be expected
to have some shared phenotype features. The possibilities in functional
experiments vary widely and are highly dependent on the candidate genes.
The procedure outlined in this hypothetical example is generally
applicable in for the majority of single-case studies and illustrates
the importance of tailored analysis. The initial findings of genomic
analysis may produce more follow up questions, including whether other
probable gene candidates can be ruled out, for which the patient carries
only the “normal” reference alleles (e.g. <em>CFTR</em> screening for cystic
fibrosis/lung disease).</p>

<h1 id="integrating-databases">Integrating databases</h1>
<h2 id="subsec:gnomad">Population genetics</h2>
<p>GnomAD (version r2.0.2) [@lek2016analysis] was used in these studies as
the best source of population genetics data. The reference genome is
GRCh37. Offline local database mirrors were used in most cases. Input
sets used GnomAD variant allele frequencies and reference sequences
processed as VCF and CSV files. outlines a specific data transformation
using the gnomAD database, but in general, gnomAD was used as a
filtering threshold for determining the expected population frequency of
each variant. A strict threshold for rare variants could be set to
ignore and candidate variants that are more frequent than 0.001.
However, in most cases a more lenient level is used and any remaining
benign or common variants are removed by “technical control” (filter on
cohort to remove common variants between individuals that do not share a
phenotype). A more modest cut-off threshold allows us to sometimes
identify variant that are present in the general population, which are
responsible for a recessive disease with no predictable heterozygous
loss-of-function intolerance.</p>

<p>Other sources of population genetics data comes from resources such as
CliVar and dbSNP, which as they grow in size become an annotated and
curated for of population data. These resources allow us to calculate
the expected frequencies for disease-causing variants. However, since
these are manually curated database and predominantly European based,
they are inherently biased and not reliable for statistical
applications.</p>

<h2 id="phenotype-genotype-and-function">Phenotype, genotype, and function</h2>
<p>Population genetics database gnomAD has been individually addressed in
section [subsec:gnomad], as this is the most important type of
annotation and filtering criteria for genetic determinants of rare
disease. Additionally, in these studies many phenotype and genotype
databases have been used for annotation and interpretation.
Specifically, the most frequently used data came from MGI Phenotype,
MorbidMap, VOC MammalianPhenotype, Gencode symbol, UniProtKB, Enterez
ID, ENSGene ID, GO ID, Description, OMIM, BIOGRID interactions, HGMD
human phenotype, ClinVar, and dbSNP. In most cases, every candidate
variant was annotated with the main information per gene from a local
database containing the information from each of the listed resources.</p>

<p>These are the “basic” information databases that we used to annotate
variants. In a cohort study, data mining can find correlations and was
therefore included for posterity as it does not significantly increase
the data storage. Even if an obvious cause of disease was found we may
later return to the data to find other cofactors or genetic modifiers.
Or for example, in a single case study, a variant of unknown
significance may have no statistical basis to be selected or ignored. We
use this information to decide if that mutation is worth consideration:
Is it in a protein domain of known function? Are there other cases
reported with the same phenotype? What is the gene function, ontology,
etc.?</p>

<p>We have also used some gene lists that are specific to disease,
druggability, etc. A major contributor for collecting these gene lists
has been the Mac Arthur et al. [@macarthurgit]. These gene lists can be
used is special cases. For example, a study looking at (1) dominant
pathogenic mutations, and (2) in known immune genes might filter to
included only those known observables. We could decide to only study
SNPs in FDA-approved drug targets.</p>

<table>
  <tbody>
    <tr>
      <td>[</td>
      <td>p[4.5cm]{}</td>
      <td>c</td>
      <td>p[6cm]{}</td>
      <td>]{}</td>
    </tr>
  </tbody>
</table>

<p><br />
&amp; [<strong>Gene Count</strong>]{} &amp; [<strong>Reference</strong>]{}<br />
&amp; [19,194]{} &amp; [HUGO 2018 [@HUGO2018]]{}<br />
[FDA-approved drug targets]{} &amp; [385]{} &amp; [Wishart 2018
[@Wishart2018]]{}<br />
[Drug targets]{} &amp; [201]{} &amp; [Nelson 2012 [@Nelson2012]]{}<br />
[Autosomal dominant genes]{} &amp; [307]{} &amp; [Blekhman 2008
[@Blekhman2008]]{}<br />
[Autosomal dominant genes]{} &amp; [631]{} &amp; [Berg 2013 [@Berg2013]]{}<br />
[Autosomal recessive genes]{} &amp; [527]{} &amp; [Blekhman 2008
[@Blekhman2008]]{}<br />
[Autosomal recessive genes]{} &amp; [1073]{} &amp; [Berg 2013 [@Berg2013]]{}<br />
[X-linked genes]{} &amp; [66]{} &amp; [Blekhman 2008 [@Blekhman2008]]{}<br />
[X-linked recessive genes]{} &amp; [102]{} &amp; [Berg 2013 [@Berg2013]]{}<br />
[X-linked dominant genes]{} &amp; [34]{} &amp; [Berg 2013 [@Berg2013]]{}<br />
[X-linked ClinVar genes]{} &amp; [61]{} &amp; [Landrum 2014 [@Landrum2014]]{}<br />
[All dominant genes]{} &amp; [709]{} &amp; [Blekhman 2008, Berg 2013
[@Blekhman2008; @Berg2013]]{}<br />
[All recessive genes]{} &amp; [1183]{} &amp; [Blekhman 2008, Berg 2013
[@Blekhman2008; @Berg2013]]{}<br />
[Homozygous LoF tolerant]{} &amp; [330]{} &amp; [Lek 2016 [@lek2016analysis]]{}<br />
[Essential in culture]{} &amp; [283]{} &amp; [Hart 2014 [@Hart2014]]{}<br />
[Essential in culture*]{} &amp; [683]{} &amp; [Hart 2017 [@Hart2017]]{}<br />
[Non-essential in culture*]{} &amp; [913]{} &amp; [Hart 2017 [@Hart2017]]{}<br />
[Essential in mice]{} &amp; [2,454]{} &amp; [Blake ‘11, Georgi ‘13, Liu ‘13
[@Blake2010; @Georgi2013; @Liu2013]]{}<br />
[Genes nearest to GWAS peaks]{} &amp; [6,336]{} &amp; [MacArthur 2017
[@MacArthur2017]]{}<br />
[DNA Repair Genes]{} &amp; [178]{} &amp; [Wood 2005 [@Wood2005]]{}<br />
[DNA Repair Genes]{} &amp; [151]{} &amp; [Kang 2012 [@Kang2012]]{}<br />
[ClinGen haploinsufficient genes]{} &amp; [294]{} &amp; [Rehm 2015
[@Rehm2015]]{}<br />
[Olfactory receptors]{} &amp; [371]{} &amp; [Mainland 2015 [@Mainland2015]]{}<br />
[Reported in ClinVar]{} &amp; [3078]{} &amp; [Landrum 2014 [@Landrum2014]]{}<br />
[Kinases]{} &amp; [347]{} &amp; [UniProt 2016 [@UniProt2016]]{}<br />
[GPCRs from guide to pharmacology]{} &amp; [391]{} &amp; [Alexander 2017,
Harding 2018. [@Alexander2017; @Harding2017]]{}<br />
[GPCRs from Uniprot]{} &amp; [756]{} &amp; [UniProt 2016 [@UniProt2016]]{}<br />
[Natural product targets]{} &amp; [37]{} &amp; [Dancik 2010 [@Dancik2010]]{}<br />
[BROCA - Cancer Risk Panel]{} &amp; [66]{} &amp; [BROCA Cancer Risk Panel
[@BROCACancerRiskPanel]]{}<br />
[ACMG V2.0]{} &amp; [59]{} &amp; [Kalia 2017 [@Kalia2016]]{}<br />
[GPI-anchored proteins]{} &amp; [135]{} &amp; [UniProt 2016 [@UniProt2016]]{}\</p>

<p>@verma2018rare take an interesting approach to comparing druggable
targets with population genetics data. DrugBank is a database for over
800 genes with over 950 unique drugs. Genetic data can be filtered for
these genes and targeted for LoF variants. Association analysis consists
of logistic regression using the ICD-9 codes, and linear regression
using quantitative variables. This gene binding and regression analysis
steps are done using BioBin.</p>

<p>The International Statistical Classification of Diseases and Related
Health Problems (commonly known as the ICD) provides alpha-numeric codes
to classify diseases and a wide variety of signs, symptoms, abnormal
findings, complaints, social circumstances and external causes of injury
or disease. Nearly every health condition can be assigned to a unique
category and given a code, up to six characters long. Such categories
usually include a set of similar diseases</p>

<p>BioBin relies on the Library of Knowledge Integration (LOKI), which
integrates multiple databases providing a comprehensive biological
knowledge platform for variant binning [@pendergrass2013genomic]. The
LOKI database consolidates biological information from several sources,
most notably the National Center for Biotechnology (NCBI) dbSNP and
Entrez Gene, Kyoto Encyclopedia of Genes and Genomes (KEGG), Reactome,
Gene Ontology (GO), Protein families database (Pfam), NetPath-signal
transduction pathways, amongst others
[@coordinators2017database; @kanehisa2011kegg; @milacic2012annotating; @ashburner2000gene; @finn2013pfam; @kandasamy2010netpath].</p>

<h1 id="sec:cohort_network_analysis">Rare disease cohort network analysis</h1>
<h2 id="introduction-1">Introduction</h2>
<p>The exome sequencing is most commonly used for genetic diagnosis in
single use cases. Over the next decade exome and genome sequencing will
become very commonplace for the average person at least in high-GDP
countries. A massive expanse in population genetics data will provide
the information that GWAS studies have always sought. We will still be
left with large genomic black holes; that is conserved coding and
non-coding regions that are obviously important as the determinants of
human health within Mendelian genetics. To uncover the function of these
genetic loci we will still be at the mercy of cohort size in rare
disorder studies. For true rare genetic disorders, a disease frequency
of 0.01% equates to approximately 800,000 cases worldwide for diseases
that are not embryonic lethal and where lifespan is about normal. If we
consider high income populations where genomic sequencing would be
common, then we may have 100,000 cases. A very well organised rare
disease study would do well to recruit 1 per 1000 in a clearly defined
disease; this is currently the situation although most studies cannot
finance full genomic investigation. Therefore, now and well into the
future, rare disease studies will generally be limited to a maximum
number of living participants on the scale of hundreds.</p>

<p>Current best practices in genomic analysis will first identify “low
hanging fruit”; single cases in a cohort with a clear genetic
determinant (e.g. haploinsufficiency of a well-defined dominant gene).
The second order will identify commonly mutated genes or loci based on
burden testing comparing cases to controls or background population
genetics. Many disorders have a phenotype that can be derived from
mutations in several different genes. The encoded genes generally are a
part of the same protein pathway, even directly upstream and downstream
of each other in some cases. For example, covers this topic with
individual cases of RAG1 and RAG2 deficiency.</p>

<p>Proposed here is a statistically robust and unbiased method to find
variants in protein-coding genes that share a common functional protein
pathway for a disease cohort. <strong>Figure [fig:method]</strong> provides a
high-level graphical summary of the concept. <strong>Figure [fig:abstract]</strong>
conveys the theory of the procedures for this method in more detail with
the major datasets explicitly shown.</p>

<p><strong>Deleterious rare variants in damaged protein pathways in rare
disease.</strong> A. GATK best practices were used for whole exome analysis
with joint genotyping for cases and controls; 200 in total. Custom
filtering [@vcfhacks2015Parry] extracted variants of high impact
consequence (ostensibly loss-of-function (LoF)), present only in cohort
cases. B. Genes harboring rare predicted LoF variants were grouped based
on protein-protein interactions [@String2017Szklarczyk] using a Markov
cluster algorithm [@Enright2002efficienct]. C. Case-control testing was
performed on each protein pathway cluster.</p>

<p><strong>FigureLabel</strong>
method
<img src="/images/net_analysis/method.png" width="40%" /></p>

<p><strong>Rare variant analysis and protein pathway significant enrichment.</strong>
A. DNA is collected and sequenced. B. Routine genomic analysis is
carried out according to best practices, for both (i) control and (ii)
case groups of patients. First, all rare variants are output, followed
by a smaller subset of loss-of-function (LoF) variants. C.
Variant-carrying genes are cluster of protein-protein interactions based
on function and ontology. D. A clustering method is applied to break a
large highly connected network into smaller individual ones. E. The
number of tests can be reduced by, for example, testing only networks
that carry a threshold level of LoFs and are therefore biologically
relevant to disease. F. Deleterious variant load per network was tested
for enrichment in cases, controls, or random sampling. G. Multiple
testing correction is applied to identify the critical significant
threshold.
<strong>FigureLabel</strong>
abstract
<img src="/images/net_analysis/abstract.pdf" width="40%" /></p>

<h2 id="exome-analysis">Exome analysis</h2>
<p>Exome sequencing analyses has been discussed in detail. The rare disease
cohort network analysis requires less tailored analysis steps than
traditional variant interpretation. Therefore, the data preparation is
briefly outlined here.</p>

<p>Sequences were trimmed and quality controlled using FastQC via
Trim-galore. Reads were aligned to GrCh37 using BWA-MEM. GATK “best
practices” were used for marking duplicate reads, recalibration, and
whole cohort variant quality score recalibration before generating
genomic VCFs with HaplotypeCaller and joint genotyping. Filtering and
prediction of functional consequences was performed using Variant Effect
Predictor<br />
(http://www.ensembl.org/info/docs/tools/vep/index.html), Exome Variant
Server<br />
(http://evs.gs.washington.edu/EVS/), The Single Nucleotide Polymorphism
database<br />
(https://www.ncbi.nlm.nih.gov/projects/SNP/) and ClinVar
(https://www.ncbi.nlm.nih.gov/ clinvar/), The Exome Aggregation
Consortium and The Genome Aggregation Database<br />
(http://gnomad.broadinstitute.org). Filtering of common variations and
annotation was performed using VCFhacks<br />
(https://github.com/gantzgraf/vcfhack). Candidate variants were required
to pass the following filtering conditions: frequency (count/coverage)
between 20-100%, according to VEP-annotation at least one canonical
transcript is affected with one of the following consequence: variants
of the coding sequence, frameshift, missense, protein altering, splice
acceptor, splice donor, or splice region; an inframe insertion or
deletion; a start lost, stop gained, or stop retained, or according to
VEP an GnomAD frequency unknown, &lt;=0.01, or with clinical
significance ‘path’. VCFhacks [@vcfhacks2015Parry] was used for
cohort-specific filtering retained functional variants that were present
in at least one case but absent in controls (for case-driven PPI
clustering). The same criteria were used to also collect functional
variants that were present in at least one control but absent in
controls (for control-driven PPI clustering).</p>

<h2 id="cluster-list-preparation">Cluster list preparation</h2>
<p>Group-specific variant data was extracted from the joint cohort.
Specifically, the datasets came from the routine analysis pipeline show
in <strong>Figure [fig:analysis_flow]</strong> as the output of the process
“filter on Sample” and converted from VCF to tsv format using the
process “annovcftoSimple” using the tool VCFhacks [@vcfhacks2015Parry].
Four gene lists were prepared consisting of the following groups; (1)
variants present in controls and (2) variants present in cases and
further divided for genes that harboured either (i) all rare variants or
(ii) only potential loss-of-function variants. Specifically, the
datasets for (i) all rare variants came from the output of the process
“filter on sample” via “get functional variants”. The datasets for (ii)
potential loss-of-function variants is a subset of (i), processed by the
R script at the step where “damaging variants” are written out to file.</p>

<p>Analysis workflow structure. Tools used are shown in square boxes.
Reference data used secondary to inputs are shown as light boxes with
curved sides. Key output files are shown by light slanted boxes. Storage
structure is divided between long-term and short-term
storage.
<strong>FigureLabel</strong>
analysis_flow
<img src="/images/net_analysis/analysis_flow.pdf" width="80%" /></p>

<h2 id="sec:net_construction">Network construction</h2>
<p>Group-specific gene lists [1 (i-ii) and 2 (i-ii)] were assessed for
PPI using the STRING database [@String2017Szklarczyk] via Cytoscape
[@Shannon2003cytoscape]. An initial PPI network was generated for each
of the 4 dataset groups. The STRINGdb default confidence score cut-off
(0.4) was used for these tests. This score is the measure of evidence
required to create an interaction between two nodes. A stricter value
can be set if networks are too large. Query genes were defined as nodes,
PPI were defined as edges, and networks of proteins linked through PPI
were defined as clusters. Clusters or networks can also be generally
considered as making up a part of a protein pathway.</p>

<p>[*[5]{}[Z]{}]{}</p>

<p><br />
&amp; [Network cluster]{} &amp; [Number of nodes]{} &amp; [Number of edges]{} &amp;
[Number of clusters]{}<br />
&amp; [Total ]{} &amp; [1956]{} &amp; [9559]{} &amp; [114]{}<br />
&amp; [No edges]{} &amp; [1]{} &amp; [0]{} &amp; [107]{}<br />
&amp; [One edge]{} &amp; [2]{} &amp; [1]{} &amp; [6]{}<br />
&amp; [Large multi-edge]{} &amp; [1837]{} &amp; [9553]{} &amp; [1]{}<br />
&amp; [Total]{} &amp; [2305]{} &amp; [14139]{} &amp; [102]{}<br />
&amp; [No edges]{} &amp; [1]{} &amp; [0]{} &amp; [77]{}<br />
&amp; [One edge]{} &amp; [2]{} &amp; [1]{} &amp; [3]{}<br />
&amp; [Two edges]{} &amp; [3]{} &amp; [2]{} &amp; [1]{}<br />
&amp; [Large multi-edge]{} &amp; [2219]{} &amp; [14134]{} &amp; [1]{}\</p>

<p><strong>Table [table:node_summary]</strong> lists the characteristics of PPIs for
genes found to harbour functional, potential LoF rare variants in cases
and controls (i.e. gene lists 2 [i-ii]). Most query proteins were seen
to cluster into one large multi-edge node which contained many weak
interactions. The data used in this table is represented again visually
in <strong>Figure [fig:damage_list_case]</strong>. Each dot, or node represents a
protein-coding gene that has at least one potentially damaging variant.
The edges, or lines connecting nodes, represent known PPI data that link
proteins. This visual information clearly illustrates the body of
functional protein data that can be included in variant analysis.
However, since nearly every protein has some potential evidence of
effect on many other proteins, then no clear definable protein pathway
can be seen.</p>

<p><strong>Genes harbouring potentially damaging variants in a disease
cohort.</strong> A visual representation of PPI occurring in all genes that
harbour potentially damaging functional variants in a typical disease
cohort.
<strong>FigureLabel</strong>
damage_list_case
<img src="/images/net_analysis/damage_list_case.png" width="50%" /></p>

<p>To segregate protein pathways and refine the number of genes (nodes) in
each cluster, the Markov cluster algorithm (MCL) was used
[@van2000graph; @Enright2002efficienct]. The principal data-specific
adjustment required for using MCL is the inflation operator, which
regulates cluster granularity or tightness. The optimum inflation
parameter for separating protein pathways was found to be 2.5, using a
measure of uniform distribution across datasets. <strong>Figure
[fig:inflation]</strong> illustrates an optimal inflation of a large PPI
network into smaller, clearly defined protein pathway clusters.</p>

<p>As a reference example, <strong>table [table:node_edge]</strong> lists three
inflation parameters tested for most consistent separation (2.5, 3, 4)
and shows the effect of adjustment on the total number of edges (protein
interactions). The median number of nodes (query proteins) are shown for
cases and controls (also shown as total number of nodes in <strong>table
[table:node_summary]</strong>).</p>

<p>[@l@l@]{} [0pt][0pt]</p>

<p>[Inflation separates protein pathways][<strong>=0 Inflation separates
protein pathways.</strong> A visual representation the ideal inflation
parameter used on a PPI cluster. Weak bonds are broken and strong bonds
draw nodes closer together. No bonds are retained between clusters. With
this type of inflation each protein network cluster can be investigated
without considering overlaps.]{} [fig:inflation]</p>

<p>[*[4]{}[Z]{}]{}</p>

<p><br />
&amp; &amp; [Total count median]{} &amp; [Node/Edge ratio ]{}<br />
&amp; &amp; [Case/control $\pm$ S.D.]{} &amp; [Case/control $\pm$ S.D.]{}<br />
&amp; &amp; [2130.5 $\pm$ 246.78]{} &amp;<br />
&amp; [PPI only *]{} &amp; [11849 $\pm$ 3238.55]{} &amp; [0.18 $\pm$ 0.03]{}<br />
&amp; [Inflation 2.5]{} &amp; [2787.5 $\pm$ 740.34]{} &amp; [0.78 $\pm$ 0.12]{}<br />
&amp; [Inflation 3]{} &amp; [4229.5 $\pm$ 3669.18]{} &amp; [0.77 $\pm$ 0.61]{}<br />
&amp; [Inflation 4]{} &amp; [1199.5 $\pm$ 146.37]{} &amp; [1.78 $\pm$ 0.01]{}\</p>

<p><strong>Figure [fig:network_size_inflation]</strong> illustrates the effect of
adjusting the inflation parameter for MCL clustering on protein
networks. After MCL clustering, cases and controls were found to group
into 928 and 1034 networks clusters respectively. Of these, 494 and 568
were single-node (single-protein) “clusters” which shared no interaction
with another protein while 434 and 466 clusters had at least one
interaction between proteins. The cumulative probability plot (figure
[fig:cumulative_sum_net_rank]) shows the cumulative sum of proteins
per network against network rank size. <strong>Figure [fig:qqplot_2]</strong>
shows qqplot for the same data for distribution compared between groups
after inflation at 2.5.</p>

<p><strong>Effect of inflation on network size distribution.</strong> The outcome on
network size is demonstrated to compare effect of two inflation
parameters. An ideal separation of networks should provide an geometric
decrease in the number of proteins per network regardless of the sample
group. Inflation parameter 2.5 produced the ideal distribution while
inflation parameter 3 produced one large, poorly separated network and a
large increase in single-protein nodes on one group. Binwidth of
10.
<strong>FigureLabel</strong>
network_size_inflation
<img src="/images/net_analysis/network_size_inflation_free.pdf" width="40%" /></p>

<p><strong>Cumulative sum of network rank by size</strong>. The effect of inflation on
network size distribution could be potentially measured automatically by
quantifying the cumulative sum of network rank by size and determining
the best inflation parameter to use. This process would reduce user
bias.
<strong>FigureLabel</strong>
cumulative_sum_net_rank
<img src="/images/net_analysis/cumulative_sum_net_rank.pdf" width="40%" /></p>

<p><strong>QQ plot illustrating uniform inflation.</strong> The data presented in
figure [fig:cumulative_sum_net_rank] is used to produce the
quantile-quantile plot for the most uniform distribution between the
case and control groups after all inflation parameters were
tested.
<strong>FigureLabel</strong>
qqplot_2
<img src="/images/net_analysis/qqplot_2.pdf" width="40%" /></p>

<p><strong>Figure [fig:network_size_nodes]</strong> shows the number of proteins per
network. For example, 235 clusters (470 protein nodes) were seen for
cases where only one interaction was shared between two proteins. A
median of 0.78 nodes-per-edge (proteins-per-interaction) was found in
the cases group; naturally the majority of edges appear in large network
clusters and therefore the frequency of nodes-per-edge increases as
network sizes decrease.</p>

<p>Number of proteins per network for case-driven clustering. The size of
protein networks has a geometric distribution that decreases until
protein (nodes) with no interactions remain; in this cases approximately
200 out of 400 proteins did not play a major role in a single
pathway.
<strong>FigureLabel</strong>
network_size_nodes
<img src="/images/net_analysis/network_size_nodes.pdf" width="40%" /></p>

<h2 id="sec:random_sample">Random sampling</h2>
<p>With our group-specific gene lists [1 (i-ii) and 2 (i-ii)], prepared
in section [sec:net_construction], we found the distribution of genes
per networks and output the list of genes in each network for all 4
datasets. The mean number of genes per network rank was found between
cases and controls, again for (i) all rare variants and (ii) only
potential loss-of-function variants. A third gene cluster list was
produced by random sampling gene symbols in artificial networks equal to
the same median size as case-driven and control-driven clusters in from
datasets (i) and (ii). The resulting dataset [3 (i-ii)] mirrors those
of cases and controls but instead of true PPI networks, the networks
contained randomly assigned genes.</p>

<h2 id="expanding-damaged-gene-mcl-clusters">Expanding damaged gene MCL clusters</h2>
<p>For each of the 4 MCL-clustered datasets, cases and controls [1 (i-ii)
and 2 (i-ii)], the cluster ID and list of gene symbols was extracted.
The gene lists of network clusters made from datasets (ii) (potential
LoF) were used to find the network clusters in (i), all-variant gene
clusters, the contained the same overlapping genes. This occurs where
list (ii) is a subset of list (i). The clusters that contained gene
overlaps were extracted since they contained at least one potential LoF
per network. Using this output, the total variant load in “damaged
pathways” could be compared. For clarity, this procedure is summarised
again in Box [box:expanding]; items <strong>A-B</strong>. Item <strong>C</strong> outlines the
remaining steps. <strong>Figure [fig:damage_list_case]</strong> illustrates the
effect of inflation with an ideal inflation parameter. The large network
of PPI were separated into individually contained protein networks.</p>

<p>[box:expanding]</p>

<h2 id="sec:burden_rank">Burden rank</h2>
<p>Our downstream case-control testing compares the mean total variant load
per network. To prevent dilution of our significance threshold due to
multiple testing an arbitrarily high number of networks we assumed that
protein networks harbouring loss-of-function variants at a consistent
frequency in all groups were unlikely to contain genes of interest. To
remove these networks, we firstly found (<em>p</em>) the ratio of LoF to all
variants within the group per network, and secondly found (<em>q</em>); the
ratio of <em>p</em> between groups per network. Networks were ranked by value
<em>q</em>. Values passing a threshold of 0.7 were included for total variant
load means testing (i.e. 70% of ostensibly damaging variants occurred in
cases regardless of the proportion of total variants). This also has the
effect that even if there is no significant difference in a case/control
total-variant means test downstream, potential false negatives may be
rescued by checking for LoF enrichment. This method is applied to real
data in section [sec:enrichment_test] and <strong>table
[table:bh_real]</strong>.</p>

<h2 id="sec:number_test">Determining the number of tests <em>m</em></h2>
<p>The number of tests should be determined by the predefine LoF ratio per
network, <em>q</em>. This value is arbitrarily set and has the problem that an
investigator can decide to use a higher threshold to nude the critical
significant threshold in a desired direction. However, testing roughly
the top 20-30% of networks is suggested. In our experiments we set our
test number as the top 25% of burden-ranked networks. This will be
approximately 10 networks to test (the asymptote of network numbers peak
when the study size increases over approx. 400 samples as all of the
possible PPIs are saturated once the maximum queryable genes are
included). Study sizes that are much larger than this will likely only
(1) be for disease that are not very rare and (2) be large enough to
start expecting single gene significance levels without requiring
network analysis. However, some very strict filtering rules could allow
larger studies with this method.</p>

<h2 id="significance-testing">Significance testing</h2>
<p>We hypothesised firstly that no variant enrichment would be seen in
random sampling or control-driven gene clusters, and secondly enrichment
would only be seen in case-driven clusters for protein-pathways that
provide susceptibility to viral infection. For measuring a significant
enrichment of functional variants in a protein network, there are three
factors to consider.</p>

<ol>
  <li>
    <p>Our aim is to do a comparison of means between case and control, for
total variant load per network.</p>
  </li>
  <li>
    <p>This is done in three iterations; [1] control-driven, [2]
case-driven, and [3] random sample-driven.</p>
  </li>
  <li>
    <p>We correct our significance threshold to account for multiple
testing using the Benjamini-Hochberg procedure.</p>
  </li>
</ol>

<p>With our group-specific gene lists [1 (i-ii), 2 (i-ii), and [3
(i-ii)], prepared in sections [sec:net_construction] and
[sec:random_sample], we found the distribution of genes per networks
and output the list of genes in each network for all 6 datasets. In each
of the 3 “all variant” datasets we simply do a comparison of means for
total variant load per network comparing case to control, or random.</p>

<p>While the test is not complicated, the significance threshold deserves
an in-depth explanation; this is a novel method and most people
replicating this study will not have experience with the statistical
procedures required. The statistical significance also only allows a
narrow margin for successful discovery. When a large number of tests are
performed, one is likely to produce P-values that are “statistically
significant” by chance (P &lt; 0.05), even if the null hypothesis is
true. The null hypothesis would state that “random controls and people
with disease have the same average frequency of potentially pathogenic
variants in some protein pathway”. The alternate hypothesis would state
that “people with disease have an increased frequency of potentially
pathogenic variants in some protein pathway than random controls”.</p>

<p>Traditionally, Bonferroni correction has been used in cases like this.
For each “family” (network means test) being tested one must correct the
critical P-value. For example, for one test a significant P-value might
be 0.05 and below this we consider the result to be significant. The
chance of getting this result if the null hypothesis was true would be
5%. That does not mean that there is 5% chance that it is true. The
following examples are reiterated summary of the topic found in the
Handbook of biological statistics [@mcdonald2009handbook].</p>

<p>For multiple tests of “families” then we need to adjust the P-value
since we are more likely to get false positives by chance. In a
published example, @garcia2014calorie tested 25 associations with
mammographic density, which is an important risk factor for breast
cancer. The 25 “families” tested were dietary variables including “Total
calories”, “Olive oil”, “whole milk”, “white meat”, etc. For each
variable a P-value was given for its association with mammographic
density, i.e. total calories P &lt; 0.001, olive oil P = 0.008, whole
milk P = 0.039.</p>

<p>To perform a Bonferroni correction, the critical P-value (or significant
threshold) should be divided by the number of tests, 0.05/25 = 0.002.
Therefore, only “total calories” would be significantly associated with
the risk factor. If 75 more variables were measured (100 total) then the
critical P-value would have to be 0.05/100 = 0.0005. However, it may not
be reasonable to invalidate the significance of the original findings.
Using Bonferroni correction for family-wise error rate can mean
extremely small P-values. So instead we use a more powerful method for
controlling the false discovery rate; the Benjamini-Hochberg procedure
[@simes1986improved; @benjamini1995controlling].</p>

<p>In this procedure, we compare each individual P-value to its
Benjamini-Hochberg critical value, $(i/m)Q$, where $i$ is the rank, $m$
is the total number of tests, and $Q$ is the chosen false discovery
rate. The largest P-value that has $P&lt;(i/m)Q$ (i.e. P less than
BH-critical value) is significant, and all of the P-values smaller than
it are also significant, even the ones that aren’t less than their own
Benjamini-Hochberg critical value.</p>

<p>So in the same example, with 25 tests and Benjamini-Hochberg critical
value for a false discovery rate set to 0.25, table
[table:bh_example] shows the outcome. The largest P-value that is
less than its $(i/m)Q$ values is 0.042 for protein. Therefore, the first
5 variables are significantly associated, including whole milk and white
meat despite the fact that their BH-critical value is higher than their
P-value. If we were to never have measured protein in this example, $m$
the number of tests would be 24, slightly increasing the BH-critical
value, and again identify a significant association for the first 4
tests. Someone interested can recalculate this table to see this effect.</p>

<p>[*[2]{}[Z]{}]{}</p>

<p><br />
&amp; [P value]{}<br />
&amp; [&lt;0.001]{}<br />
[Olive oil]{} &amp; [0.008]{}<br />
[Whole milk]{} &amp; [0.039]{}<br />
[White meat]{} &amp; [0.041]{}<br />
[Proteins]{} &amp; [0.042]{}<br />
[Nuts]{} &amp; [0.06]{}<br />
[Cereals and pasta]{} &amp; [0.074]{}<br />
[White fish]{} &amp; [0.205]{}<br />
[Butter]{} &amp; [0.212]{}<br />
[Vegetables]{} &amp; [0.216]{}<br />
[Skimmed milk]{} &amp; [0.222]{}<br />
[Red meat]{} &amp; [0.251]{}<br />
[Fruit]{} &amp; [0.269]{}<br />
[Eggs]{} &amp; [0.275]{}<br />
[Blue fish]{} &amp; [0.34]{}<br />
[Legumes]{} &amp; [0.341]{}<br />
[Carbohydrates]{} &amp; [0.384]{}<br />
[Potatoes]{} &amp; [0.569]{}<br />
[Bread]{} &amp; [0.594]{}<br />
[Fats]{} &amp; [0.696]{}<br />
[Sweets]{} &amp; [0.762]{}<br />
[Dairy products]{} &amp; [0.94]{}<br />
[Semi-skimmed milk]{} &amp; [0.942]{}<br />
[Total meat]{} &amp; [0.975]{}<br />
[Processed meat]{} &amp; [0.986]{}\</p>

<p>The choice of a false discover rate depends on the application. False
positives can waste time, resources, and pollute future work. Minimising
false negatives could result in missing a very important finding, that
is, when there is a real effect but it is not deemed statistically
significant. Allowing a pre-determined level false negatives is often
reasonable. As in our application, finding enriched protein networks is
the main goal, and downstream work will also be done such as clinical
interpretation or functional studies which will catch false negatives.
Therefore, the false discovery rate does not have to be very small;
consider that our input dataset is already filtered down to ostensibly
damaging rare variants. Furthermore, the input dataset is essentially
the result of traditional best practices in exome or genome sequencing
analysis.</p>

<h2 id="sec:enrichment_test">Enrichment testing</h2>
<p>For all networks, the top 30 networks in size (largest to smallest;
1-30) were ordered using the burden rank (sec [sec:burden_rank]).
From these, the number of tests was set (according to the rules defined
in sec [sec:number_test], so that only the top 8 burden-ranked
networks were means tested for their total variant load. <strong>Figure
[fig:means_test]</strong> shows the test of means for the top 8 protein
pathway networks. Table [table:bh_real] lists the P-values assessed
for significance using the BH-procedure. We found that only one of the
networks was significantly associated with a pathogen-specific
immunodeficiency. The variant load was significantly higher than for
controls. The total potential LoF variants only accounted for 30.5% of
total variants in the network but was ranked high during the burden rank
(see sec [sec:burden_rank]) because no controls harboured potential
LoF variants in this network and therefore 100% occurred in cases. This
protein network contained genes responsible for pathogen detection; some
genes <em>might have been</em> identified as candidates using the routine exome
analysis pipeline such as the antiviral receptors and antiviral
interferon regulatory factors. However, most of the other genes that are
integral to this pathway would not have been identified by standard best
practices. The protein network is shown in <strong>Figure
[fig:immune_cluster]</strong> where potential LoF variants-harbouring genes
are coloured in red. Gene candidates with variants of unknown
significance are coloured in red and, anecdotally, the colouring
thereafter becomes lighter (orange to yellow) based on the likelihood of
candidates being identified by manual interpretation of unknown
candidates.</p>

<p><strong>Case and control means test.</strong> The total rare variants per network
are shown, comparing groups. A test of means was conducted in this test
dataset and P-values are
shown.
<strong>FigureLabel</strong>
means_test
<img src="/images/net_analysis/means_test.pdf" width="30%" /></p>

<p>[*[6]{}[Z]{}]{}</p>

<p><br />
&amp; [<strong>LoF freq in cases</strong>]{} &amp; [<strong>LoF freq due to cases per network</strong>]{}
&amp; [<strong>P-value</strong>]{} &amp; [<strong>rank</strong>]{} &amp; [<strong>$(i/m)Q$</strong>]{}<br />
&amp; [0.306]{} &amp; [1]{} &amp; [0.023]{} &amp; [1]{} &amp; [0.025]{}<br />
[27]{} &amp; [0.429]{} &amp; [1]{} &amp; [0.12]{} &amp; [2]{} &amp; [0.05]{}<br />
[16]{} &amp; [0.6]{} &amp; [0.919]{} &amp; [0.13]{} &amp; [3]{} &amp; [0.075]{}<br />
[19]{} &amp; [0.281]{} &amp; [0.835]{} &amp; [0.14]{} &amp; [4]{} &amp; [0.1]{}<br />
[25]{} &amp; [0.25]{} &amp; [1]{} &amp; [0.28]{} &amp; [5]{} &amp; [0.125]{}<br />
[11]{} &amp; [0.357]{} &amp; [0.838]{} &amp; [0.33]{} &amp; [6]{} &amp; [0.15]{}<br />
[10]{} &amp; [0.516]{} &amp; [0.856]{} &amp; [0.34]{} &amp; [7]{} &amp; [0.175]{}<br />
[18]{} &amp; [0.474]{} &amp; [0.85]{} &amp; [0.47]{} &amp; [8]{} &amp; [0.2]{}\</p>

<p>[@l@l@]{} [0pt][0pt]</p>

<p>[Protein network with significantly enriched variant load.][<strong>=0
Protein network with significantly enriched variant load.</strong> From the
example data, network 22 was significantly enriched for rare variants.
The same clustering method was again used on all variants with a less
stringent variant frequency (&lt;1% in general population and present in
any cohort sample). With the resulting, more common variants, the full
protein network can be seen (about double in size compared to only very
rare variants). Gene candidates with variants of unknown significance
are coloured in red and, anecdotally, the colouring thereafter becomes
lighter (orange to yellow) based on the likelihood of candidates being
identified by manual interpretation of unknown candidates. ]{}
[fig:immune_cluster]</p>

<h1 id="discussion">Discussion</h1>
<p>Exome sequence data is usually about 4 GB of information per person.
Whole genomes are approximately 50GB of data. The analysis of whole
genome sequencing is almost identical to the exome pipeline outlined
here. While there is much more information (for not much of a higher
cost), a lot of the non-coding sequence contains information that we
can’t yet interpret. For Mendelian disease the whole exome often
uncovers the coding variants that explain disease. We may not understand
anything else outside the exome (and the surrounding splice regions) in
relation to a patients’ disease. Mutations in the promoters or enhancers
that prevent transcription may not be as readily interpretable as the
majority of coding variant effects. Therefore, whole genome is often not
required. This excuse for performing exome sequencing rather than whole
genome mostly depends on value for money. Performing all the different
kinds of analysis, including non-coding genome analysis, requires many
people with expertise in each topic. Even if whole genome data was
available to smaller research teams, it is often the case that they
cannot carry out all the work required to interpret it. For national
level genomics, there is no question that whole genome sequencing is
preferential. We can retain the data for decades with hundreds of
experts to share the work-load, while the cost is essentially a
political factor. An important question to address is the right for a
person to agree to genetic forfeiture. We are at the brink of
preventative medicine using genome sequencing in newborns. Regardless of
the popular ethical consensus, any preventative non-consenting genomic
analysis can be considered coercion.</p>

<p><strong>Figure [fig:immune_cluster]</strong> illustrates how not only can very
rare or damaging variants be clustered, but the same network can be
expanded to include peripherally interacting genes. This modification
may be used for downstream functional work such as looking at
pathway-level expression data. An important consideration for protein
network cohort analysis is evident in <strong>Figure
[fig:network_size_nodes]</strong>. About 50% of genes with a functional
variant are do not get clustered into a PPI network (protein pathway).
However, some of these genes could still harbour a potential
loss-of-function or damaging variant. If we found 3 significantly
enriched protein networks, a potential 4th missed network (false
negative), because of unclustered genes, would not detract from the
significant findings. Therefore, the singleton genes remaining from MCL
clustering should be listed and reassessed based on traditional
interpretations; variant effect, loss-of-function intolerance, etc. The
converse, a false positive because of over clumping weakly related
proteins, would be negative.</p>

<p>The analysis of genomic data is an iterative process. Therefore, access
to raw unprocessed genetic information is often required to utilize
cutting edge methods [@Auwera2013From; @Poplin2018Scaling]. Furthermore,
genetic analysis is a complex, multi-stage procedure. Due to the
inherent complexity, there is a number of output streams which consist
of different data types. To provide seamless integration with current
best practices in precision medicine, it is valuable to adhere to
standard genomic data types, including CRAM, SAM/BAM, FASTQ, and VCF
[@Li2009Sequence; @Hsi2011Efficient; @cock2009sanger; @Danecek2011Variant].
There is benefit to creating new data formats that increase efficiency.
However, by focusing on key data types in genomics, one can enable
integration with most current software
[@pabinger2014survey; @Auwera2013From].</p>

<p>An interesting caveat to genetic data is that at pre-processing stages,
several data types cannot be currently provided with protection through
the use of modern cryptographic methods
[@froelicher2017unlynx; @juvekar2018gazelle]. There is currently a
severe lack of tools that complement current methods required to
interrogate genetic data at different stages while protecting individual
personal genetic records. Furthermore, despite the attempts to promote
data privacy and integrity through global initiatives, such as Global
Aliance 4 Genome Health, little has been done to produce queryable data
that protects the genetic identity of a subject.</p>

<p>The privacy concerns at the early stages of data processing are often
overlooked. Almost every method offered for data security relies on
protecting only fully process data (e.g. already variant called VCF
format data) or summary statistics. In worst cases, privacy concerned
genomics falls back to “trust-based” systems where data generators or
researchers are required to accept responsibility for preventing any
re-anonymisation. Of course, researcher trust is an important factor,
however, relying on this method for protecting subject information is
immoral. Unlike nearly all clinical data, genetic data is inherently
identifiable and is not readily anonymised. The information that makes
up the data is itself the identity or commodity. In nearly every other
type of clinical data, it is only a commodity when there is an identity
to which it is paired or if it is part of a large dataset-of-normals.
The lack of strong methods of genetic data protection is not an apparent
risk generally. Extrapolating the risk which differentiates other types
of data that requires informed consent is a difficult task for many
experts. Relying on patient consent and trust in data protection is not
sufficient for the future of global genomics. Successfully overcoming
these challenges will allow for the use of analysis methods that
otherwise provide vulnerabilities against the protection of private data
[@Li2009Fast], [GA4GH (https://www.ga4gh.org)].</p>

<h1 id="conclusion">Conclusion</h1>
<p>A pipeline of routine exome analysis was outlined. Important points on
tailored analysis are demonstrated. A new method was developed for the
unbiased detection of a protein network, driving disease, based on
potential loss of function variants.</p>

<h1 id="command-line-example-code">Command line example code</h1>
<p><strong>Whole exome analysis</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash </span>
<span class="c">############################################ </span>
<span class="c">#### The basic protocol for analysis. </span>
<span class="c">#### It is best to set up a loop that </span>
<span class="c">#### can run the protocol on all samples. </span>
<span class="c">############################################ </span>
<span class="c"># Make project organisation folders </span>
<span class="nb">mkdir</span> ~/1.fastq/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/2.trim/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/3.sort/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/4.dedup/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/5.realtar/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/6.indelrealn/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/7.baserecal/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/9.printbam/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/10.gvcf/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">mkdir</span> ~/geno/ <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Typical workflow </span>
<span class="c">############################################ </span>

<span class="c">############################################ </span>
<span class="c">#### [command_cut] </span>
<span class="c">############################################ </span>
trim_galore <span class="nt">-q</span> 20 <span class="nt">-fastqc_args</span> <span class="se">\</span>
<span class="nt">-outdir</span> ~/2.trim/QC_reports <span class="nt">-illumina</span> <span class="nt">-gzip</span> <span class="se">\</span>
<span class="nt">-o</span> ~/2.trim/ <span class="nt">-length</span> 20 <span class="nt">-paired</span> <span class="se">\</span>
~/1.fastq/Sequencing_ID_L001_R1_001.fastq.gz <span class="se">\</span>
~/1.fastq/Sequencing_ID_L001_R2_001.fastq.gz <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_align] </span>
<span class="c">############################################ </span>
bwa mem <span class="nt">-t</span> 12 <span class="nt">-M</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
~/2.trim/Sequencing_ID_L001_R1_001_val_1.fq.gz <span class="se">\</span>
~/2.trim/Sequencing_ID_L001_R2_001_val_2.fq.gz <span class="se">\</span>
<span class="nt">-v</span> 1 <span class="nt">-R</span> <span class="s1">'@RG\tID:Sample_ID\tSM:Sample_ID \
tPL:ILLUMINA\tLB:Sample_ID'</span> <span class="se">\</span>
<span class="nt">-M</span> | samtools view <span class="nt">-Sb</span> - <span class="o">&gt;</span> ~/2.trim/Sample_ID.bam <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_sort] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/picard/picard-tools-2.5.0/picard.jar <span class="se">\</span>
SortSam <span class="se">\</span>
<span class="nv">I</span><span class="o">=</span> ~/2.trim/Sample_ID.bam <span class="se">\</span>
<span class="nv">O</span><span class="o">=</span> ~/3.sort/Sample_ID.sort.bam <span class="se">\</span>
<span class="nv">SO</span><span class="o">=</span>coordinate <span class="nv">CREATE_INDEX</span><span class="o">=</span>TRUE <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_dedup] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/picard/picard-tools-2.5.0/picard.jar <span class="se">\</span>
MarkDuplicates <span class="se">\</span>
<span class="nv">I</span><span class="o">=</span> ~/3.sort/Sample_ID.sort.bam <span class="se">\</span>
<span class="nv">O</span><span class="o">=</span> ~/4.dedup/Sample_ID.sort.dedup.bam <span class="se">\</span>
<span class="nv">M</span><span class="o">=</span> ~/4.dedup/Sample_ID.sort.dedup.metrics <span class="se">\</span>
<span class="nv">CREATE_INDEX</span><span class="o">=</span>TRUE <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_realtar] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx6g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> RealignerTargetCreator <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-known</span> ~/ref/1000G_phase1.indels.b37.vcf <span class="se">\</span>
<span class="nt">-known</span> ~/ref/Mills_and_1000G_gold_standard.<span class="se">\ </span>indels.b37.sites.vcf <span class="se">\</span>
<span class="nt">-I</span> ~/4.dedup/Sample_ID.sort.dedup.bam <span class="se">\</span>
<span class="nt">-o</span> ~/5.realtar/Sample_ID.sort.dedup.bam.intervals <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_indelrealign]</span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx6g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> IndelRealigner <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-known</span> ~/ref/1000G_phase1.indels.b37.vcf <span class="se">\</span>
<span class="nt">-known</span> ~/ref/Mills_and_1000G_gold_standard.<span class="se">\ </span>indels.b37.sites.vcf <span class="se">\</span>
<span class="nt">-I</span> ~/4.dedup/Sample_ID.sort.dedup.bam <span class="se">\</span>
<span class="nt">-targetIntervals</span> <span class="se">\</span>
~/5.realtar/Sample_ID.sort.dedup.bam.intervals <span class="se">\</span>
<span class="nt">-o</span> ~/6.indelrealn/Sample_ID.sort.dedup.indelrealn.bam <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_bsqr] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> BaseRecalibrator <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-knownSites</span> ~/ref/dbSnp146.b37.vcf.gz <span class="se">\</span>
<span class="nt">-knownSites</span> ~/ref/1000G_phase1.indels.b37.vcf <span class="se">\</span>
<span class="nt">-knownSites</span> ~/ref/Mills_and_1000G_gold_standard.<span class="se">\ </span>indels.b37.sites.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/7.baserecal/Sample_ID.sort.dedup.indelrealn.recal.grp <span class="se">\</span>
<span class="nt">-I</span> ~/6.indelrealn/Sample_ID.sort.dedup.indelrealn.bam <span class="se">\</span>
<span class="nt">-nct</span> 6 <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Optional check for base recalibration </span>
<span class="c">############################################ </span>
<span class="c">####  Print reads</span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx12g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> PrintReads <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-I</span> ~/6.indelrealn/Sample_ID.sort.dedup.indelrealn.bam <span class="se">\</span>
<span class="nt">-BQSR</span> ~/7.baserecal/Sample_ID.sort.dedup.indelrealn.recal.grp <span class="se">\</span>
<span class="nt">-o</span> ~/9.printbam/Sample_ID.sort.dedup.indelrealn.recal.bam <span class="se">\</span>
<span class="nt">-disable_indel_quals</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### [command_hc] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> HaplotypeCaller <span class="nt">-emitRefConfidence</span> GVCF <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-D</span> ~/ref/dbSnp146.b37.vcf.gz <span class="se">\</span>
<span class="nt">-stand_call_conf</span> 30 <span class="se">\</span>
<span class="nt">-stand_emit_conf</span> 10 <span class="se">\</span>
<span class="nt">-o</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-L</span> ~/ref/SureSelectAllExonV6/S07604514_Regions_b37.bed <span class="se">\</span>
<span class="nt">-ip</span> 30 <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="c"># deprecated -I ~/9.printbam/Sample_ID.sort.dedup.indelrealn.recal.bam</span>

<span class="c">############################################ </span>
<span class="c">#### [command_joint] </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx12g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> GenotypeGVCFs <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-D</span> ~/ref/dbSnp146.b37.vcf.gz <span class="nt">-stand_call_conf</span> 30 <span class="se">\</span>
<span class="nt">-stand_emit_conf</span> 10 <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-V</span> ~/10.gvcf/Sample_ID.sort.dedup.indelrealn.recal.HC.g.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.vcf <span class="nt">-nda</span> <span class="nt">-showFullBamList</span> <span class="nt">-nt</span> 12 <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Hard filter selecting SNVs </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx12g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> SelectVariants <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-selectType</span> SNP <span class="se">\</span>
<span class="nt">-variant</span> ~/geno/genotype.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.raw-snps.vcf <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Hard filter selecting INDELs </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx12g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> SelectVariants <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-variant</span> ~/geno/genotype.vcf <span class="se">\</span>
<span class="nt">-selectType</span> INDEL <span class="nt">-selectType</span> MNP <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.raw-indels.vcf <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Applying hard filter for SNVs </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> VariantFiltration <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-V</span> ~/geno/genotype.raw-snps.vcf <span class="se">\</span>
<span class="nt">-filterExpression</span> QD &lt; 2.0 <span class="o">||</span> FS <span class="o">&gt;</span> 60.0 <span class="o">||</span> MQ &lt; 40.0 <span class="o">||</span><span class="se">\</span>
MappingQualityRankSum &lt; <span class="nt">-12</span>.5 <span class="o">||</span> ReadPosRankSum &lt; <span class="nt">-8</span>.0 <span class="se">\</span>
<span class="nt">-filterName</span> snp_hard_filter <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.raw-snps.filtered.snvs.vcf <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Applying hard filter for INDELs </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> VariantFiltration <span class="se">\</span>
<span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-V</span> ~/geno/genotype.raw-indels.vcf <span class="se">\</span>
<span class="nt">-filterExpression</span> QD &lt; 2.0 <span class="o">||</span> FS <span class="o">&gt;</span> 200.0 <span class="o">||</span><span class="se">\</span>
ReadPosRankSum &lt; <span class="nt">-20</span>.0 <span class="se">\</span>
<span class="nt">-filterName</span> indel_hard_filter <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.raw-indels.filtered.indels.vcf <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Combine filtered results </span>
<span class="c">############################################ </span>
java <span class="nt">-Xmx8g</span> <span class="nt">-jar</span> ~/GATK/GenomeAnalysisTK.jar <span class="se">\</span>
<span class="nt">-T</span> CombineVariants <span class="nt">-R</span> ~/ref/human_g1k_v37.fasta <span class="se">\</span>
<span class="nt">-variant</span> ~/geno/genotype.raw-snps.filtered.snvs.vcf <span class="se">\</span>
<span class="nt">-variant</span> ~/geno/genotype.raw-indels.filtered.indels.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.fltd-combinedvars.vcf <span class="se">\</span>
<span class="nt">-genotypemergeoption</span> UNSORTED <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Filter variants in EdbSNP &gt;/= 1% \</span>
<span class="c">#### and not listed as pathogenic by ClinVar </span>
<span class="c">############################################ </span>
perl ~/vcfhacks-v0.2.0/annotateSnps.pl <span class="se">\</span>
<span class="nt">-d</span> ~/ref/dbSnp146.b37.vcf.gz ~/ref/clinvar_20160531.vcf.gz <span class="se">\</span>
<span class="nt">-f</span> 1 <span class="nt">-pathogenic</span> <span class="se">\</span>
<span class="nt">-i</span> ~/geno/genotype.fltd-combinedvars.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.vcf <span class="se">\</span>
<span class="nt">-t</span> 12 <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Filter variants in EVS greater &gt;/= 1% </span>
<span class="c">############################################ </span>
perl ~/vcfhacks-v0.2.0/filterOnEvsMaf.pl <span class="nt">-d</span> ~/ref/evs/ <span class="se">\</span>
<span class="nt">-f</span> 1 <span class="nt">-progress</span> <span class="se">\</span>
<span class="nt">-i</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.vcf <span class="se">\</span>
<span class="nt">-t</span> 12 <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Exac filter for population frequency </span>
<span class="c">############################################ </span>
perl ~/vcfhacks-v0.2.0/filterVcfOnVcf.pl <span class="se">\</span>
<span class="nt">-i</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.vcf <span class="se">\</span>
<span class="nt">-f</span> ~/ref/ExAC/ExAC.r0.3.sites.vep.vcf.gz <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.exac.vcf <span class="se">\</span>
<span class="nt">-w</span> <span class="nt">-y</span> 0.01 <span class="se">\</span>
<span class="nt">-b</span> <span class="se">\</span>
<span class="c"># progress bar \</span>
<span class="nt">-t</span>  <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="c"># number of threads</span>

<span class="c">############################################ </span>
<span class="c">#### Annotate with variant effect predictor </span>
<span class="c">############################################ </span>
perl ~/variant_effect_predictor/variant_effect_predictor.pl <span class="se">\</span>
<span class="nt">-offline</span> <span class="nt">-vcf</span> <span class="nt">-everything</span> <span class="se">\</span>
<span class="nt">-dir_cache</span> ~/variant_effect_predictor/vep_cache <span class="se">\</span>
<span class="nt">-dir_plugins</span> ~/variant_effect_predictor/vep_cache/Plugins <span class="se">\</span>
<span class="nt">-plugin</span> Condel,<span class="se">\</span>
~/variant_effect_predictor/vep_cache/Plugins/config/Condel/config/ <span class="se">\</span>
<span class="nt">-plugin</span> ExAC,~/ref/ExAC/ExAC.r0.3.sites.vep.vcf.gz <span class="se">\</span>
<span class="nt">-plugin</span> SpliceConsensus <span class="se">\</span>
<span class="nt">-fasta</span> <span class="se">\</span>
~/variant_effect_predictor/fasta/<span class="se">\</span>
Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz <span class="se">\</span>
<span class="nt">-i</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.exac.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.exac.vep.vcf <span class="se">\</span>
<span class="nt">-fork</span> 12 <span class="o">&amp;&amp;</span> <span class="se">\</span>

<span class="c">############################################ </span>
<span class="c">#### Confirm samples names </span>
<span class="c">############################################ </span>
perl ~/vcfhacks-v0.2.0/getSampleNames.pl <span class="se">\</span>
<span class="nt">-i</span> ~/geno/genotype.fltd-combinedvars.1pcdbsnp.1pcEVS.exac.vep.vcf <span class="o">&amp;&amp;</span> <span class="se">\</span>


<span class="c">############################################ </span>
<span class="c">## Data extraction </span>
<span class="c">############################################ </span>
<span class="c">#### Extract columns </span>
<span class="c">############################################ </span>
<span class="c">#### A list of files for</span>
<span class="c">#### where the data from</span>
<span class="c">#### one column is compiled into</span>
<span class="c">#### a master table </span>
<span class="c">############################################ </span>

<span class="c">############################################ </span>
<span class="c">#### This takes column 3 from every file</span>
<span class="c">#### and appends to the output file.</span>
<span class="c">#### Space delimited. </span>
<span class="c">############################################ </span>
<span class="nb">awk </span><span class="nv">FNR</span><span class="o">==</span>1<span class="o">{</span>f++<span class="o">}{</span>a[f,FNR]<span class="o">=</span><span class="nv">$3</span><span class="o">}</span>END<span class="o">{</span><span class="k">for</span><span class="o">(</span><span class="nv">x</span><span class="o">=</span>1<span class="p">;</span>x&lt;<span class="o">=</span>FNR<span class="p">;</span>x++<span class="o">)</span><span class="se">\ </span>
<span class="o">{</span><span class="k">for</span><span class="o">(</span><span class="nv">y</span><span class="o">=</span>1<span class="p">;</span>y&lt;ARGC<span class="p">;</span>y++<span class="o">)</span><span class="nb">printf</span><span class="o">(</span>%s ,a[y,x]<span class="o">)</span><span class="p">;</span>print <span class="o">}}</span> <span class="se">\</span>
./../<span class="k">*</span>/pheno.txt <span class="o">&gt;</span> master.txt

<span class="c">############################################ </span>
<span class="c">#### The spacer method can be changed; tab, space, comma, etc.</span>
<span class="c">#### Another way to convert later is with the following command.</span>
<span class="c">#### [The tab character (after s/) must be removed</span>
<span class="c">#### and printed to the command line using ctrl+v then tab.] </span>
<span class="c">############################################ </span>
<span class="nb">sed</span> <span class="s1">'s/ /,/g'</span> input.tsv <span class="o">&gt;</span> output.csv

<span class="c">############################################ </span>
<span class="c">#### Candidate filter</span>
<span class="c">############################################ </span>
<span class="c">#### Filter a VCF on a candidate gene list. </span>
<span class="c">############################################ </span>
<span class="c">#### List format as X:1-2000,</span>
<span class="c">#### or -b for a bed file or</span>
<span class="c">#### a list file with 1 per line. </span>
<span class="c">############################################ </span>
<span class="k">for </span>f <span class="k">in</span> ~/immune.panel/vep/<span class="k">*</span>.vcf <span class="k">do 
</span>perl ~/vcfhacks-v0.2.0/filterVcfOnLocation.pl <span class="se">\</span>
<span class="nt">-i</span> ~/immune.panel/vep/<span class="nv">$f</span> <span class="se">\</span>
<span class="nt">-b</span> ~/deep.panel.bed <span class="se">\</span>
<span class="nt">-o</span> ~/immune.panel/filter/<span class="si">$(</span> <span class="nb">basename</span> <span class="nt">-s</span> .vcf <span class="nv">$f</span> <span class="si">)</span>.panel.vcf <span class="se">\</span>
<span class="k">done</span>

<span class="c">############################################ </span>
<span class="c">#### Post-routine analysis candidate filtering.</span>
<span class="c">#### Similar filtering can be done without going back</span>
<span class="c">#### to analysis stages to create a virtual panel. </span>
<span class="c">############################################ </span>
<span class="c">#### Export all gene names and give the count.</span>
<span class="nb">sort </span>list.txt | <span class="se">\</span>
<span class="nb">uniq</span> <span class="nt">-c</span> <span class="o">&gt;</span> InflammatoryDisorderCohortHitCount.txt
<span class="c">#### Format to csv.</span>
<span class="c">#### Cross against a master list of immune genes.</span>

<span class="c">############################################ </span>
<span class="c">#### In R, import data </span>
<span class="c">############################################ </span>
master &lt;- read.csv<span class="o">(</span>./master.csv, <span class="nv">stringsAsFactors</span><span class="o">=</span>FALSE<span class="o">)</span>
InflammatoryDisorderCohortHitCount &lt;- <span class="se">\</span>
read.csv<span class="o">(</span>./ InflammatoryDisorderCohortHitCount.csv, <span class="nv">stringsAsFactors</span><span class="o">=</span>FALSE<span class="o">)</span>

<span class="c">############################################ </span>
<span class="c">#### Merge the master immune gene list</span>
<span class="c">#### with the Inflammatory disorder cohort hits. </span>
<span class="c">############################################ </span>
combine &lt;- merge<span class="o">(</span>master, InflammatoryDisorderCohortHitCount, 
        by <span class="o">=</span> Gene, all <span class="o">=</span> TRUE<span class="o">)</span>

<span class="c">############################################ </span>
<span class="c">#### Remove the genes that happen to overlap</span>
<span class="c">#### gene of interest and remove anything from</span>
<span class="c">#### the master list that is not in the cohort list. </span>
<span class="c">############################################ </span>
clean &lt;- na.omit<span class="o">(</span>combine<span class="o">)</span>

<span class="c">############################################ </span>
<span class="c">#### Write out the table. </span>
<span class="c">############################################ </span>
write.csv<span class="o">(</span>clean, ./GenesOfInterest.csv, row.names <span class="o">=</span> FALSE<span class="o">)</span>

<span class="c">############################################ </span>
<span class="c">#### The output can be sorted as of interest</span>
<span class="c">#### e.g. autosomal dominant autoinflammatory gene. </span>
<span class="c">############################################ </span>

<span class="c">############################################ </span>
<span class="c">#### Tailored filtering</span>
<span class="c">############################################ </span>
<span class="c">#### Filter on sample.</span>
<span class="c">#### May need use a -freq option</span>
<span class="c">#### to account for index hopping.</span>
<span class="c">#### Filter on sample removes anything shared</span>
<span class="c">#### with cases (-s) that are not listed but not others (-x). </span>
<span class="c">############################################ </span>
perl /home/vcfhacks-v0.2.0/filterOnSample.pl <span class="se">\</span>
<span class="nt">-i</span> ~/samples.vep.vcf <span class="se">\</span>
<span class="nt">-s</span> <span class="k">case</span>.1 <span class="k">case</span>.2 <span class="k">case</span>.3 <span class="nt">-x</span> <span class="se">\</span>
<span class="nt">-o</span> ~/samples.getFunctionalVariantsVep.vcf

<span class="c">############################################ </span>
<span class="c">#### Get variants. </span>
<span class="c">############################################ </span>
<span class="c">#### Getting functional variants. The -n flag allows</span>
<span class="c">#### selections only when &gt;2 samples</span>
<span class="c">#### have variants in a shared gene. </span>
perl /home/vcfhacks-v0.2.0/getFunctionalVariants.pl <span class="se">\</span>
<span class="nt">-s</span> <span class="k">case</span>.1 <span class="k">case</span>.2 <span class="k">case</span>.3 <span class="se">\</span>
<span class="nt">-i</span> ~/samples.vep.vcf <span class="se">\</span>
<span class="nt">-f</span> <span class="nt">-n</span> 2 <span class="se">\</span>
<span class="nt">-o</span> ~/samples.getFunctionalVariantsVep.SharedGenes.vcf

<span class="c">#### Candidate compound heterozygous.</span>
<span class="c">#### Only variants that are common in ALL -s are considered.</span>
<span class="c">#### Flag -n specifies the number of cases</span>
<span class="c">#### required to return a genotype. </span>
perl /home/vcfhacks-v0.2.0/findBiallelic.pl <span class="se">\</span>
<span class="nt">-i</span> ~/samples.vep.vcf <span class="se">\</span>
<span class="nt">-s</span> <span class="k">case</span>.1 <span class="k">case</span>.2 <span class="k">case</span>.3 <span class="se">\</span>
<span class="nt">-n</span> 1 <span class="se">\</span>
<span class="nt">-o</span> ~/samples.findBiallelic.all.vcf

<span class="c">############################################ </span>
<span class="c">#### Rank, annontate, and simplify </span>
<span class="c">############################################ </span>
perl /home/vcfhacks-v0.2.0/rankOnCaddScore.pl <span class="se">\</span>
<span class="nt">-c</span> /data/shared/cadd/v1.3/<span class="k">*</span>.gz <span class="se">\</span>
<span class="nt">-i</span> ~/samples.getFunctionalVariantsVep.SharedGenes.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/samples.getFunctionalVariantsVep.SharedGenes.cadd.ranked.vcf <span class="se">\</span>
<span class="nt">-progress</span>

perl /home/vcfhacks-v0.2.0/geneAnnotator.pl <span class="se">\</span>
<span class="nt">-d</span> /home/vcfhacks-v0.2.0/data/geneAnnotatorDb <span class="se">\</span>
<span class="nt">-i</span> ~/samples.findBiallelic.all.vcf <span class="se">\</span>
<span class="nt">-o</span> ~/samples.findBiallelic.all.gene.anno

perl /home/vcfhacks-v0.2.0/annovcfToSimple.pl <span class="se">\</span>
<span class="nt">-i</span> ~/samples.findBiallelic.all.gene.anno <span class="se">\</span>
<span class="nt">-vep</span> <span class="nt">-gene_anno</span> <span class="se">\</span>
<span class="nt">-canonical_only</span> <span class="se">\</span>
<span class="nt">-u</span> <span class="nt">-contains_variant</span> <span class="se">\</span>
<span class="nt">-o</span> ~/samples.findBiallelic.all.gene.anno.simple.xlsx

</code></pre></div></div>
</p>

  <h2> - </h2>
  <p><h1 id="websites-for-basic-genetic-variant-information">Websites for basic genetic variant information</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#websites-for-basic-genetic-variant-information" id="markdown-toc-websites-for-basic-genetic-variant-information">Websites for basic genetic variant information</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#sites-and-tools-for-getting-basic-genetic-information" id="markdown-toc-sites-and-tools-for-getting-basic-genetic-information">Sites and tools for getting basic genetic information</a></li>
  <li><a href="#communities-and-learning" id="markdown-toc-communities-and-learning">Communities and learning</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>Identifying pathogenic variants with whole genome and whole exome sequencing is not simple.
Determining the correct filtering method can take some time but it is not the most difficult task.
Validating genetic factors is generally the most time consuming part of this type of research.
Here is a compilation of some of the websites and resources that I use constantly.
I will begin this simply as a list but continue to contribute information on how to use all of these over time.
I use most of the listed resources daily.</p>

<p>There are several steps in assessing if a gene variant is a good candidate to explain a clinical phenotype.
Often a clear story can be made between the genetic mutation and the resulting phenotype.
Other times (usually) a genetic finding (particularly biallelic mutations) seem to have a direct link to the clinical phenotype but it can take  weeks-months to functionally validate such a finding.
 With that in mind, it is good to have some sort of routine way to quickly assess the possible pathogenicity of a mutation by hand.
I will mostly discuss these in the context of rare mutations which are likely to be under selective pressure and occur at very low frequencies in a healthy population.</p>

<h1 id="sites-and-tools-for-getting-basic-genetic-information">Sites and tools for getting basic genetic information</h1>
<p>For assessing rare variants Ensembl and Exac (now gnomAD) are my bread and butter.
I haven’t done it yet but I need to set up a hotkey to open a browser with both of these sites simultaneously.
To demonstrate how I like to use these we could use an example.
Lets say we have NGS results for a patient with immunodeficiency with coding variants in the gene RAG2.
OK, well known gene, important for antibody production as wells as TCR and BCR development.
Looks good so far.
Let’s see if the variants are common SNPs or could they be likely to cause damage if they are not reported (of course this is in your pipeline automatically but it’s good practice and takes less than 60 seconds; valuable if there is a real person affected by your results).</p>

<p>After a long time getting confused about transcripts and coordinates, I now know how important it is to accurately report coordinates so there is no confusion if collaborating or reporting the mutations etc.</p>

<p><a href="http://exac.broadinstitute.org">Exac.org</a></p>

<p>This is, in a nutshell, the exomes of ~ 60,000 individuals which can be used to view how frequently mutations occur in the general population (unfortunately it is mostly just European but there is some global representation). Exac is vital for checking coding variants.
It covers some of the intronic regions (exon intron splice sites) and some of the upstream and downstream regions.
This is typical for anyone who does whole exome sequencing.
We mentioned confusion about transcripts and coordinates, Exac automatically loads the coverage as shown for the canonical transcript.
Stick with this transcript for reporting or at least for your own notes.
When you head over to Ensembl grab the same one in the transcript table.<br />
Update! <a href="http://gnomad.broadinstitute.org/about">The Genome Aggregation Database (gnomAD) is online</a>.<br />
This data set is the combination of “123,136 exomes and 15,496 genomes from unrelated individuals” which has “removed individuals known to be affected by severe pediatric disease, as well as their first-degree relatives.”
This is n extremely exciting resource. If you are familiar with Exac then you will know the value of this expansion into gnomAD.
<a href="https://youtu.be/_uRuFZv4JaU">youtube.com</a><br />
<a href="http://www.ensembl.org/index.html">Ensembl</a><br />
Any good pipeline will have annotation of the details for any coding variants but it can be pretty valuable to go and look at these again by hand.
It doesn’t take long but can end up saving time in the long run.
If you do it often, the first check on Exac take less than 60 seconds.
The next check on Ensemble will only take 2-3 minutes.
In the quick search I plug in the gene name, luckily for my stuff the top hit is always the human gene (sorry Alpaca researchers).</p>

<p>When you get to the gene page first click is always “Show transcript table.”
If you are lucky there is only one coding transcript like for RAG2.
Most of the time there are about 6 transcripts of wildly varying lengths just to confuse matters.
Go for the transcript ID of the canonical transcript which you noted on Exac.
If you do so then life will be easier when you go to check the coordinates. 
On the left hand side in the table “Transcript-based displays” click “cDNA” shown under “Sequence”.
You can then search through to find the variant and amino acid to see if everything lines up.
You see the cDNA position and amino acid positions overlaid. If you were to pick a different transcript then of course the coordinates are likely to be different.
From here I usually go back to the table on the left of the screen to search Exons.
This obviously just lays out the exon sequences in blocks along with useful information.
Only a small segment of the introns are displayed.
If you want reference sequences of multiple types just find the down load sequence button and chose FASTA and decide which type you want to display.
You would likely have the information based on the annotated NGS data but you may want to look at the different transcripts and Ensembl is the best option.</p>

<p>So far (in just a couple of minutes) you could have looked up the allele frequencies, affect of mutation on different transcripts and check that everything that should be reported from the NGS output matches up.
My next step is to check if these variants a already reported.
Everyone has their favourite method, searching PubMed etc.
For my topics OMIM often produces good results and a quick search.<br />
<a href="https://www.omim.org/">Online Mendelian Inheritance in Man</a>
This is a curated database and is generally very good.
Hopefully it continues to grow for a long time into the future.
Depending on how much you already know about your gene it is sometimes helpful to jump straight down to the “Allelic variants” section (if one is present).
You may find a few variants already reported with a similar phenotype being described as your case.
You may find the exact mutations already reported.
If this is the case then it is likely that it would have taken a few minutes longer to find the same cases on one of the other databases.</p>

<p>Whether you have found that there are many mutations reported similar to those that interest you or if you have found nothing reported so far, my next step is always to run through UniProt.<br />
<a href="http://www.uniprot.org">UniProt</a><br />
UniProt is so rich in information that there is no need to expnad on it here.
If you have never used it then just pick your favourite protein and go look it up now.
There is (usually) a combination of nearly everything you need to get a quick overview of a protein.
Gene function, functional domains, known variants, reported knockouts/mutagenesis studies, protein structures, expression, localisation, the list goes on.
Actually, as much as I love PDB, I find that using UniProt is usually quicker to check for available PDB protein structures before actually going to PDB to download from the source.</p>

<p>With these four websites one would likely be able to decide how confident you are about a candidate mutation/s.
At least if you are just looking coding variants.
Assessing non-coding regions is much messier business.
From here on in validation of a mutation can require a widely variable amount of functional work.
One thing is certain however, Sanger sequencing will be needed to confirm your NGS finding.<br />
<a href="https://www.youtube.com/watch?v=3amsDkyiMu8">youtube.com</a><br />
<a href="https://github.com/gantzgraf/autoprimer3/releases/tag/v3.0.2">Autoprimer3</a></p>

<p>Autoprimer3 is an excellent application that you can use to design primers for a gene of interest.
It is super quick for producing primers to be used on genomic DNA for “any UCSC genome and design PCR/sequencing primers to genes or genome coordinates”.
As an example I timed myself to see how long it takes to get a primer list for all exons of the gene RAG2 and a reference sequence from default genomic coordinates on hg38 while avoiding SNPs based on dbsnp142.
It took me 46 seconds to open the application and produce a primer list and reference sequence.
Less than 1% of the time I may have to go and redesign a primer manually because of an awkward sequence or a patient’s DNA may have some uncommon variant at the primer site. 
Depending on which supplier you order oligos from, Sanger sequencing to confirm a variant by found during NGS can be done within 3 days; about 90 seconds to design and order the oligos, a day or 2 until they are delivered,  and a day to PCR and sequence.
The explanation may be a bit long winded here but this app is excellent.
Just give it a try if you do any routine PCR or sequencing for coding variant.
As the name suggests, it is a simple version of Primer3 but super quick.
<a href="https://software.broadinstitute.org/gatk/">Genome Analysis Toolkit: Variant Discovery in High-Throughput Sequencing Data.</a>
GATK most useful to jump straight to: <a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/">Tool Documentation Index</a> Genome hg38 <a href="http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=chr7:142299011,142813287">(TCR region as example)</a>
<a href="https://gpgtools.org">GPGtools</a>
for sending sensitive patient info.
<a href="https://www.gnupg.org">GnuPG</a> is GPL licensed alternative to the PGP suite for sending sensitive patient info.
See also Pretty good privacy for academic data.<br />
Human splice finder <a href="http://www.umd.be/HSF3/HSF.html">http://www.umd.be/HSF3/HSF.html</a><br />
Illumina-Pipeline-V2 (“Version 2 of Illumina pipeline that incorporates <a href="https://github.com/nirav99/Illumina-Pipeline-V2/blob/master/IlluminaPipelineCASAVA1_8.pdf">CASAVA 1.8”)</a><br />
Sequence Manipulation Suite<br />
<a href="http://www.coccidia.icb.usp.br/sms2/index.html">http://www.coccidia.icb.usp.br/sms2/index.html</a><br />
Sequence Ontology <a href="http://www.sequenceontology.org">http://www.sequenceontology.org</a><br />
UCSC Genome Bioinformatics FAQ <a href="https://genome.ucsc.edu/FAQ/FAQformat">https://genome.ucsc.edu/FAQ/FAQformat</a><br />
UCSC Table Browser <a href="https://genome.ucsc.edu/cgi-bin/hgTables">https://genome.ucsc.edu/cgi-bin/hgTables</a><br />
MutScan <a href="https://github.com/OpenGene/MutScan">https://github.com/OpenGene/MutScan</a><br />
Detect and visualise target mutations by scanning FastQ files directly.
Very useful if you are interested in some certain mutations but saves the time it would take to normally through your pipeline. </p>

<h1 id="communities-and-learning">Communities and learning</h1>
<p>No need to reinvent the wheel here. Stephen Turner has a better list of resources than I will produce with his post “Staying Current in Bioinformatics &amp; Genomics: 2017 Edition.” 
http://www.gettinggeneticsdone.com/2017/02/staying-current-in-bioinformatics-genomics-2017.html<br />
Essentially it boils down to the journals, Twitter, some expert blogs, and several genomics communities.
The journals and other sites I like to follow are detailed here.
When all directed into a single feed I think it produces an essential resource for most genetics/bioinformatics scientists.
Literature of Interest - In this post I show the use of Feedly to condense all the litereature that I follow into a single source and allow the option to view by category.</p>

<p>In this post I have started to gather some of the resources I like to use and topics that I find interesting.
Some other links tagged on at the end:<br />
BioStarts - Bioinformatics academic community https://www.biostars.org<br />
Useful bash Bioinformatics one-liners<br />
https://github.com/stephenturner/oneliners<br />
Efficient R programming https://csgillespie.github.io/efficientR/<br />
Cheat sheets for data.   science http://www.datasciencecentral.com/…
RStudio Cheat Sheets<br />
https://www.rstudio.com/resources/cheatsheets/#515</p>
</p>

  <h2> - </h2>
  <p><h1 id="genomic-analysis-tools">Genomic analysis tools</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#genomic-analysis-tools" id="markdown-toc-genomic-analysis-tools">Genomic analysis tools</a></li>
  <li><a href="#command-line-tool" id="markdown-toc-command-line-tool">command line tool</a></li>
  <li><a href="#desktop_applications" id="markdown-toc-desktop_applications">desktop_applications</a></li>
  <li><a href="#websites" id="markdown-toc-websites">Websites</a></li>
</ul>

<h1 id="command-line-tool">command line tool</h1>
<p><a href="https://github.com/virajbdeshpande/AmpliconArchitect">AmpliconArchitect</a> AmpliconArchitect is used to identify circular DNA fragments in genomic data.<br />
<a href="https://anaconda.org">Anaconda2</a> <br />
<a href="https://anaconda.org">Anaconda3</a> <br />
<a href="http://annovar.openbioinformatics.org/">annovar</a> Annotate functional consequences of genetic variation from sequencing data<br />
<a href="https://github.com/cancerit/ascatNgs">ASCAT</a> <br />
<a href="https://github.com/Crick-CancerGenomics/ascat">ASCAT</a> <br />
<a href="https://www.gnu.org/software/autoconf/autoconf.html">Autoconf</a> <br />
<a href="https://www.gnu.org/software/automake/">Automake</a> <br />
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">Autotools</a> <br />
<a href="https://github.com/cancerit/cgpBattenberg">Battenberg</a> <br />
<a href="https://github.com/Wedge-Oxford/battenberg">Battenberg</a> <br />
<a href="https://github.com/andyrimmer/Platypus/blob/master/extensions/DeNovo/bayesianDeNovoFilter.py">bayesianDeNovoFilter</a> <br />
<a href="http://www.htslib.org/doc/bcftools.html">bcftools</a> Utilities for variant calling and manipulating VCFs and BCFs<br />
<a href="http://bedops.readthedocs.io/">bedops</a> Toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data.<br />
<a href="http://bedtools.readthedocs.io/">bedtools</a> Set of tools for a wide-range of genomics analysis tasks.<br />
<a href="NA">bertha</a> <br />
<a href="https://www.gnu.org/software/binutils/">binutils</a> <br />
<a href="https://www.gnu.org/software/bison/">Bison</a> <br />
<a href="ftp://ftp.ncbi.nlm.nih.gov/blast/db/">Blast</a> It searches through non-human sequence looking for bacteria/viruses.<br />
<a href="http://www.bzip.org">bzip2</a> <br />
<a href="http://github.com/Illumina/canvas">canvas</a> A tool for calling copy number variants [CNVs) from human DNA sequencing data<br />
<a href="https://github.com/opencb/cellbase/wiki">cellbase</a> A comprehensive collection of RESTful web services for retrieving relevant biological information from heterogeneous sources<br />
<a href="https://github.com/lindaszabo/KNIFE">Circular RNA analysis</a> <br />
<a href="http://bonsai.hgc.jp/%7Emdehoon/software/cluster/software.htm">cluster</a> <br />
<a href="https://github.com/abyzovlab/CNVnator">CNVnator</a> A tool for CNV discovery and genotyping from depth-of-coverage by mapped reads<br />
<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwia5f_lu93cAhUI3KQKHTZNC1kQFjABegQIAhAB&amp;url=https%3A%2F%2Fdeveloper.nvidia.com%2Fcuda-zone&amp;usg=AOvVaw2J1e7C-ir5D_lz8OgoiUKF">CUDA</a> <br />
<a href="https://developer.nvidia.com/cudnn">cuDNN</a> <br />
<a href="https://curl.haxx.se">curl</a> <br />
<a href="https://github.com/im3sanger/dndscv">dndscv</a> Ability to calculate the non-synonymous to synonymous ratio [dN/dS) in cancer to find genes under negative or positive selection.<br />
<a href="https://github.com/dotnet">dotnet</a> <br />
<a href="NA">Eagle</a> <br />
<a href="https://github.com/easybuilders/easybuild">EasyBuild</a> <br />
<a href="NA">ERDS</a> <br />
<a href="https://sites.google.com/site/bioericscript/home">Eric-script</a> <br />
<a href="https://libexpat.github.io">expat</a> <br />
<a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/">FastQC</a> All the necessary tools for RNA-Seq analysis from raw-sequence read quality assessment, to alignment, transcript quantification, and differential expression. Also it includes analysis for gene-fusion analysis and circular RNA analysis.<br />
<a href="http://www.fftw.org">FFTW</a> <br />
<a href="https://github.com/gabraham/flashpca">flashPCA</a> flashPCA performs fast principal component analysis [PCA) of single nucleotide polymorphism [SNP) data<br />
<a href="xhttps://github.com/westes/flex">fle</a> <br />
<a href="https://www.freedesktop.org/wiki/Software/fontconfig/">fontconfig</a> <br />
<a href="https://www.freetype.org">freetype</a> <br />
<a href="https://software.broadinstitute.org/gatk/">GATK</a> Toolkit with a primary focus on variant discovery and genotyping<br />
<a href="http://gcc.gnu.org">GCC</a> <br />
<a href="https://github.com/easybuilders/easybuild-easyconfigs/tree/master/easybuild/easyconfigs/g/GCCcore">GCCcore</a> <br />
<a href="https://www.gnu.org/software/gettext/">gettext</a> <br />
<a href="https://gmplib.org">GMP</a> <br />
<a href="NA">gompi</a> <br />
<a href="http://www.brown.edu/Research/Istrail_Lab/resources/hapcompass_manual.html">hapcompass</a> A software package for haplotype assembly of diploid, polyploid, and tumor genomes<br />
<a href="https://github.com/vibansal/hapcut/blob/master/README.md">hapcut</a> A max-cut based algorithm for haplotype assembly that uses the mix of sequenced fragments from the two chromosomes of an individual<br />
<a href="https://www.gnu.org/software/help2man/">help2man</a> <br />
<a href="https://ccb.jhu.edu/software/hisat2/index.shtml">HiSAT</a> <br />
<a href="http://www.htslib.org/doc/#manual-pages">htslib</a> A C library for reading/writing high-throughput sequencing data<br />
<a href="https://www.open-mpi.org/projects/hwloc/">hwloc</a> <br />
<a href="http://mathgen.stats.ox.ac.uk/impute/impute_v2.html">impute2</a> IMPUTE version 2 [also known as IMPUTE2) is a genotype imputation and haplotype phasing program based on ideas from Howie et al. 2009<br />
<a href="https://www.ebi.ac.uk/~zerbino/velvet/">install velvet</a> It Assembles unmapped reads into longer contigs, to help identify their source.<br />
<a href="https://java.com/en/download/">java</a> <br />
<a href="https://github.com/comprna/Junckey">Junckey</a> <br />
<a href="https://pachterlab.github.io/kallisto/download.htm">Kallisto</a> <br />
<a href="http://people.virginia.edu/~wc9c/KING/manual.html">king</a> A toolset to explore genotype data from a genome-wide association study and a sequencing project<br />
<a href="https://github.com/davidaknowles/leafcutter">LeafCutter</a> <br />
<a href="http://www.libpng.org/pub/png/libpng.html">libpng</a> <br />
<a href="https://tiswww.case.edu/php/chet/readline/rltop.html">libreadline</a> <br />
<a href="https://www.gnu.org/software/libtool/">libtool</a> <br />
<a href="http://xmlsoft.org">libxml2</a> <br />
<a href="https://genome.ucsc.edu/util.html">LiftOver</a> <br />
<a href="https://github.com/arq5x/lumpy-sv/blob/master/README.md">lumpy</a> A probabilistic framework for structural variant discovery<br />
<a href="https://www.gnu.org/software/m4/m4.html">M4</a> <br />
<a href="https://majiq.biociphers.org/">MAJIQ</a> <br />
<a href="https://getmanta.com">manta</a> <br />
<a href="https://maven.apache.org">maven</a> <br />
<a href="https://bitbucket.org/uwlabmed/msings">msings</a> <br />
<a href="http://multiqc.info/docs/#manual-installation">MultiQC</a> MultiQC is a program that generates reports from the log files of common bioinformatics tools, and would be very good to have in the environment. It provides report creation, information aggregation functionalities.<br />
<a href="https://www.gnu.org/software/ncurses/">ncurses</a> <br />
<a href="https://linux.die.net/man/8/numactl">numactl</a> <br />
<a href="https://www.openblas.net">OpenBLAS</a> <br />
<a href="https://www.open-mpi.org">OpenMPI</a> <br />
<a href="https://pandoc.org/">Pandoc</a> Pandoc is a Haskell library for converting from one markup format to another, and a command-line tool that uses this library.<br />
<a href="https://www.gnu.org/software/parallel/">parallel</a> <br />
<a href="https://www.perl.org">perl</a> <br />
<a href="http://evolution.genetics.washington.edu/phylip.html">phylip</a> <br />
<a href="https://broadinstitute.github.io/picard/">picard</a> <br />
<a href="http://www.pixman.org">pixman</a> <br />
<a href="https://www.freedesktop.org/wiki/Software/pkg-config/">pkg-config</a> <br />
<a href="http://www.well.ox.ac.uk/platypus-doc">Platypus</a> A Haplotype-Based Variant Caller For Next Generation Sequence Data<br />
<a href="http://zzz.bwh.harvard.edu/plink/">PLINK</a> A whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner<br />
<a href="https://www.python.org">python</a> <br />
<a href="https://www.r-project.org">R</a> <br />
<a href="http://rnaseq-mats.sourceforge.net/">rMATS</a> <br />
<a href="https://bioconductor.org/biocLite.R">RMySQL R package</a> <br />
<a href="https://github.com/RealTimeGenomics/rtg-tools">rtg-tools</a> Utilities for accurate VCF comparison and manipulation<br />
<a href="https://www.ruby-lang.org/en/">ruby</a> <br />
<a href="https://github.com/COMBINE-lab/salmon">Salmon</a> For direct transcript quantification from FastQ<br />
<a href="http://lomereiter.github.io/sambamba/index.html">Sambamba</a> Utilities for viewing, manipulating and merging bam files<br />
<a href="https://github.com/GregoryFaust/samblaster">samblaster</a> A tool to mark duplicates and extract discordant and split reads from sam files<br />
<a href="http://www.htslib.org/doc/samtools.html">samtools</a> Utilities for accessing and manipulating sam files<br />
<a href="https://www.osc.edu/resources/available_software/software_list/scalapack">ScaLAPACK</a> <br />
<a href="http://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.html">SHAPEIT2</a> SHAPEIT is a fast and accurate method for estimation of haplotypes [aka phasing) from genotype or sequencing data.<br />
<a href="https://github.com/alexdobin/STAR">STAR</a> <br />
<a href="https://github.com/STAR-Fusion/STAR-Fusion/wiki">STAR-fusion</a> <br />
<a href="https://github.com/comprna/SUPPA">SUPPA</a> <br />
<a href="https://tug.org/texlive/">texlive</a> <br />
<a href="https://github.com/vcflib/vcflib/blob/master/README.md">vcflib</a> A C++ library for parsing and manipulating VCF files<br />
<a href="https://vcftools.github.io/index.html">vcftools</a> A program package designed for working with VCF files<br />
<a href="https://github.com/Ensembl/ensembl-vep">vep</a> Determines the effect of variants [SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions<br />
<a href="https://github.com/statgen/verifyBamID/releases">verifybamid</a> <br />
<a href="http://www.lstmed.ac.uk/vtbuilder">vt</a> A tool for the inference of non-chimeric contigs from read data that has been sequenced from complex multi-isoformic transcriptomes<br />
<a href="NA">weCall</a> <br />
<a href="https://tukaani.org/xz/">xz</a> <br />
<a href="https://www.zlib.net">zlib</a></p>
<h1 id="desktop_applications">desktop_applications</h1>
<p><a href="https://atom.io/packages/atom-ide-ui">atom-ide-ui</a> <br />
<a href="xhttps://www.mozilla.org/en-US/firefox/new/">Firefo</a> <br />
<a href="https://atom.io/packages/ide-python">ide-python</a> requires server https://github.com/palantir/python-language-server<br />
<a href="https://www.libreoffice.org">LibreOffice</a> <br />
<a href="https://www.rstudio.com">RStudio</a></p>
<h1 id="websites">Websites</h1>
<p><a href="www.ncbi.nlm.nih.gov/clinvar/*">ClinVar</a> www.ncbi.nlm.nih.gov/clinvar/<br />
<a href="compbio.charite.de/hpoweb/*">Compbio HPO Browser</a> http://compbio.charite.de/hpoweb/<br />
<a href="www.ncbi.nlm.nih.gov/CCDS/*">Consensus CDS Project</a> www.ncbi.nlm.nih.gov/CCDS/<br />
<a href="cancer.sanger.ac.uk/wgs/*">COSMIC</a> http://cancer.sanger.ac.uk/cosmic/<br />
<a href="www.ncbi.nlm.nih.gov/snp/*">DBsnp</a> www.ncbi.nlm.nih.gov/snp/<br />
<a href="decipher.sanger.ac.uk/*">DECIPHER</a> http://decipher.sanger.ac.uk/<br />
<a href="www.ebi.ac.uk/*">EMBL EBI</a> www.ebi.ac.uk/<br />
<a href="www.ebi.ac.uk/interpro/*">EMBL EBI</a> <br />
<a href="www.ebi.ac.uk/wen_guidelines/*">EMBL EBI</a> <br />
<a href="static.ensembl.org/*">Ensembl</a> <br />
<a href="static.ensembl.org/minified/*">Ensembl</a> <br />
<a href="www.ensembl.org/id/*">Ensembl</a> <br />
<a href="ensembl.org/*">Ensembl Human</a> http://ensembl.org/Homo_sapiens/<br />
<a href="ensembl.org/Homo_sapiens/Gene/*">Ensembl Human</a> <br />
<a href="ensembl.org/Homo_sapiens/*">Ensembl Human</a> <br />
<a href="http://exac.broadinstitute.org">exAC Browser beta</a> http://exac.broadinstitute.org<br />
<a href="ghr.nlm.nih.gov/*">Genetics Home Reference Gene Browser</a> http://ghr.nlm.nih.gov/gene<br />
<a href="ghr.nlm.nih.gov/gene/*">Genetics Home Reference Gene Browser</a> <br />
<a href="ghr.nlm.nih.gov/images/*">Genetics Home Reference Gene Browser</a> <br />
<a href=".incoming01.genomicsplc.com">Genomics Plc Misc.</a> <br />
<a href="http://gnomad.broadinstitute.org">gnomAD Browser beta</a> http://gnomad.broadinstitute.org<br />
<a href="www.gstatic.com/charts/*">Google Misc.</a> <br />
<a href="www.genenames.org/*">HUGO Gene Nomenclature Committee</a> http://www.genenames.org/<br />
<a href="www.genenames.org/data/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/sites/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/css/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.genenames.org/js/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="rest.genenames.org/*">HUGO Gene Nomenclature Committee</a> <br />
<a href="www.human-phenotype-ontology.org/hpoweb/*">Human Phenotype Ontology</a> http://compbio.charite.de/hpoweb/showterm<br />
<a href="www.human-phenotype-ontology.org/*">Human Phenotype Ontology</a> <br />
<a href="human-phenotype-ontology.github.io/*">Human Phenotype Ontology</a> <br />
<a href="igv.broadinstitute.org/*">IGV Browser</a> <br />
<a href="igvdata.broadinstitute.org/*">IGV Browser</a> <br />
<a href="www.ncbi.nlm.nih.gov/portal*">NCBI</a> www.ncbi.nlm.nih.gov/portal<br />
<a href="www.nlm.nih.gov/core/*">NCBI</a> <br />
<a href=".nlm.nih.gov/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/gtr/tests/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/core/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/project/*">NCBI</a> <br />
<a href="www.ncbi.nlm.nih.gov/gene*">NCBI Gene</a> www.ncbi.nlm.nih.gov/gene<br />
<a href="www.ncbi.nlm.nih.gov/nuccore/*">NCBI Nucleotide</a> www.ncbi.nlm.nih.gov/nuccore/<br />
<a href="omim.org/*">OMIM</a> http://omim.org<br />
<a href="omim.org/entry/*">OMIM</a> <br />
<a href="omim.org/static/*">OMIM</a> <br />
<a href="omim.org/search/*">OMIM</a> <br />
<a href="api.europe.omim.org/api/*">OMIM</a> <br />
<a href="https://panelapp.genomicsengland.co.uk/">Panelapp</a> https://panelapp.genomicsengland.co.uk/<br />
<a href="static.pubmed.gov/*">Pubmed</a> www.ncbi.nlm.nih.gov/pubmed/<br />
<a href="www.ncbi.nlm.nih.gov/pubmed/*">Pubmed</a> <br />
<a href="dev-static.pubmed.gov/*">Pubmed</a> <br />
<a href="cancer.sanger.ac.uk/_asset/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/cancergenome/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/javascripts/*">Sanger</a> <br />
<a href="cancer.sanger.ac.uk/cosmic/*">Sanger</a> <br />
[SMART <a href="smart.embl-heidelberg.de/*">a Simple Modular Architecture Research Tool)</a> http://smart.embl-heidelberg.de<br />
<a href="https://www.sib.swiss/*">Swiss Institute of Bioinformatics</a> https://www.sib.swiss<br />
<a href="www.hgmd.cf.ac.uk/ac/*">The Human Gene Mutation Database</a> www.hgmd.cf.ac.uk<br />
<a href="www.hgmd.cf.ac.uk/*">The Human Gene Mutation Database</a> <br />
<a href="www.sequenceontology.org/miso/current_release/term/*">The Sequence Ontology  database</a> www.sequenceontology.org/<br />
<a href="www.sequenceontology.org/css/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/js/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/img/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/browser/*">The Sequence Ontology  database</a> <br />
<a href="www.sequenceontology.org/*">The Sequence Ontology  database</a> <br />
<a href="www.genome.ucsc.edu/*">UCSC Genome Browser</a> <br />
[Xfam <a href="pfam.sanger.ac.uk/family/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a> http://xfam.org<br />
[Xfam <a href="pfam.sanger.ac.uk/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a> <br />
[Xfam <a href="xfam.org/*">Pfam, Rfam, Dfam, Treefam, iPfam, Antifam)</a></p>
</p>

  <h2> - </h2>
  <p><h1 id="genome-wide-assocciation-study">Genome wide assocciation study</h1>

<ul id="markdown-toc">
  <li><a href="#genome-wide-assocciation-study" id="markdown-toc-genome-wide-assocciation-study">Genome wide assocciation study</a></li>
  <li><a href="#abbreviations" id="markdown-toc-abbreviations">Abbreviations</a></li>
  <li><a href="#sample-collection-and-genotyping" id="markdown-toc-sample-collection-and-genotyping">Sample collection and genotyping</a></li>
  <li><a href="#pre-imputation" id="markdown-toc-pre-imputation">Pre-imputation</a></li>
  <li><a href="#imputation" id="markdown-toc-imputation">Imputation</a>    <ul>
      <li><a href="#file-formats" id="markdown-toc-file-formats">File formats</a></li>
      <li><a href="#imputation-services" id="markdown-toc-imputation-services">Imputation services</a></li>
    </ul>
  </li>
  <li><a href="#information-score" id="markdown-toc-information-score">Information score</a></li>
  <li><a href="#get-snptest-summary-with-impute-information-score" id="markdown-toc-get-snptest-summary-with-impute-information-score">Get snptest summary with impute information score¬</a></li>
  <li><a href="#imputation-to-plink-format" id="markdown-toc-imputation-to-plink-format">Imputation to Plink format</a></li>
  <li><a href="#quality-control" id="markdown-toc-quality-control">Quality control</a>    <ul>
      <li><a href="#relatedness" id="markdown-toc-relatedness">Relatedness</a></li>
      <li><a href="#missingness" id="markdown-toc-missingness">Missingness</a></li>
      <li><a href="#genotype" id="markdown-toc-genotype">Genotype</a></li>
      <li><a href="#hwe" id="markdown-toc-hwe">HWE</a></li>
    </ul>
  </li>
  <li><a href="#phenotypes-and-covariates" id="markdown-toc-phenotypes-and-covariates">Phenotypes and covariates</a></li>
  <li><a href="#pca" id="markdown-toc-pca">PCA</a></li>
  <li><a href="#biological-interpretation" id="markdown-toc-biological-interpretation">Biological interpretation</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#command-line-example-code" id="markdown-toc-command-line-example-code">Command line example code</a></li>
</ul>

<h1 id="abbreviations">Abbreviations</h1>
<p>BWA (Burrows-Wheeler transformation aligner), 
GrCh38 (Genome Reference Consortium Human Build 38), 
VCF (variant call format).</p>

<h1 id="sample-collection-and-genotyping">Sample collection and genotyping</h1>
<p>A good paper on the “Basic statistical analysis in genetic case-control studies” by <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3154648/">Clarke et al. 2011</a>.
Genomic control: https://en.wikipedia.org/wiki/Population_structure_(genetics)</p>

<h1 id="pre-imputation">Pre-imputation</h1>
<p>Before imputation with study genotypes, filter the data to remove low-quality variants and samples. 
Standard GWAS quality control filters are usually sufficient to prepare a dataset for imputation. 
It may also help to add an imputation-based QC step to the filtering process.</p>

<p><a href="https://academic.oup.com/bfg/article/15/4/298/2412127">Coleman et al. 2016</a></p>

<h1 id="imputation">Imputation</h1>

<h2 id="file-formats">File formats</h2>
<p>The file format required depends on the method chosen.
VCF file is the most common input type (e.g. Sanger imputation).
Plink format files (bim, bed, fam, or pgen, psam ) can be converted to VCF using Plink.
VCF may be converted to a gen file (e.g. Impute2).</p>

<p><strong>Example</strong>
Plink binary files to VCF.
https://www.cog-genomics.org/plink/1.9</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plink <span class="nt">--bfile</span> binary_fileset <span class="nt">--recode</span> vcf <span class="nt">--out</span> new_vcf
</code></pre></div></div>

<h2 id="imputation-services">Imputation services</h2>
<p>Online: Sanger Imputation server
<a href="https://www.sanger.ac.uk/tool/sanger-imputation-service/">sanger.ac.uk</a>
is a genotype imputation and phasing service provided by the Wellcome Sanger Institute. 
You can upload GWAS data in VCF or 23andMe format and receive imputed and phased genomes back. 
Optional pre-phasing is with EAGLE2 or SHAPEIT2 and imputation is with PBWT into a choice of reference panels including 1000 Genomes Phase 3, UK10K, and the Haplotype Reference Consortium.</p>

<p>Online: Michigan Imputation Server
<a href="https://imputationserver.sph.umich.edu/index.html#!">imputationserver.sph.umich.edu</a>
based on 
<a href="https://pubmed.ncbi.nlm.nih.gov/27571263/">Foreret al. 2016</a>
.
Open source, offers methods to build your own server.
Reference panels: 
HapMap Release 2.
1000 Genomes,
Phase 1 and 3,
CAAPA,
African American,
Haplotype Reference Consortium.</p>

<p>Local software: IMPUTE version 2 (also known as IMPUTE2) is a genotype imputation and haplotype phasing program based on ideas from 
<a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000529">Howie et al. 2009</a>
but has features from additional publications.
From Impute2, the process is summed up as “the most common scenario in which imputation is used: 
unobserved genotypes (red question marks) in a set of study individuals are imputed (or predicted) using a set of reference haplotypes and genotypes from a SNP chip.</p>

<h1 id="information-score">Information score</h1>
<p>Since the imputation non-genotyped variants is based on a reference panel that
may not fully represent the ancestry of a study cohort, a quality score may be 
desired.
One tool is SNPTEST
<a href="https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#info_measures">mathgen.stats.ox.ac.uk</a>.
From mathgen.stats.ox.ac.uk, the IMPUTE info measure reflects the information in imputed genotypes relative to the information if only the allele frequency were known.</p>

<p>[\text{info} = 1 - \text{mean} (
\frac {
\text{ variance in imputed genotype} }
{ \text{variance if only allele frequency were known} }
)]</p>

<p>The numerator of this expression is computed over the imputed genotype distribution for each sample. The denominator is computed using the estimated allele frequency</p>

<p>[\theta = \sum_{i} 
\frac{
P(g_{i}=1)+2P(g_{i}=2)) }
{2\sum_{i,g}P(g_{i}=g) }]</p>

<p>and the assumption of Hardy-Weinberg equilibrium.
The info measure takes the value 1 if all genotypes are completely certain, 
and the value 0 if the genotype probabilities for each sample are completely uncertain in Hardy-Weinberg proportions 
(i.e. they equal
\((1-\theta)^{2}, 2\theta(1-\theta), \theta^{2}).\)
It is also possible for info to drop below zero.</p>

<p>Info is usually computed as if assuming all samples are diploid and that the genotype probabilities for each sample sum to one. This is what IMPUTE computes, and also what SNPTEST computes when you use a method other than newml.
Haploid samples, e.g. for males on the X chromosome, will be treated a little differently.
This can be done with -method newml (maximum likelihood test). 
Discussion is not warrented here but can be read at
<a href="https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#info_measures">snptest measures</a> and
<a href="https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.v2.pdf">snptest.v2.pdf</a>.</p>

<h1 id="get-snptest-summary-with-impute-information-score">Get snptest summary with impute information score¬</h1>
<p>Here is an example of a script that might get this value for all genotype files.
The cohort of 1,000 individuals may have data split into 22 files (one per chromosome),
and each chromosome split into managable sizes of 100,000 SNPs giving maybe 600 individual files in the format:
Chr1.pos10024053-15008865.impute2.
The input -data files will be (1) the genotype gen file and (2) the sample list.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use 22 cores to run in parallel</span>
<span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..22<span class="o">}</span>
<span class="k">do
for </span>file <span class="k">in </span>Chr<span class="nv">$i</span>.<span class="k">*</span>impute2
<span class="k">do</span>
›   ~/tool/snptest_v2.5.4-beta3_<span class="se">\</span>
linux_x86_64_dynamic/snptest_v2.5.4-beta3 <span class="se">\</span>
›   <span class="nt">-data</span> <span class="nv">$file</span> <span class="se">\</span>
›   Samples_IDs <span class="se">\</span>
›   <span class="nt">-filetype</span> gen <span class="se">\</span>
›   <span class="nt">-summary_stats_only</span> <span class="se">\</span>
›   <span class="nt">-o</span> <span class="nv">$file</span><span class="se">\_</span>snptest
<span class="k">done</span> &amp;
<span class="k">done
</span><span class="nb">wait</span>
</code></pre></div></div>
<p>The output might look something like this (with only 4 columns shown - the real output would have much more information).
A threshold will be set to decide what quality will be used for your analysis, 
e.g. &gt;0.7.</p>

<table>
  <thead>
    <tr>
      <th>chr</th>
      <th>rsid</th>
      <th>position</th>
      <th>A1</th>
      <th>A2</th>
      <th>info</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>rs70998334</td>
      <td>10027728</td>
      <td>T</td>
      <td>TA</td>
      <td>0.990047</td>
    </tr>
    <tr>
      <td>1</td>
      <td>rs191096997</td>
      <td>10027884</td>
      <td>A</td>
      <td>G</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<h1 id="imputation-to-plink-format">Imputation to Plink format</h1>
<p>File formats output</p>

<h1 id="quality-control">Quality control</h1>
<h2 id="relatedness">Relatedness</h2>
<h2 id="missingness">Missingness</h2>
<h2 id="genotype">Genotype</h2>
<h2 id="hwe">HWE</h2>

<h1 id="phenotypes-and-covariates">Phenotypes and covariates</h1>

<h1 id="pca">PCA</h1>

<h1 id="biological-interpretation">Biological interpretation</h1>

<h1 id="conclusion">Conclusion</h1>

<h1 id="command-line-example-code">Command line example code</h1>
<p><strong>Whole exome analysis</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

</code></pre></div></div>
</p>

  <h2> - </h2>
  <p><h1 id="pharmacogenomics-for-personal-medicine">Pharmacogenomics for personal medicine</h1>
<p class="meta">26 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#pharmacogenomics-for-personal-medicine" id="markdown-toc-pharmacogenomics-for-personal-medicine">Pharmacogenomics for personal medicine</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#running-a-real-example" id="markdown-toc-running-a-real-example">Running a real example</a></li>
  <li><a href="#comparing-annotated-genetic-data-to-drug-lists" id="markdown-toc-comparing-annotated-genetic-data-to-drug-lists">Comparing annotated genetic data to drug lists</a>    <ul>
      <li><a href="#small-example-check-of-gene-list-versus-drug-gene-list" id="markdown-toc-small-example-check-of-gene-list-versus-drug-gene-list">Small example check of gene list versus drug-gene list</a></li>
    </ul>
  </li>
  <li><a href="#a-real-example-of-merging-genetic-and-pharmacogenomic-data" id="markdown-toc-a-real-example-of-merging-genetic-and-pharmacogenomic-data">A real example of merging genetic and pharmacogenomic data</a></li>
  <li><a href="#annotation" id="markdown-toc-annotation">Annotation</a></li>
  <li><a href="#drug-indication" id="markdown-toc-drug-indication">Drug indication</a></li>
  <li><a href="#optimising-vcf-annotation" id="markdown-toc-optimising-vcf-annotation">Optimising VCF annotation</a></li>
  <li><a href="#how-to-get-coordinates-for-a-gene-list" id="markdown-toc-how-to-get-coordinates-for-a-gene-list">How to get coordinates for a gene list</a></li>
  <li><a href="#extracting-regions-from-a-vcf-using-a-bed-file" id="markdown-toc-extracting-regions-from-a-vcf-using-a-bed-file">Extracting regions from a VCF using a bed file</a></li>
  <li><a href="#unknown-variants" id="markdown-toc-unknown-variants">Unknown variants</a></li>
  <li><a href="#gene-dosage" id="markdown-toc-gene-dosage">Gene dosage</a></li>
  <li><a href="#drug-indication-1" id="markdown-toc-drug-indication-1">Drug indication</a></li>
  <li><a href="#a-large-scale-example-summary" id="markdown-toc-a-large-scale-example-summary">A large scale example summary</a></li>
  <li><a href="#more-questions" id="markdown-toc-more-questions">More questions</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>With the popularisation of commercial genetics services, more and more people are now able to “decode” their genetic data.
Questions that might arise from this information include “do I have potentially disease-causing variants that can be treated with a drug?”, or “am I taking a drug that will be affected by my genetics?”.
To tackle such questions with an example, we use public data in combination with pharmacogenomics.
Outside of genotype data (offered by <a href="https://www.23andme.com">23andMe</a> for example), the most common file type will be VCF:
<a href="https://gatkforums.broadinstitute.org/gatk/discussion/1268/what-is-a-vcf-and-how-should-i-interpret-it">What is a vcf and how should I interpret it?</a>.</p>

<p>Here is a data source with different genetic data files.
<a href="https://my.pgp-hms.org/public_genetic_data">https://my.pgp-hms.org/public_genetic_data</a>.
To check that it works OK, I tried a quick version of this challenge.
I picked the first whole genome VCF file that I saw (hu24385B 2019-04-07.vcf.g_z)
<a href="https://my.pgp-hms.org/profile/hu24385B">https://my.pgp-hms.org/profile/hu24385B</a>. 
The VCF has 3,461,639 variants.
VCF files can contain a large range of information for each variant, however only the first 7 column are strictly neccessary; Chromosome, position, ID, Reference, Alternate, Qulaity, Filter, info. 
<a href="https://gatkforums.broadinstitute.org/gatk/discussion/1268/what-is-a-vcf-and-how-should-i-interpret-it">The details are explained on this GATK forum post.</a>.
Annotation information about the gene name (or related diseases) is often not present when the VCF is generated and only added later.
Therefore the most common input source may be lacking gene symbols.
To get the gene names of a single file, the simplest way was is to upload a VCF (or a part of it) to <a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor</a> to get the gene symbol (and any other information that you wish).
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.45.png" width="80%" /><br />
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.58.png" width="80%" /></p>

<p>To reduce the time and output you can limit the options.
Split the file and run in batches to save time.
Here I tried the first ~1800 variants.</p>

<p><br />
<code class="highlighter-rouge">head -2000 56001801068861_WGZ.snp.vcf &gt; test.vcf</code></p>

<p><br />
And then annotated that extract with <a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor.</a>
The results would be retained by a URL address such as this, for a few days, but will be deleted by the time your read this.<br />
<br />
<a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/Results?db=core;tl=jNYYW5ONeVFYnaMM-5265700">http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/Results?db=core;tl=jNYYW5ONeVFYnaMM-5265700</a> <br />
<br />
<img src="https://dylanlawlessblog.files.wordpress.com/2019/05/screenshot-2019-05-07-at-17.01.10.png" width="80%" /><br />
<br />
Make certain that you use the same reference genome as used on the input data.
The VCF file was made using reference genome GRCh37.
Therefore the Ensembl/VEP website URL should be for that genome build (grch37, not the default GRCh38).</p>

<h1 id="running-a-real-example">Running a real example</h1>
<p>If you would like to try this using a whole genome using the Ensembl web interface you will need to split your VCF into smaller block first.
For routine usage the command-line version of VEP and it’s databases should be installed on run locally.
There are several bioinformatics tools that are commonly used for manipulating genetic file formats such as VCFtools. 
To get a real understanding of the data type, I inlcude here a method using command line bash to split a VCF file into smaller blocks. 
A bash script is printed below where I use very mainstream traditional command-line tools to wrangle data, including 
<a href="https://en.wikipedia.org/wiki/Gzip">gunzip</a>
to unzip compressed files, 
<a href="https://en.wikipedia.org/wiki/wc_(Unix)">wc</a>
to could lines, 
<a href="https://en.wikipedia.org/wiki/Cat_(Unix)">cat</a>
to print a file, 
<a href="https://en.wikipedia.org/wiki/Head_(Unix)">head</a>
to read the top of a file, 
<a href="https://en.wikipedia.org/wiki/Sed">sed</a>
to edit lines, 
<a href="https://en.wikipedia.org/wiki/AWK">awk</a>
for data extraction, and
<a href="https://en.wikipedia.org/wiki/Grep">grep</a>
which is not used here but it fits well with these other tools - for text search.
Creating a file containing the code below and ending with the filename extension with “.sh” will allow it to be run by your terminal, in this case using the bash language.
I encourage you to read each line and figure out what should happen. If it makes sense then it is reasonable to swap such a manual method with a more efficient specialised tool.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># VEP accept files of &lt;50MB size.</span>
<span class="c"># We will split our large VCF into smaller files.</span>
<span class="c"># Each file requires the same original </span>
<span class="c"># headers and file extension ".vcf"</span>

<span class="c"># Unzip the VCF.gz</span>
<span class="nb">gunzip </span>56001801068861_WGZ.snp.vcf.gz

<span class="c"># Count the number of lines in vcf</span>
<span class="nb">wc</span> <span class="nt">-l</span> 56001801068861_WGZ.snp.vcf

<span class="c"># How should a vcf file look?</span>
<span class="c"># See the links posted in this tutorial above</span>

<span class="c"># Take a look at the header</span>
<span class="c"># This VCF has 140 lines of header metadata (beginning with "#")</span>
<span class="c"># Line 141 shows the column headers: CHROM	POS	ID	REF	ALT...</span>
<span class="c"># Line 142 starts with the first variant</span>
<span class="nb">head</span> <span class="nt">-142</span> 56001801068861_WGZ.snp.vcf

<span class="c"># Print the header to a new file for later</span>
<span class="nb">head</span> <span class="nt">-141</span> 56001801068861_WGZ.snp.vcf <span class="o">&gt;</span> header
<span class="c"># Print everything else (the body) to </span>
<span class="c"># a new file that we will then split.</span>
<span class="nb">sed</span> <span class="s1">'1,141d'</span> 56001801068861_WGZ.snp.vcf <span class="o">&gt;</span> body.vcf

<span class="c"># Make a new directory for the next step</span>
<span class="nb">mkdir </span>split_files
<span class="c"># Move the large file inside</span>
<span class="nb">mv </span>body.vcf split_files/
<span class="nb">cd </span>split_files
<span class="c"># Now split the body.vcf into smaller </span>
<span class="c"># files of 200,000 lines each</span>
<span class="nb">split</span> <span class="nt">-l</span> 150000 body.vcf

<span class="c"># You will now how ~10 files "xaa, xab, etc."</span>
<span class="c"># Add the header back onto all of these files to make them VCFs again.</span>
<span class="c"># This "for loop" will do the following for each file:</span>
<span class="c"># Print the header and the vcf body to </span>
<span class="c"># a file with the same name, </span>
<span class="c"># adding a file extension ".vcf".</span>
<span class="c"># Then remove the vcf body file that </span>
<span class="c"># does not have the ".vcf" extension</span>
<span class="c"># leaving you with the original whole genome VCF split</span>
<span class="c"># into smaller files, each with the same headers.</span>

<span class="k">for </span>file <span class="k">in</span> ./x<span class="k">*</span> <span class="p">;</span> 
    <span class="k">do </span><span class="nb">cat</span> ../header <span class="nv">$file</span> <span class="o">&gt;&gt;</span> <span class="nv">$file</span>.vcf <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nv">$file</span> <span class="p">;</span>
<span class="k">done</span>

<span class="c"># These should be small enough to run on VEP online.</span>
<span class="c"># You could edit the split command to make a </span>
<span class="c"># reasonable number of files, </span>
<span class="c"># uploading &gt;10 is not efficient.</span>

</code></pre></div></div>

<h1 id="comparing-annotated-genetic-data-to-drug-lists">Comparing annotated genetic data to drug lists</h1>
<p>Uploading either a small example or your split-whole-genome example, VEP will process the data and annotate it will whatever features you require, including gene names, variant consequence, pahtnogenicy prediction, etc.
The next aim will be to see if any of your output genes are also present in your drug-gene database.
The method will require merging both dataset (gene and drug datasets) based on shared features. 
A simple sanity test would be useful first:</p>

<h2 id="small-example-check-of-gene-list-versus-drug-gene-list">Small example check of gene list versus drug-gene list</h2>
<p>From the VEP output I extracted the gene symbols column and compared against a list of druggable target genes from 
<a href="https://www.drugbank.ca">DrugBank</a>
(because I happen to have their data on hand, FDA may be more reliable).
I used another command-line method to do this efficienctly:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cut</span> <span class="nt">-f1</span> <span class="nt">-d</span> <span class="s2">","</span> vep_output_file.csv | <span class="nb">uniq</span> <span class="o">&gt;</span> unique.genes.txt
</code></pre></div></div>
<ul>
  <li>Cut column 1 (f1)</li>
  <li>with a delimiter comma (,)</li>
  <li>from the vep output csv file (or tsv, or text file)</li>
  <li>then pipe (|) that result into another program (sort) to sort the result in alphabetic order</li>
  <li>pipe (|) again this result into(uniq)</li>
  <li>so that only one unique gene name is output</li>
  <li>then (&gt;) write the output into the new file “unique.genes.txt”.<br />
<br />
Repeat the same type of method on the DrugBank dataset to get a list of the gene names contained in DrugBank into a file called “unique.druggable.txt”
The next command will then compare both lists.
<br />
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sort </span>unique.genes.txt unique.druggable.txt | <span class="nb">uniq</span> <span class="nt">-c</span> <span class="nt">-i</span> | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'1 '</span>
</code></pre></div>    </div>
    <p>This command also used “uniq -c” to count how many times each gen name occurs and then “grep -v ‘1 ‘” meaning ignore genes that are only present 1 time. 
We want the genes that are present twice, once in each list.
The genes which were in present in both the variant list and DrugBank list are:<br />
From a 2,000 line VCF file = 
<a href="https://www.drugbank.ca/bio_entities/BE0003599">GABRD</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0004895">PRKCZ</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0000495">SCNN1D</a><br />
From a 10,000 VCF line file = 
<a href="https://www.drugbank.ca/bio_entities/BE0003599">GABRD</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0004895">PRKCZ</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0000495">SCNN1D</a>,
<a href="https://www.drugbank.ca/bio_entities/BE0008994">TP73</a>.</p>
  </li>
</ul>

<h1 id="a-real-example-of-merging-genetic-and-pharmacogenomic-data">A real example of merging genetic and pharmacogenomic data</h1>
<p>Now that a small example has shown us the logic of the process, we can try a more complex real-world example. 
The following R language script is used to merge the VEP annotated VCF file with a DrugBank database based on the gene names that are common to both datasets.
Read each line and try to understand the process. 
The are many alternative ways to do the same thing in different programming languages. 
This example is done using R. 
I recommending installing R and then installing R studio to edit and run your commands.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comment lines are ignored because of "#" symbol.</span><span class="w">
</span><span class="c1"># Command lines are run by clicking "Run" or "command+enter" on Mac</span><span class="w">

</span><span class="c1"># csv = comma sep file</span><span class="w">
</span><span class="c1"># tsv = tab spaced</span><span class="w">
</span><span class="c1"># txt = white space</span><span class="w">

</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># To do</span><span class="w">
</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># Files: vepfile, drugs</span><span class="w">
</span><span class="c1"># merge based on Gene symbol</span><span class="w">

</span><span class="c1">#///////////////////////////////</span><span class="w">
</span><span class="c1"># import the VEP file</span><span class="w">
</span><span class="c1">#///////////////////////////////</span><span class="w">

</span><span class="c1"># Important note:</span><span class="w">
</span><span class="c1"># The VEP file will start with a header line </span><span class="w">
</span><span class="c1"># that begins with "#" symbol. But this is being ignored by R.</span><span class="w">
</span><span class="c1"># Open the file with text edit and remove that symbol.</span><span class="w">
</span><span class="c1"># Maybe there is an R command to do this on import, whatever is faster.</span><span class="w">

</span><span class="c1"># Split vcf into ~ 10 files. </span><span class="w">
</span><span class="c1"># file 1 "xaa" was anylised on VEP, </span><span class="w">
</span><span class="c1"># download the TXT version, </span><span class="w">
</span><span class="c1"># or unzip my provided example one - cfKJCLRm0eKXsaEG.txt.zip</span><span class="w">
</span><span class="c1"># https://grch37.ensembl.org/Homo_sapiens/Tools/VEP/</span><span class="w">

</span><span class="c1"># Import the VEP output</span><span class="w">
</span><span class="n">vepfile</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="w">
  </span><span class="n">file</span><span class="o">=</span><span class="s2">"cfKJCLRm0eKXsaEG.txt"</span><span class="p">,</span><span class="w">
  </span><span class="n">na.strings</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="s2">"NA"</span><span class="p">),</span><span class="w">
  </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w">
  </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Import drugbank table</span><span class="w">
</span><span class="c1"># the "fill=TRUE" is needed because not all </span><span class="w">
</span><span class="c1"># file lines have the same number of elements.</span><span class="w">
</span><span class="n">drugs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="w">
  </span><span class="n">file</span><span class="o">=</span><span class="s2">"all.txt"</span><span class="p">,</span><span class="w">
  </span><span class="n">na.strings</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="s2">"NA"</span><span class="p">),</span><span class="w">
  </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w">
  </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w">
  </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w"> </span><span class="p">)</span><span class="w">


</span><span class="c1"># We can merge these two files based on 1 common column.</span><span class="w">
</span><span class="c1"># However, the gene name column does not have the same name.</span><span class="w">
</span><span class="c1"># One of them needs to be renamed:</span><span class="w">
</span><span class="c1"># vepfile="SYMBOL"</span><span class="w">
</span><span class="c1"># drugs="Gene.Name"</span><span class="w">

</span><span class="c1"># colnames(df)[colnames(df) == 'oldName'] &lt;- 'newName'</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">vepfile</span><span class="p">)[</span><span class="n">colnames</span><span class="p">(</span><span class="n">vepfile</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"SYMBOL"</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"Gene.Name"</span><span class="w">

</span><span class="c1"># Merge keeping only matches</span><span class="w">
</span><span class="n">merged</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merge</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vepfile</span><span class="p">,</span><span class="w">
                </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">drugs</span><span class="p">,</span><span class="w">
                </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gene.Name"</span><span class="p">,</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">


</span><span class="c1"># Remove empty data "NA"</span><span class="w">
</span><span class="c1"># Install packeges once (comment out then)</span><span class="w">
</span><span class="c1"># Load library each time to use "%&gt;%" (command join) and filtering</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"tidyr"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"dplyr"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">df1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merged</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">drop_na</span><span class="p">(</span><span class="n">Drug.IDs</span><span class="p">)</span><span class="w">

</span><span class="c1"># Make a list of benign variant types that should be removed</span><span class="w">
</span><span class="n">filter_out</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> 
    </span><span class="s1">'synonymous_variant|UTR|NMD_transcript\
	|non_coding|downstream|upstream\
	|intron|mature_miRNA_variant'</span><span class="w">

</span><span class="c1"># Then filter out anything matching these terms.</span><span class="w">
</span><span class="n">df2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df1</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">filter_all</span><span class="p">(</span><span class="n">all_vars</span><span class="p">(</span><span class="o">!</span><span class="n">grepl</span><span class="p">(</span><span class="n">filter_out</span><span class="p">,</span><span class="n">.</span><span class="p">)))</span><span class="w">

</span><span class="c1"># Save an output tsv file for Excel, etc.</span><span class="w">
</span><span class="n">write.table</span><span class="p">((</span><span class="n">df2</span><span class="p">),</span><span class="w"> </span><span class="n">file</span><span class="o">=</span><span class="s1">'./output.tsv'</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="w">
</span><span class="n">quote</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>If you complete this process the output will contain a perfectly merged dataset.
So in this simple example it takes just 5 minutes to get from a real genome VCF to possibly druggable target genes (see further note on <em>drug indication</em> below).
The difficulty lies downstream in interpreting which variants can have an effect that would justify the use of the drug.
Anyone implementing a usable version of this method will incur several obstacles;
e.g. are non-coding or synonymous variants worth reporting?, genes have multiple transcripts which means one variant can be both coding and non-coding depending on transcript splicing, etc.
Other sources of sequence data, including the sequences of Watson and Venter;<br />
<a href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/">http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/</a><br />
23andMe open snp data; <a href="https://opensnp.org/genotypes">https://opensnp.org/genotypes</a>.
There are many layers to a this problem to create a usable product.
For example, how to integrate pharmacodynamics, covariats to drug response, contraindications, variant pathogenicity, etc.
However, this is a good start as a learning experience.</p>

<h1 id="annotation">Annotation</h1>
<p><a href="http://grch37.ensembl.org/Homo_sapiens/Tools/VEP/">Variant Effect Predictor</a> is very useful.
Note: For a real product, the code can run offline (a perl program with a few local library dependencies).
The databases/cache that it uses are a bit too large to include on in a user software.
In the real world you would have to send <em>anonymised</em> packets from the user via an API for accessing the genomic databases hosted on your servers.
Make sure to check their license to see if you can use oftware and databases in a commercial product.<br />
<a href="http://www.ensembl.org/info/about/legal/code_licence.html">http://www.ensembl.org/info/about/legal/code_licence.html</a></p>

<h1 id="drug-indication">Drug indication</h1>
<p>My example used <a href="https://www.drugbank.ca">DrugBank</a> for pharmacogenomic information.
However, it may be safest to use the <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">FDA information</a> as the primary source, but including <a href="https://www.drugbank.ca">DrugBank</a> info is no problem.
Drugs might be either a treatment for a genetic determinant, or a warning for drug usage in someone who also has a genetic variation that might effect their treatment.
The “Labelling Section” listed by FDA might offer the best information.</p>

<p><a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">https://www.fda.gov/drugs/</a>
For example, if we go and check the Prescribing Information PDF to compare two drugs we see that</p>

<p><br />
<strong>(1)</strong> One is used to directly block a gene product,<br />
<strong>(2)</strong> The second warns about use with certain genetic complications.</p>

<p><br />
<strong>Drug 1</strong>: <a href="https://www.accessdata.fda.gov/scripts/cder/daf/index.cfm?event=overview.process&amp;varApplNo=761034">Atezolizumab</a> (1),<br />
<strong>Gene</strong>: <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling"><em>CD274</em></a> <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">(PD-L1)</a><br />
<strong>Labeling</strong>: Indications and Usage<br />
<strong>PRESCRIBING</strong> <strong>INFORMATION</strong>: TECENTRIQ (Atezolizumab) is a programmed death-ligand 1 (PD-L1) blocking antibody indicated for the treatment of patients with…
<a href="https://www.accessdata.fda.gov/drugsatfda_docs/label/2016/761034Orig1s000lbl.pdf">linked PDF</a>.<br />
<strong>Explained</strong>: Genetic disorder and the drug to treat it, exactly what you want.</p>

<p><br />
<strong>Drug 2</strong>: Avatrombopag (3)<br />
<strong>Gene</strong>: <a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling"><em>PROC</em></a><br />
<strong>Labeling</strong>: Warnings and Precautions<br />
<strong>PRESCRIBING INFORMATION</strong>: Thrombotic/Thromboembolic Complications: DOPTELET is a thrombopoietin (TPO) receptor agonist… Monitor platelet counts and for thromboembolic events
<a href="https://www.accessdata.fda.gov/drugsatfda_docs/label/2019/210238s002lbl.pdf">linked PDF</a>.<br />
<strong>Explained</strong>: Atezolizumab is used to treat thrombocytopenia (low levels of thrombocytes).<br />
You <em>do not want to give</em> this to someone who has <a href="https://omim.org/entry/176860?search=proc&amp;highlight=proc"><em>PROC</em></a><a href="https://omim.org/entry/176860?search=proc&amp;highlight=proc"> deficiency</a>;
their disease is <a href="https://en.wikipedia.org/wiki/Thrombophilia">Thrombophilia</a> (hypercoagulability, or <a href="https://en.wikipedia.org/wiki/Thrombosis">thrombosis</a>).
With this in mind, perhaps an application doing this job could work two ways.
(1) If someone has a genetic disorder, the drug, gene, and Indicated usage appears.
(2) If someone is prescribed a drug a suggestion appears to check their genetics, with a link to the gene and Warnings and Precautions.</p>

<h1 id="optimising-vcf-annotation">Optimising VCF annotation</h1>
<p>The slowest part of the method is VCF annotation.
You can significantly increase the speed by first reducing the input to contain only regions of interst.
That is, prepare a list of coordinates for each gene, and select for those regions in your input VCF or genotype data before annotation (VEP).</p>

<h1 id="how-to-get-coordinates-for-a-gene-list">How to get coordinates for a gene list</h1>
<p>Use Biomart.
Their main server was down when I tried, so I went via Ensembl, data access section:<br />
<a href="http://www.ensembl.org/info/data/biomart/index.html">http://www.ensembl.org/info/data/biomart/index.html</a><br />
Then to use the BioMart data mining tool<br />
<a href="http://www.ensembl.org/biomart/martview/28fdaf82da6c02dc5892f99b757e2c44">http://www.ensembl.org/biomart/martview/</a><br />
I actually needed the positions using GRCh37 (rather than 38), so I switched to the old Ensembl using<br />
<a href="http://www.ensembl.org/info/website/tutorials/grch37.html">http://www.ensembl.org/info/website/tutorials/grch37.html</a><br />
to get to <a href="http://grch37.ensembl.org/index.html">http://grch37.ensembl.org/index.html</a> 
then the Biomart section<br />
<a href="http://grch37.ensembl.org/biomart/martview/04f009257dadbafbe595155ba910eb5e">http://grch37.ensembl.org/biomart/martview/</a></p>

<p>Choose DataBase: Genes 93 Dataset: Human Filter -&gt; Gene -&gt; Input external ref ID list -&gt; (change dropdown) Gene
Name paste your list.
e.g. VPS45 PSMB8 BLNK NEFL NLRP7 SMAD4 PSMB9<br />
To set the output type: Attributes -&gt; Gene -&gt; select “gene start”, “gene stop”, “gene name”, or anything extra.
Select the “Results” button at the top and export.
The results can be tsv or csv.
You would have to figure out how to extract the regions from the vcf (sed, grep, awk, R code, etc.).
When I needed this, I used my own tools which required converting to format like this “X:1-2000”, and ordered by number and alphabetic (some positions in the reference genome were patches added later and have an alphanumeric instead of the normal chromosome).
If you use this list to extract regions from a VCF, remember to include all the original VCF header information.</p>

<h1 id="extracting-regions-from-a-vcf-using-a-bed-file">Extracting regions from a VCF using a bed file</h1>
<p>The early part of this tutorial shows how old-school command line tools can be used to extract data. 
Indeed, this may be computationally most efficient but there are some specialised tools that make the process easier in general.
You can use VCFtools to extract specified regions.<br />
<a href="https://vcftools.github.io/man_latest.html">https://vcftools.github.io/man_latest.html</a><br />
You could use a list of defined genome position to reduce the size of your dataset. 
The defined genomic coordinates are generally supplied in a file format called the (BED file](https://en.wikipedia.org/wiki/BED_(file_format))
Note that sometimes the bed file “chrom” ID - the name of the chromosome (e.g. chr3) does not match if the VCF file uses “3” instead of “chr3”.
You might need to edit the bed.
My bed file was like this:<br />
(tab spaced), ref.bed<br />
<br />
chrom    chromStart    chromEnd<br />
1    3549    13555<br />
<br />
And this command ran OK for me to give “output_prefix.recode.vcf”<br /></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$:</span>~/tools/vcftools_0.1.13/bin <span class="se">\</span>
./vcftools <span class="se">\</span>
<span class="nt">--vcf</span> ~/input.vcf <span class="se">\</span>
<span class="nt">--bed</span> ~/ref.bed <span class="se">\</span>
<span class="nt">--out</span> output_prefix <span class="se">\</span>
<span class="nt">--recode</span> <span class="nt">--keep-INFO-all</span>
</code></pre></div></div>
<p>This new VCF will now only contain gene regions that are potentially “druggable”, or at least included on the FDA list.
VCF annotation will be <em>much faster</em> than annotation of the whole genome.</p>

<h1 id="unknown-variants">Unknown variants</h1>
<p>In the majority of situations you will be stuck with <em>variants of unknown significance</em>.
In the absence of tailored analysis and interpretation of each invidual variant, one must often rank unknown variants based on a predicted pathogenicity.
Carefully consider that predictions can be completed wrong and address how such an annotation should be presented. 
One can rank unknown variants based on PHRED-scaled CADD score, highest being more predicted pathogenic.
https://cadd.gs.washington.edu/info<br />
<a href="http://genetics.bwh.harvard.edu/pph2/">Polyphen</a> gives a predicted outcome label and a probability score 0-1 from benign to probably damaging.
See what other pathogenicity prediction tools you can find and estimate how widespread/accepted their usage is.</p>

<h1 id="gene-dosage">Gene dosage</h1>
<p>An important cosideration of variant effect depends on gene dosage.
A <a href="https://en.wikipedia.org/wiki/Dominance_(genetics)">dominant gene</a> may be affected by a single heterozgous variant while a recessive gene may be able to compensate against the negative effect of a heterozyous variant due the presence of a second functional gene copy.
Therefore, the presence of heterozygous or homozygous allele is an important consideration.
Some genes may be sensitive to a <a href="https://en.wikipedia.org/wiki/Zygosity">hemizygous</a> effect, low frequency <a href="https://en.wikipedia.org/wiki/Somatic_(biology)">somatic variants</a>,
<a href="https://en.wikipedia.org/wiki/Mosaic_(genetics)">mosaisism</a>, etc.
<a href="https://en.wikipedia.org/wiki/SNV_calling_from_NGS_data">SNV calling in NGS</a> is a broad topic, but it is safe to say that at least the allele dosage (generally heterozygous or homozygous) should be included in result summary.
If possible, when a gene is linked to a specific disease then the <a href="https://en.wikipedia.org/wiki/Heredity">inheritance type</a> associated with that gene-disease should also be included. 
For example, <a href="https://www.omim.org">https://www.omim.org</a> is a good place to see examles.
The genetic disease <a href="https://www.omim.org/entry/219700?search=cystic%20fibrosis&amp;highlight=cystic%20fibrosi">cystic fibrosis</a> is shown with an inheritance type AR (autosomal recessive) meaning that damaging variants on both gene alleles are required to cause disease. 
The gene <em>cftr</em>, which is the genetic determinant of cystic fibrosis, also has an <a href="https://www.omim.org/entry/602421?search=cftr&amp;highlight=cftr">OMIM page <em>cftr</em></a> that also lists AR inheritance.
An excellent resource for matching gene to disease is the <a href="https://panelapp.genomicsengland.co.uk">https://panelapp.genomicsengland.co.uk</a>.
Individual genes can be explored, or “panels” of disease-specific gene lists can be explored. For example, here is the “<a href="https://panelapp.genomicsengland.co.uk/panels/545/">Bleeding and platelet disorders</a>” panel. 
This shows the “Mode of inheritance” and colour-coded confidence in the disease-gene relationship.
Integrating this type of expert-curated open datasets can be extremely useful.</p>

<h1 id="drug-indication-1">Drug indication</h1>
<p>The indication or warning can be difficult to automate.
For the example drug<br />
<a href="https://www.drugbank.ca/drugs/DB11595">https://www.drugbank.ca/drugs/DB11595</a><br />
the section “Pharmacology” “Indication” has the Indication info.<br />
The FDA label is contained as a PDF attachment in the section “REFERENCES” FDA label Download (245 KB).
If I had to automate the process I would add a URL link for each drug:<br />
for gene name CD274<br />
the drugbank column Drug IDs has these:<br />
DB11595; DB11714; DB11945<br />
and for each ID you could append the ID onto the drugbank URL to link to the webpage
<a href="https://www.drugbank.ca/drugs/">https://www.drugbank.ca/drugs/</a>.
You can do this in R with some technical how-to reading, or do it manually for a quick example like this and removing space to create a web URL.<br />
URL				Drug IDs<br />
https://www.drugbank.ca/drugs/	DB00303<br />
https://www.drugbank.ca/drugs/	DB00114<br />
https://www.drugbank.ca/drugs/	DB00142<br />
https://www.drugbank.ca/drugs/	DB01839<br />
https://www.drugbank.ca/drugs/	DB00125</p>

<h1 id="a-large-scale-example-summary">A large scale example summary</h1>
<p>I do not suggest this for a small project, but if I was to automate subsection requests for real:</p>
<ul>
  <li>[1] Download the whole database (probably a big table sized &gt;100MB) and</li>
  <li>[2] For every query (the Drug ID) extract the sections of interest (indication,  Biologic Classification, Description,  FDA label, etc.)</li>
  <li>[3] Display each section as additional columns in candidate genes table.<br />
<br /></li>
  <li>[1] Would be here: <a href="https://www.drugbank.ca/releases/latest">https://www.drugbank.ca/releases/latest</a></li>
  <li>[2] Would be like this: <a href="https://www.w3schools.com/xml/default.asp">https://www.w3schools.com/xml/default.asp</a><br />
Look at example 2, your database request might be something like:
[get food name = Belgian Waffles, description] or
[get drug ID = DB11595, indication,  Biologic Classification, Description,  FDA label.]
The database request problem can be tricky to optimise but not especially difficult with some experience in SQL-type management.</li>
  <li>[3] For every line in the gene candidate table, do this query request and output the result into the same row.<br />
The final table would be something that includes colunm headers like:<br />
Gene, consequence, variant, amino acid, genome position, CADD, DrugBank ID, Description, Indication, FDA label PDF link, etc.
This table could be ranked based on consequence, CADD score.
The top couple of rows then might be converted into a more readable format like a PDF.</li>
</ul>

<h1 id="more-questions">More questions</h1>

<p>Q: Do we have to infer that in the future everybody will have his genome sequenced ?<br />
You do not have to assume this. It may become true. There could be privacy concerns or social problems arising from genetic prejudice, etc.</p>

<p>Q: Before using our algorithm, patient will have to sequence a part of their genome and thus this is a weakness of our algorithm?<br />
Your tool will provide a service based on genetics.<br />
Option [1] One has their genetics already and will use it for personal medicine.<br />
Option [2] they are prescribed a drug and want to only sequence the genes of interest that could affect this drug.<br />
Option [3] they do not want any genetic info and therefor your product is irrelevant.<br />
Option [4] they do not want their personal genetics, but are willing to estimate their relatedness to others in a genetic database and therefore calculate a probability of accuracy for this drug-gene information. e.g. both parents are Swiss and therefore based on the population they have probability of X that their genotype is Y.</p>

<p>Q: Is it realistic to assume that it will be feasible based on the fact that the sequencing cost is decreasing ?<br />
Irrelevant in this case, but yes, whole genome seq is sometimes &lt; 200CHF will likely be common soon.</p>

<p>Q:  We plan to use polyphen and/or sift in order to discriminate between those types of variants. Is that a good idea?<br />
That is a good start. CADD score is also pretty well known among physicians.
In my opinion, I do not trust the scores often.
However, it is common that for processing a large amount of data, such prediction tools are useful in general.
For example, I might [1] rank first on VEP variant “consequences”; stop mutations with most importance.
[2] Then rank secondly with these values since you cannot interpret most with consequence = missense variant.</p>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://www.fda.gov/drugs/science-research-drugs/table-pharmacogenomic-biomarkers-drug-labeling">https://www.fda.gov/drugs/science-research-drugs/</a></li>
  <li><a href="https://www.pharmgkb.org/view/drug-labels.do">https://www.pharmgkb.org/view/drug-labels.do</a></li>
  <li>Mary V. Relling &amp; William E. Evans. Pharmacogenomics in the clinic. <em>Nature</em> 2015; 526, 343–350. doi: 10.1038/nature15817</li>
  <li>Yip VL, Hawcutt DB, Pirmohamed M. Pharmacogenetic Markers of Drug Efficacy and Toxicity. <em>Clin Pharmacol Ther.</em> 2015;98(1):61-70. doi: 10.1002/cpt.135.</li>
  <li>David R. Adams, M.D., Ph.D.,  and Christine M. Eng, M.D. Next-Generation Sequencing to Diagnose Suspected Genetic Disorders N Engl J Med Oct 2018 doi: 10.1056/NEJMra1711801</li>
</ul>

</p>

  <h2> - </h2>
  <p><h1 id="pca">PCA</h1>
<ul id="markdown-toc">
  <li><a href="#pca" id="markdown-toc-pca">PCA</a></li>
</ul>

<p><br />
We will look at principal component analysis (PCA) and singular value decomposition (SVD).</p>

<p>\documentclass{article}
\usepackage[utf8]{inputenc}</p>

<p>\title{An Introduction to Second-Generation p-Values}
\author{dylan.lawless }
\date{July 2020}</p>

<p>\begin{document}
\maketitle</p>

<p>\noindent Jeffrey D. Blume, Robert A. Greevy, Valerie F. Welty, Jeffrey R. Smith \&amp; William D. Dupont. The American Statistician. 20 Mar 2019 \ https://doi.org/10.1080/00031305.2018.1537893 <br />
https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537893</p>

<p>\section*{Terms}
Second-generation p-value (SGPV) <br />
False discovery rates (FDRs)</p>

<p>\section{Introduction}
A little history of the trouble with p-values and attempts to improve.</p>

<p>\section{Evidential Metrics}
The author suggests what pieces of evidence are required to make a metric:\</p>
<ol>
  <li>A numerical assessment of the strength of evidence in a given body of observations\</li>
  <li>The probability that the numerical assessment will be misleading in a given setting\</li>
  <li>The probability that an observed assessment—one computed from observed data—is mistaken</li>
</ol>

<p>\section{3}</p>

<p>\section{4}
\section{5}
\section{6}
\section{7}</p>

<p>\end{document}</p>

</p>

  <h2> - </h2>
  <p><h1 id="fischers-exact-test">Fischer’s exact test</h1>
<ul id="markdown-toc">
  <li><a href="#fischers-exact-test" id="markdown-toc-fischers-exact-test">Fischer’s exact test</a></li>
</ul>

<p><br />
https://www.statology.org/fishers-exact-test/
Statology page</p>

<p>Fisher’s Exact Test is used to determine whether or not there is a significant association between two categorical variables. It is typically used as an alternative to the Chi-Square Test of Independence when one or more of the cell counts in a 2x2 table is less than 5.</p>

<p>Fisher’s Exact Test uses the following null and alternative hypotheses:</p>

<p>H0: (null hypothesis) The two variables are independent.
H1: (alternative hypothesis) The two variables are not independent.</p>

<p>Suppose we have the following 2x2 table:</p>

<p>Group 1	Group 2	Row Total
Category 1	a	b	a+b
Category 2	c	d	c+d
Column Total	a+c	b+d	a+b+c+d = n</p>

<p>The one-tailed p value for Fisher’s Exact Test is calculated as:</p>

<p>p = (a+b)!(c+d)!(a+c)!(b+d)! / (a!b!c!d!n!)</p>

<p>This produces the same p value as the CDF of the hypergeometric distribution with the following parameters:</p>

<p>population size = n
population “successes” = a+b
sample size = a + c
sample “successes” = a</p>

<p>The two-tailed p value for Fisher’s Exact Test is less straightforward to calculate and can’t be found by simply multiplying the one-tailed p value by two. To find the two-tailed p value, we recommend using the Fisher’s Exact Test Calculator.</p>

<p>Fisher’s Exact Test: Example</p>

<p>Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 25 voters and survey them on their political party preference. The following table shows the results of the survey:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Democrat	Republican	Total Male	4	9	13 Female	8	4	12 Total	12	13	25 Step 1: Define the hypotheses.
</code></pre></div></div>

<p>We will perform Fisher’s Exact Test using the following hypotheses:</p>

<p>H0: Gender and political party preference are independent.
H1: Gender and political party preference are not independent.</p>

<p>Step 2: Calculated the two-tailed p value.</p>

<p>We can use the Fisher’s Exact Test Calculator with the following input:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Group1 Group1 Category1	4	9 Category2	8	4
</code></pre></div></div>

<p>One tailed p value: 0.081178
Two-tailed p value is 0.115239</p>

<p>Fisher’s Exact test example</p>

<p>The two-tailed p value is 0.115239. Since this value is less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that there is any statistically significant association between gender and political party preference.</p>

<p>Additional Resources</p>

<p>Additional Resources</p>

<p>The following tutorials explain how to perform a Fisher’s Exact Test using different statistical programs:</p>

<p>How to Perform Fisher’s Exact Test in R
https://www.statology.org/fishers-exact-test-in-r/</p>

<p>Fisher’s Exact Test Calculator
https://www.statology.org/fishers-exact-test-calculator/</p>

</p>

  <h2> - </h2>
  <p><h1 id="how-latex-is-used">How LaTeX is used</h1>
<p class="meta">28 Apr 2020</p>

<ul id="markdown-toc">
  <li><a href="#how-latex-is-used" id="markdown-toc-how-latex-is-used">How LaTeX is used</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>This page simply demonstrates how LaTeX format is included on this website.
The LaTeX equation is written as normal
The code is then converted to SVG by http://latex.codecogs.com
The resulting file is saved into site.baseurl/equations</p>

<p>code cogs for latex
<img src="https://latex.codecogs.com/svg.latex?\Large&space; x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" title="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" /></p>

<p><img src="https://latex.codecogs.com/svg.latex?x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D" alt="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" /></p>

<p><img src="/equations/svg.latex.svg" width="40%" /></p>

</p>

  <h2> - </h2>
  <p><h1 id="linear-regression">Linear regression</h1>
<ul id="markdown-toc">
  <li><a href="#linear-regression" id="markdown-toc-linear-regression">Linear regression</a></li>
  <li><a href="#simple-linear-regression" id="markdown-toc-simple-linear-regression">Simple linear regression</a></li>
  <li><a href="#drawing-the-line" id="markdown-toc-drawing-the-line">Drawing the line</a></li>
  <li><a href="#accuracy-of-the-coefficient-estimates" id="markdown-toc-accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</a></li>
  <li><a href="#se-in-hypothesis-tests-for-coefficients" id="markdown-toc-se-in-hypothesis-tests-for-coefficients">SE in hypothesis tests for coefficients</a></li>
  <li><a href="#model-accuracy" id="markdown-toc-model-accuracy">Model accuracy</a>    <ul>
      <li><a href="#residual-standard-error" id="markdown-toc-residual-standard-error">Residual Standard Error</a></li>
      <li><a href="#r2-statistic" id="markdown-toc-r2-statistic">\(R^2\) Statistic</a></li>
    </ul>
  </li>
  <li><a href="#multiple-linear-regression" id="markdown-toc-multiple-linear-regression">Multiple linear regression</a></li>
</ul>

<p><br />
This page is being formulated currently by plagerising “An Introduction to Statistical Learning”. If you find this page before a lot of changes are complete, then keep this in mind. Once major changes/completion occurs, this message will be updated to references instead.</p>

<h1 id="simple-linear-regression">Simple linear regression</h1>

<p>Predict a quantitative response \(Y\) using a predictor variable \(X\); regressing \(Y\) onto \(X\).
The <em>intercept</em> and <em>splope</em> are written as \(\beta_0\) and \(\beta_1\), respectively.
These are unknwon constants and together are knwon as the model <em>coefficients</em> or <em>parameters</em>.
The simple linear regression is written as</p>

<p>[\label{eq1}\tag{1} 
Y \approx \beta_0 + \beta_1 X]</p>

<p>Values that are estimated are labelled with a “hat”, e.g. 
\(\hat y\) - a prediction of \(Y\) based on \(X = x\).
With some sample data, one can begin to predict \(Y\) based on the predictor \(X\) using the estimated model coefficients \(\hat \beta_0\) and \(\hat \beta_1\);</p>

<p>[\label{eq2}\tag{2} 
\hat y \approx \hat\beta_0 + \hat\beta_1 x]</p>

<p>In this case, the estimated response \(\hat y\) equals the estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) according to a sample of the predictor values (\(x\)).</p>

<h1 id="drawing-the-line">Drawing the line</h1>

<p>The estimated intercept and slope (\(\hat \beta_0\) and \(\hat \beta_1\)) are unknown, therefore we need to get these values to predict \(Y\) based on \(X\).
A number (\(n\)) of obersevations are made where we measure \(X\) and \(Y\). Measurements could be recorded as: (measure 1, x = 5, y = 10), (measure 2, x = 10, y = 20), and so on up to \(n\) obersavations;</p>

<p>[\label{eq3}\tag{3} 
(5,10), (10,20),…, (x_n,y_n)]</p>

<p>[(x_1,y_1), (x_2,y_2),…, (x_n,y_n)]</p>

<p>We want to combine each measurment on a plot so that the line drawn through the data fits well and produces coefficient estimates \(\hat \beta_0\) and \(\hat \beta_1\).
Each measurement (\(i\)) is represented with \(y_i \approx \hat \beta_0 + \hat \beta_1 x_i\) for \(i = 1,2,...,n\).
The ideal result will be a line that fits all points closely. 
The measure of <em>closeness</em> has many topics of interest, but the most common method is to minimise the <em>least squares</em>.</p>

<p>\(e_i = y_i - \hat y_i\) represents the \(i\)th <em>residual</em> - the difference between our \(i\)th response according to our model versus the true \(i\)th observed response.
The <em>residual sum of squares</em> (RSS) is written as
\(\label{eq4}\tag{4} 
RSS = e^2 _1 + e^2 _2 +...+ e^2 _n\)</p>

<p>[RSS = (y_1-\hat\beta0-\hat\beta_1x_1)^2+(y_2-\hat\beta_0-\hat\beta_1x_2)^2+…+(y_n-\hat\beta_0-\hat\beta_1x_n)^2.]</p>

<p>The least squares method uses \(\hat\beta_0 and \hat\beta_1\) such that RSS is minimised. The minimisers are as follows</p>

<p>[\label{eq5}\tag{5} 
 \hat \beta_1  = 
\frac{ 
\sum_{i=1}^{n}	(	xi -\bar{x} )	(yi - \bar{y}	)	} 
{\sum_{i=1}^{n}	(	xi -\bar{x} )^2
}]</p>

<p>[\hat\beta_0 - \bar{y} - \hat\beta_1 \bar{x}]</p>

<p>where the sample means are
\(\bar{y} \equiv \frac{1}{n}\sum_{i=1}^{n} y_i\)
and
\(\bar{x} \equiv \frac{1}{n}\sum_{i=1}^{n} x_i\)
so the equation above (\ref{eq5}) defines the least squares coefficient estimates for simple linear regression.</p>

<h1 id="accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</h1>

<p>We will want to account for error \(\epsilon\) to draw an accurate line. 
\(\epsilon\) would be the mean-zero random error for the relationship between \(Y\) and \(X\) for the unknown function \(f\).</p>

<p>[\label{eq6}\tag{6} 
Y = \beta_0 + \beta_1 X + \epsilon]</p>

<p>This equation represents the <em>population regression line</em>.
We assume that the error term will be independet of \(X\), and the model is a best approximation of the \(X - Y\) relationship.
Equation 5 shows the least squares regression coefficient estimates to model the <em>least squares line</em>.
In real data, the population regression line is not observed but from the sample observations we can calculate the least squares line.</p>

<p>We would like to know how closely these two lines could be from our sampled data. 
The mean of a random variable \(Y\) would be written as \(\mu\).
If we have \(n\) number of observations of \(Y\) (\(y_1, y_2,...,y_n\)) then we can get the estimate of \(\mu\). 
So, the estimate \(\hat \mu = \hat y\), where the sample mean is \(\hat y = \frac{1}{n} \sum_{i=1}^{n} y_i\). 
Obviously the sample mean and true population mean may not be the very same, but they should be close. 
Similary, we want to know the true \(\beta_0\) and \(\beta_1\) in a linear regression so we will estimate the \(\hat\beta_0\) and \(\hat\beta_1\) as shown in eqn \ref{eq5}, the coefficient estimates that define the least squares line.</p>

<p>Likewise, \(\hat\mu\) is used to estimate \(\mu\) in an unbiased way; each estimate might be over- or underestimated but there is no particular bias in one direction from the random samples. 
The least squares coefficient estimates for \(\beta_0\) and \(\beta_1\) will also be unbiased. 
Over a large number of observations the estimate should be very accurate but since we will have a limited number of samples the estimate accuracy estimate should be established. 
This is gerneally done using the <em>standard error</em>.
When considering the estimate mean \(\hat\mu\), the formula for SE(\(\hat\mu\)) will be</p>

<p>[\label{eq7}\tag{7} 
Var( \hat\mu ) = SE( \hat\mu )^2 = \frac{\sigma^2}{n}]</p>

<p>\(\sigma\) (sigma) is the standard deviation for every \(y_i\) of \(Y\) (approximately; further reading on Gaussian error assumption and number of observations).
This SE will shirk with more frequent observations.
In the same direction we can measure the standard errors for \(\hat\beta_0\) and \(\hat\beta_1\)</p>

<p>[\label{eq8}\tag{8} 
SE(\hat\beta_0)^2 = \sigma\left[  \frac{1}{n}</p>
<ul>
  <li>\frac {\bar x^2}{ \sum_{i=1}^{n} (x_i - \bar x)^2 } \right]]</li>
</ul>

<p>[SE(\hat\beta_1)^2 =  \frac{1}{n}</p>
<ul>
  <li>\frac {\sigma^2}{ \sum_{i=1}^{n} (x_i - \bar x)^2 }]</li>
</ul>

<p>and \(\sigma^2 = Var(\epsilon)\). (To be strictly valid, the errors \(\epsilon_i\) for each observation should be uncorrelated with common variance \(\sigma^2\))
The estimate of \(\sigma\) is the <em>residual standard error</em>, and shown by \(RSE = \sqrt{ \frac{RSS}{(n-2)} }\).</p>

<p>In turn, the SE can be used to calculate the <em>confidence interval</em>.
A range of values that will contain the true unknown value of the parameter with 95% probability is the 95% confidence interval. 
This interval for \(\hat\beta_1\) is approximately</p>

<p>[\label{eq9}\tag{9} 
\hat\beta_1 \pm 2 \cdot SE(\hat\beta_1)]</p>

<p>a 95% chance that the true \(\beta_1\) value is in this range.
The same fomula is true for \(\beta_0\), swapping terms.</p>

<h1 id="se-in-hypothesis-tests-for-coefficients">SE in hypothesis tests for coefficients</h1>

<p>The SE can be used for a <em>hypothesis test</em> on the corfficients, i.e. testing the <em>null hypothesis</em> and <em>alternative hypothesis</em>,<br />
\(\label{eq10}\tag{10} 
H_0 - there is not relationship between X and Y\)<br />
\(H_0 : \beta_1 = 0\)
\(H_0 - there is not relationship between X and Y\)
\(H_a : \beta_1 = 0\),
respectively.</p>

<p>Therefore if \(\beta_1 = 0\), according to
\ref(eqn 6)
this equation reduces to 
\(Y = \beta+0 + \epsilon\), and an association is not found between \(X\) and \(Y\).
For the null hypothesis to be true (no association) we must determine if the slope \(\beta_1\), or rather our estimate \(\hat\beta_1\) is non-zero.
If the SE\(\hat\beta_1\) is small then we are confident that \(\hat\beta_1\) is accurate, if for example \(\beta_1 \neg 0\) indicating an 
\(X - Y\) relationship.
If our estimate SE (SE \(\hat\beta_1\)) is large, then we will need to see a much greater 
\(X - Y\) association before rejecting the null hypothesis (association is true).
A <em>t</em>-statistic is used to measure how many <em>standard deviations</em> (SD) 
\(\hat\beta_1\) is from 0.
T-statistic:
\(\label{eq11}\tag{11} 
t = \frac{
\hat\beta_1 - 0}{
SE(\hat\beta_1)}\)
No relationship will give <em>t</em>-distribution \(n - 2\) degrees of freedom.
(bell curve shape, for values of n greater than \(\approx\) 30 it is similar to normal distribution).</p>

<p>The probability of observing any number equal to |<em>t</em>| or larger, assuming \(\beta_1 = 0\), is the <em>p-value</em>.
A very low p-value indicates that it is unlikely that an assocation will be measured by chance, and therefore is like a true association between predictor and response, and we can <em>reject the null hypothesis</em>.</p>

<h1 id="model-accuracy">Model accuracy</h1>

<p>After rejecting the null we want to check if the line fits well - does the model fit the data?
Ttwo related quantities can asses the fit of linear regression; <em>residual standard error</em> (RSE) and \(R^2\)
statistic.</p>
<h2 id="residual-standard-error">Residual Standard Error</h2>

<p>Because of the error term \(\epsilon\),
the LR cannot perfectly predict \(Y\) from \(X\).
RSE is an estimate of the SD of \(\epsilon\).</p>

<p>[\label{eq12}\tag{12} 
RSE = \sqrt{
\frac{1}{n-2} RSS
}
=
\sqrt{
\frac{1}{n-2}
\sum_{i=1}^n 
(y_i - \hat y_i)^2
}]</p>

<p>The RSS part is expanded on the right hand side of the equation since:
$$</p>

<p>RSS = 
\sum_{i=1}^n 
(y_i - \hat y_i)^2</p>

<p>$$.</p>

<p>The RSS is a measure of the <em>lack of fit</em> of the model equation \ref{eq6} to
the dataset.
The value calculated in eqn \ref{eq12} will become smaller when the estimate and true value become closer; 
\(\hat y_1 \approx y_1\) for 
\(i = 1,2,...,n\), in which case the model will fit the data well.</p>

<h2 id="r2-statistic">\(R^2\) Statistic</h2>
<p>We use this all the time; it will be worth getting into the discussion from 
Cosma Rohilla Shalizi in
<a href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/">Advanced Data Analysis from an Elementary Point of View</a>
<em>Chapter 2 The Truth about Linear Regression</em>
``2.2.1.1 R2: Distraction or Nuisance?’’. 
The short summary following just covers the basic “normal” usage, rather getting into the statistica weeds.</p>

<h1 id="multiple-linear-regression">Multiple linear regression</h1>

<p>In a linear regression we use a predictor variable X to predict a quantitative response Y.
If we want to use more than one variable (\(X_1, X_2, ...\)) 
that might affect the response Y, then we will use a multiple linear regression.</p>

<p>[\label{eq }\tag{ }]</p>

<p>[\label{eq }\tag{ }]</p>

<p>[\label{eq }\tag{ }]</p>

</p>

  <h2> - </h2>
  <p><h1 id="pca">PCA</h1>
<ul id="markdown-toc">
  <li><a href="#pca" id="markdown-toc-pca">PCA</a></li>
</ul>

<p><br />
We will look at principal component analysis (PCA) and singular value decomposition (SVD).
Example: variable 1 and 2 (e.g. gene expression), and 6 observations (6 mice).
One measurement could be plotted on a single line; low values (left) to high values (right).
You might see a cluster of half the group on one side (low expression) and the rest of the group on the other side of the plot line (high expression).
Measuring two variables (ie. two genes), the sample approach would now make an x/y 2-D graph. You might see one half of the measurement group clustering with low values for both variables (low expression of gene 1 and gene 2).
If we measure 3 genes it would be, of course, a 3-D graph. Higher dimensions cannot be easily graphed.
PCA can make the multiple dimension measurements into 2-D graphs and identify what measurements are most valuable for clustering. PCA can also tell us how accurate the 2-D graph is.</p>

<p>We calculate the average value of variable 1 (gene 1) and the average value of variable 2. With the average, we can calculate the center of the data. Once this is done, we no longer need the original values, rather just the relative position on the 2-D graph.
If the center of the data were a crosshair on the 2-D plots, we will shift the plotted values so the that crosshair rests on the origin (0,0) of the graph. Once the data is centered on the origin we will then fit a line through the origin that best fits the data.</p>

<p>To quantify how well a line is fit to the data, PCA projects the data onto the line.
It can either measure the distance from the data to the line to find the line that minimises distance, or it can find the line that maximises the distance from projected points to the origin.
Both of these actions are equivalent. i.e. a point at 3,2 has a set distance from the origin (line a).
The line that we use to separate the whole dataset could be labelled line c.
Our original point at 3,2 would be joints to its projected position on line c by line b at a 90 degree angle.
As our separator line c rotates around the origin to find the best fit to the dataset, the line b will grow or shrink with the projected distance.
Line a does not change, (joining origin to 3,2).
Therefore b and c form a right angle and Pythagorean theorem shows how b and c are inversely related</p>

<p>\(a^2 = b^2 + c^2\).</p>

<p>Thus, PCA can can either minimise the distance from the data to the line (via line b), or it can find the line that maximises the distance from projected points to the origin (via line c). C is the easiest to calculate (sum of squared distances of projected position to the origin, i.e. \(sum of d_{1}^2 + d_{2}^2 + d_{3}^2 ... d_{n}^2\)), “Sum of squared distances”. The values are squared to cancel negative values.
The line is rotated and the same process is repeated to find the largest sum of squared distances, the Principal Component 1 (PC1).
The slope of PC1 is recorded, e.g. slope 0.5 - for every 2 units from origin along x we move up 1 unit, and data are spread more along axis-x then axis-y. PC1 is a linear combination of variables means that to get PC1 we would have 2 parts of x-axis variable and 1 part y-axis variable (i.e. gene 1 and gene 2).</p>

<p>If we were to use singular value decomposition (SVD), we could say that the line separator line distance for 1,0.5 would be:</p>

<p>\(a^2 = b^2 + c^2\),</p>

<p>\(a^2 = 2^2 + 1^2\),</p>

<p>\(a = \sqrt{2^2 + 1^2}\),</p>

<p>[a = 2.236]</p>

<p>Doing PCA with SVD, PC1 would be scaled so that the length of the line is = 1.
Each side of the Pythagorean triangle would be divided by 2.236;</p>

<p>(2.236/2.236)^2 =  (2/2.236)^2 + (1/2.236)^2</p>

<p>PC1 we would have 2/2.236 (0.894) parts of x-axis variable and 1/2.236 (0.447) part y-axis variable (i.e. gene 1 and gene 2).
The ratio is still the same
as 2 out, 1 up.
The 1 unit along vector (the slope normalised to 1, 0.894 gene 1 and 0.447 gene 2) is the “Singular Vector” or the “Eigenvector” for PC1.
The proportions of each gene/varaible are “Loading Scores).
Squared sum distances for PC1 = Eigenvalue for PC1.
“Square root of Eigenvalue for PC1” = “Singular Value for PC1”</p>

<p>PC2 is perpendicular to PC1. 90 degrees rotated e.g. 2 up, -1 out. Scaling to get a unit vector it would be
-0.447 gene 1 and 0.894 gene 2, the Eigenvector for PC2.</p>

<p>Variation for PC1 = Square sum of dist for PC1 divided by n-1 (e.g. 12)</p>

<p>Variation for PC2 = Square sum of dist for PC2 divided by n-1 (e.g. 3)</p>

<p>The total variation around both PCs is 12+3=15. PC1 accounts for 12/15 = 0.8, 80% of variation around the PCs; PC2 accounts for 20%.
A “scree plot” shows the percentage of variation that each PC accounts for.
For a third variable, center the data, find the best fit line through the origin. The best fit is PC1. But the recipe now has 3 components. Variable 3 (gene 3) might have the most important contribution (e.g. 0.5 part gene 1, 0.1 part gene 2, 0.8 part gene 3). Find PC2, the next best fit line through the origin, that is perpendicular to PC1. This time, gene 1 might be the most important part of PC2. Lastly, PC3 is the best fit line through the origin and perpendicular to PC1 and PC2.
The PCs can be either the numbers variables or the number of samples - whichever is smallest.</p>

<p>With all of the PCs calculated, the Eigenvalues (sum of squared distances) can be used to determine the proportion of variation that each PC accounts for. The scree plot of variation accounted for by each PC, you can decide how many PCs are useful, e.g. PC1 and PC2 may account for 95% of variation.
You may keep just PC1 and PC2, projecting the data onto these, so that PC1 and PC2 are horizontal and perpendicular. The final plot might just need one 2-D graph to account for the majority of data, although we can use several plots if needed to view more complexity from PC3, PC4 etc..
Lastly, some analysis like GWAS can incorporate multiple PCs as individual covariates (with Plink or GCTA this would be a table with one data column per PC, and one row per sample ID) to account for complexity.
In that case, our visualisation plots are more for sanity and understanding the diversity within the dataset.</p>
</p>

  <h2> - </h2>
  <p><h1 id="statistics-books">Statistics books</h1>
<p>Statistical Inference, Casella &amp; Berger<br />
<a href="https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf">https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf</a><br />
Advanced Data Analysis from an Elementary Point of View by Cosma Rohilla Shalizi. <br />
<a href="http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/">http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/</a>
Mathematical statistics and data analysis<br />
<a href="https://epdf.tips/mathematical-statistics-and-data-analysis65096.html">https://epdf.tips/mathematical-statistics-and-data-analysis65096.html</a><br />
 The Probability and Statistics Cookbook<br />
<a href="http://statistics.zone">http://statistics.zone</a><br />
 The Elements of Statistical Learning Data Mining, Inference, and Prediction<br />
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print11.pdf">https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print11.pdf</a><br />
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a><br />
Core Statistics, Simon N. Wood<br />
<a href="https://people.maths.bris.ac.uk/~sw15190/core-statistics.pdf">https://people.maths.bris.ac.uk/~sw15190/core-statistics.pdf</a><br />
Calculus Made Easy Thompson<br />
<a href="http://www.gutenberg.org/ebooks/33283?msg=welcome_stranger">http://www.gutenberg.org/ebooks/33283?msg=welcome_stranger</a><br />
 Bayesian Approaches to Clinical Trials and Health-Care Evaluation<br />
<a href="http://the-eye.eu/public/Books/Medical/texts/Bayesian%20Approaches%20to%20Clinical%20Trials%20and%20HealthCare%20Eval.%20-%20D.%20Spiegelhalter%20%282004%29%20WW.pdf">http://the-eye.eu/public/Books/Medical/texts/Bayesian%20Approaches%20to%20Clinical%20Trials%20and%20HealthCare%20Eval.%20-%20D.%20Spiegelhalter%20%282004%29%20WW.pdf</a><br />
Bayesian Data Analysis<br />
<a href="https://www.academia.edu/32086149/Bayesian_Data_Analysis_Third_Edition_Gelman_.pdf">https://www.academia.edu/32086149/Bayesian_Data_Analysis_Third_Edition_Gelman.pdf</a><br />
Doing Bayesian data analysis a tutorial with R and BUGS.pdf<br />
<a href="http://www.users.csbsju.edu/~mgass/robert.pdf">http://www.users.csbsju.edu/~mgass/robert.pdf</a></p>

</p>

 -->
</feed>
